\title{Adaptive Waveform Inversion for Transmitted Wavefields}
\author{William W. Symes}

\begin{abstract}
Adaptive Waveform Inversion
applied to transient transmitted wave data yields estimates of index of
refraction (or wave velocity) similar to those obtained by travel time
inversion, provided that the data contain a single smooth wavefront.
\end{abstract}

\section{Introduction}
Full waveform inversion (FWI) refers to the inference of mechanical
parameter distribution within a material object  (the earth, a human
body, a manufactured artifact,...) from remote measurements of waves
propagating through it. Over the last several decades, this task has come to
be carried out by minimizing a measure of the difference between computational
prediction of wave data versus actual observed
data. The mechanical parameters to be inferred appear as
coefficients in systems of partial differential equations governing
the wave motion, and discrete representations of these parameters form
the unknown vectors in the optimization problems thus generated.

While widely used in recent years, FWI exhibits a pathology when wave
velocities or their parametric equivalents are amongst the parameters
to be estimated: the excessive sensitivity of predicted waveforms to
changes in velocity leads to oscillations in the difference measures,
especially for the most commonly used square-integral measure. Local
optimization algorithms (relatives of Newton;'s method) are the only practical
methods to generate sequences of approximate optimal
parameters. However the oscillations in the difference measures lead
these sequences to stagnate at parameter distributions
far from the optimum, and thus do not extract the maximum information
from the data \cite[]{VirieuxOperto:09}. 

Various modifications of straighforward best-fit FWI have been
suggested to overcome this ``cycle-skipping'' pathology. One of these
is Adaptive Waveform Inversion \cite[]{Warner:16} (AWI). This
technique has been successfully applied to industry-scale seismic data
\cite[]{GuaschWarnerRavaut:GEO19,Warneretal:SEG21} and shows promise for
ultrasonic imaging of the the human brain
\cite[]{Guaschetal:NPJDM20}.

While these and other works discuss heuristic justifications for AWI
and many details of implementation, a clear explanation for its
effectiveness is so far missing from the literature. In this short
note, I treat a special case of FWI, the recovery of smooth acoustic
medium parameters from data generated at localized energy sources,
propagated through a fluid, and recorded at receiver locations remote
from the sources. I show that the AWI objective function is an
asymptotic (in time frequency) approximation to the mean-square
time-of-travel misfit, provided that arrival times of transmitted
waves are single, that is, unique rays of geometric asymptotics
connect sources and receivers. The error in this approximation is
proportional to the source mean-square wavelength. Thus AWI and
minimization of mean-square travel time error (travel time tomography)
lead to similar material parameter estimates. Travel time tomography
appears to be much less sensitive to choice of initial model than is
FWI, and in fact is commonly used to construct an initial model for
FWI \cite[]{Bordingetal:87,SirguePratt:04,VirieuxOperto:09}. At least
in the special case treated here, AWI implicitly combines travel time
tomography with FWI.

The reader will
note that a normalization component of the AWI objective function
definition is important in reaching this conclusion. The connection
between objective normalization and the asymptotic equivalence to
travel time inversion for transmission data has nothing to do with the
reasons for normalizing the objective previoiusly advanced in the
literature \cite[]{Warner:16,LiAlkhalifah:21}. Another modification of
FWI based on similar ideas, Matched Source Waveform Inversion (MSWI)
(\cite{HuangSymes2015SEG,HuangSymes:Geo17}) lacks the AWI
normalization, and has a looser relation to travel time inversion. The
arguments presented here for AWI (and MSWI) are refinements of those
given by \cite{HuangSymes:Geo17}. which were in turn based on ideas
presented in Song's thesis \cite[]{Song:94c}.

The conclusions of this study depend on a number of limitations on the
acoustic model of wave propagation and the formulation of the related
inverse problem. The principal constraints are:
\begin{itemize}
  \item energy sources are modeled as isotropic point radiators, with
    known time dependence vanishing outside a finite time window;
  \item mechanical parameter fields (coefficients in the acoustic wave
    equation) are smooth, that is, infinitely differentiable as
    functions of position, and the material density is known and
    position-independent: only the bulk modulus (or wave velocity) is
    to be determined;
  \item waves propagate throughout Euclidean space-time - that is, no
    physical boundaries are modeled;
  \item the data of the inverse problem are the restrictions of the
    pressure field to a finite number of points in space (receiver
    positions), for a finite number of source positions and all time;
  \item local energy decays exponentially - that is, the pressure and
    all of its derivatives tend to zero at an exponential rate as $t
    \rightarrow \infty$,
    uniformly in any bounded spatial domain.
  \end{itemize}
  These constraints are discussed in detail in the next section. Some
  could certainly be relaxed. For instance, exponential decay of local
  energy implies that the finite time duration of actual measured data
  can be treated as a perturbation of the infinite time data setting
  posed here. However these hypotheses enable a simple and clear
  analysis of AWI, and seem a reasonable starting point for further
  investigation.
  
After establishing the class of problems to be studied and notations
for their description in the following section, I present the
asymptotic analysis leading to the main conclusion in the next. In
effect, AWI regularizes or relaxes the FWI problem. For many inverse
problems, regularization has been accomplished via additive penalty
functions. In section four, I explain the relation between AWI and a
penalty formulation of the wave inverse problem. The
close relation between AWI objective and travel time misfit is limited
in scope: in the fifth section, I show that it does not hold in
general for transmitted wave data in which multiple rays connect
source and receiver. 

The analysis explained here sheds no light on the effectiveness of AWI
for reflected wave data, and the approach has so far been limited
inversion of data modeled by acoustic wave propagation. I discuss
these and other aspects of AWI and related approaches in a brief final
section.

\section{Adaptive Waveform Inversion}

The version of AWI introduced by \cite{Warner:16} is based on an
active source acoustic model of data generation in Euclidean space
$\bR^3$. In this section, I will review a version of this model, place
some useful constraints on energy sources appearing in it, and define
AWI as an optimization problem.

\subsection{Acoustic Modeling of Transmitted Waves}

Each acoustic source is modeled an isotropic point radiator
with known location $\bx_s\in \bR^3$. These
sources generate acoustic waves traveling through Euclidean space. The
data are samples of these waves at receivers, modeled as isotropic
point sensors, recording pressure at points
${\bf x}_r \in \bR^3$. The collection $X_{sr}$ of source-receiver pairs
$(\bx_s,\bx_r)$ for which data is recorded should not intersect the
diagonal - that is, no receiver should be co-located with a source for
which it is active. The set of source locations $X_s = \{\bx_s: (\bx_s,\bx_r) \in X_{sr}
\mbox{ for some } \bx_r \in \bR^3\}$ will also be useful.

The pressure and velocity fields $p({\bf x},t;{\bf x}_s)$,
${\bf v}({\bf x},t;{\bf x}_s)$ for the source location ${\bf x}_s$
depend on the bulk modulus $\kappa$, buoyancy $\beta$
(reciprocal of the density $\rho$), and source wave
form $f(t)$ through the acoustic system
\begin{eqnarray}
  \label{eqn:awe}
 \frac{\partial p}{\partial t} & = &- \kappa \nabla \cdot {\bf v} +
                                    f(t) \delta({\bf x}-{\bf x}_s); \nonumber \\
\frac{\partial {\bf v}}{\partial t} & = & - \beta \nabla p; \\ 
p, {\bf v} & = & 0 \mbox{ for }  t \ll 0.
\end{eqnarray}
For simplicity, I assume that the system \ref{eqn:awe} holds over all of Euclidean
space-time $(\bx,t) \in \bR^3 \times \bR$, and ignore physical
boundaries such as the Earth's surface. Likewise for simplicity, I
assume in this study that the material density $\rho$ is spatially
homogeneous and known. In some settings, eg. the earth's
upper crust, the material density $\rho$ exhibits substantially less variance than the
bulk modulus, Density is however spatially variable in most geological
and biomedical materials, and could be included amongst the parameters
to be estimated.

The parameter to be determined in the inverse problem studied here is
the bulk modulus $\kappa$. It is presumed is bounded and smooth:
\begin{equation}
  \label{eqn:coeff}
  \log \kappa \in C^{\infty}(\bR^3) \cap L^{\infty}(\bR^3)
\end{equation}
Under these hypotheses, the symmetric hyperbolic system \ref{eqn:awe}
has a unique distribution
solution if $f \in \cal{E}'(\bR)$ (for example \cite{Lax:PDENotes},
Theorem 7.4). The Propagation of
Singularities Theorem (\cite{Tay:81}, Theorem VI.2.1) and
the pull-back property of distributions \cite{Dui:95}, Proposition 1.3.3)
imply that if $f \in \cal{E}'(\bR)$ and $(\bx_s,\bx_r) \in X_{sr}$,
then the trace $p(\bx_r, \cdot;\bx_s)$ on $\{(\bx_r,t): t \in \bR\}$ is well-defined,
as a distribution vanishing for sufficiently large
negative $t$. Define the {\em modeling operator}
\begin{equation}
  \label{eqn:modop}
  F: C^{\infty}(\bR^3) \cap L^{\infty}(\bR^3)
  \rightarrow X_{sr} \times {\cal D}'(\bR)
\end{equation}
by
\begin{equation}
  \label{eqn:modopdef}
  F[\kappa]= p|_{X_{sr} \times \bR}
\end{equation}

In this setting, acoustic FWI means: given the set of
source-receiver position pairs $X_{sr}$, source wavelet
$f \in {\cal E}'(\bR)$,  density $\rho$, and data traces
$d:  X_{sr} \rightarrow \cal{D}(\bR)$, find a bulk modulus $\kappa$
(or equivalently wave velocity $c=\sqrt{\kappa/\rho}$)
so that $F[\kappa] \approx d$ in some sense. If
$d$ and $F[\kappa]$ lie in a normed subspace of ${\cal D}'(\bR)$ for each
$(\bx_s,\bx_r) \in X_{sr}$, then the
norm provides a straightforward candidate for this sense of
approximation.

%In practice, recording takes place over a finite time interval
%$[0,t_d]$. For convenience, assume that this interval is the same for
%all receivers. Let $\phi \in C_0^{\infty}(0,t_d)$ be a window function
%supported in this interval. Additional constraints on $t_d$ and $\phi$
%will be be specified below.

\subsection{Hadamard's Decomposition}
The Hadamard decomposition of the acoustic Green's function \cite[]{Friedlander:75,Qian:JCP24} provides a
basis for a norm-based formulation of FWI. As presented by
\cite{Friedlander:75}, for instance, this construction applies to
second-order wave equations, but the system \ref{eqn:awe} is
equivalent fo a second-order problem: differentiate the first equation
with respect to $t$, eliminate $\bv$ via the second equation, and
rearrange to obtain
\begin{equation}
  \label{eqn:awe2}
  \frac{\rho}{\kappa}\frac{\partial^2 p}{\partial t^2} - \nabla^2 p -
  \nabla \log \rho \cdot \nabla p = \rho \frac{\partial f}{\partial t}
  \delta(\bx-\bx_s), \nonumber \\
  p=0, \, t \ll 0.
\end{equation}
If $(p,\bv)$ is a distribution solutions of the system
\ref{eqn:awe} then $p$ is a solution of equation
\ref{eqn:awe2}, as follows by differentiating the first equation of
\ref{eqn:awe} with respect to $t$ and eliminating $\bv$ via the second
equation. Conversely, if $p$ is a solution of the second-order initial
value problem\ref{eqn:awe2}, then under suitable conditions (including
those to be imposed below),
\[
  \bv = \beta \int_{-\infty}^t p
\]
defines a distribution, and $(p,\bv)$ solves the system \ref{eqn:awe}.

Hadamard's construction is local, in both space-time and in the set of
acoustic models. A {\em normal neighborhood} of
$\bx_s \in \bR^3$ is an open subset of $\bR^3$ 
within which every point is connected to $\bx_s$ by  a unique ray of
geometric acoustics (a geodesic for the Riemannian metric associated
to the equation \ref{eqn:awe2}) (\cite{Friedlander:75},
p. 16). Hadamard's decomposition holds in a space-time cylinder
$\Omega \times \bR$,  and $\Omega\subset \bR^3$ 
and $M \subset C^{\infty}(\bR^3) \cap L^{\infty}(\bR^3)$ are bounded open sets satisfying 
\begin{itemize}
  \item[G1. ] $X_{sr} \subset \Omega \times \Omega$;
  \item[G2. ] for every source point $\bx_s\in X_{s}$ and $\log \kappa \in M$, $\Omega$ is a
    normal neighborhood of $\bx_s$.
  \item[G3. ] for every source point $\bx_s \in X_{s};$
    $\log \kappa \in M$, ray ${\bf X}:\bR^+\rightarrow \bR^3$
    with ${\bf X}(0)=\bx_s$ and any $t>0$, either $\{{\bf X}(s): 0\le s \le t\} \subset \Omega$
    or $\{{\bf X}(s): t < s < \infty\} \subset \bR^3 \setminus \Omega$.
  \end{itemize}
Condition G3 (the ``no return'' condition) means that once a ray
leaves $\Omega$, it does not return to $\Omega$. Conditions G1 - G3
hold for any convex open set $\Omega$ if $\kappa$ is constant, and for small perturbations and some
large perturbations of constant $\kappa$: these are open conditions in
the topology of $M$.

The arguments to follow depend an additional assumption about solutions of
the wave equation \ref{eqn:awe2}, restricted to $X_{sr}\times \bR$:
\begin{itemize}
\item[G4. ] For any $n > 0$, $f \in {\cal E}'(\bR)$, there exist $K >
  0, T \in \bR,$ and $\delta>0$ so
  that for any $k$ with $0 \le k \le n$, $(\bx_s,\bx_r) \in
 X_{sr},$
  and any $\log \kappa \in M$, the solution $p$ of \ref{eqn:awe2} is smooth
  for $t>T$ and satisfies
  \[
    |\partial_t^k p(\bx_r,t;\bx_s)| \le K e^{-\delta t}, \, t > T.
  \]
\end{itemize}
Condition G4 is a special case of {\em local energy decay}, a much
studied topic. Properties which imply G4 are in turn implied by the
{\em non-trapping} hypothesis on the model $\log \kappa \in M$: all rays of
geometric asymptotics escape to infinity (\cite{Hristova:09}, Theorem
2, \cite{EgorovShubin}, Theorem 2.104, p. 192).

Under conditions G1 - G4, the construction described by
\cite{Friedlander:75} produces for each $m \in M$ functions $\tau[\kappa]$ (travel time), $a[\kappa]$
(geometric amplitude) on $X_s \times \Omega$ and $b[\kappa]$
(remainder kernel) on $X_s \times \Omega \times \bR$, satisfying
\begin{itemize}
\item[H1. ] $\tau[\kappa], a[\kappa],$ and $b[\kappa]$ are smooth off the diagonal,
that is, $\tau[\kappa](\bx_s,\cdot) \in C^{\infty}(\Omega \setminus \{\bx_s\})$
and similarly for $a[\kappa],b[\kappa]$;
\item[H2. ] there exists $A>0$ so that for any $\log \kappa \in M$
  and $(\bx_s,\bx_r) \in X_{sr}$,
  $|\log a[\kappa](\bx_s,\bx_r)| < A$;
\item[H3. ] there exist $B \ge 0,\delta > 0$ so that for
$(\bx_s,\bx_r) \in X_{sr}$, and $\kappa \in
M$,
\begin{eqnarray}
  |b[\kappa](\bx_r,t;\bx_r,s)| &\le& B e^{-\delta t} \nonumber \\
  |\partial_t b[\kappa](\bx_r,t;\bx_r,s)| &\le& B e^{-\delta t}
  \label{eqn:best}
\end{eqnarray}
\end{itemize}
(The exponential decay estimate in item H3 follows from hypothesis G4.)

The Hadamard
decomposition of the Green's function $G[\kappa]$, i.e. the solution of the
equation \ref{eqn:awe2} for $f = \beta H$, is::
\begin{equation}
  \label{eqn:green}
  G[\kappa](\bx_s,\bx,t) = a[\kappa](\bx_s,\bx)\delta(t - \tau[\kappa](\bx_s,\bx)) +
  b[\kappa](\bx_s,\bx,t-\tau[\kappa])H(t - \tau[\kappa](\bx_s,\bx))
\end{equation}
Define $\delta_{\tau} = \delta(t-\tau)$ to be the shifted delta, that
is, $\langle \delta_{\tau},\phi\rangle = \phi(\tau)$. A useful
restatement of the definition \ref{eqn:green}, suppressing the
space-time coordinates, is
\begin{equation}
  \label{eqn:regreen}
  G[\kappa] = (a[\kappa]\delta +  (b[\kappa]H))*\delta_{\tau[\kappa]}.
\end{equation}

%Under conditions G1 - G3 above, this decomposition is the restriction of
%the Greeen's function to $\Omega \times (-\infty,t_h]$. This fact
%implies a decomposition of $F[\kappa]$, provided that the convolution $G*f$
%restricted to $[0,t_d]$ is determined by the restriction of $G$ to
%$\Omega \times [0,t_h]$. That is the case if $t_h > t_d + |\mbox{supp
%|f/2$ (the factor of $1/2$ because $f$ is odd).
%Making this assumption (which is weaker than assumptions on $t_h$ to be made below),
%\[
%   F[\kappa](\bx_s,\bx_r)(t) =
%   \phi(t)a(\bx_s,\bx_r)\rho_s\frac{df}{dt}(t-\tau[\kappa](\bx_s,\bx_r))
% \]
%\begin{equation}
%  \label{eqn:fwdgreen}
% + \phi(t)\rho_s\int
%  \,ds \, \frac{df}{dt}(t-s) b[\kappa](\bx_s,\bx_r,s-\tau[\kappa](\bx_s,\bx_r))H(s-\tau[\kappa](\bx_s,\bx_%r))
%\end{equation}

\subsection{FWI via Least Squares}
For $f \in {\cal E}'(\bR)$, the Hadamard decomposition 
(equation \ref{eqn:regreen}) gives:
\begin{equation}
  \label{eqn:fwdgreen}
  F[\kappa] = G[\kappa] * \rho \partial_t f  = \rho( a[\kappa]\delta
  +  (b[\kappa]H)) *\delta_{\tau[\kappa]}*\partial_t f.
\end{equation}
Suppose that the source wavelet $f$ satisfies
\begin{itemize}
\item[F1. ] $f \in C^{\infty}_0(\bR)$;
\item[F2. ] $f$ is odd, $f(t) = -f(-t)$.
\end{itemize}
Then equation \ref{eqn:fwdgreen}, assumptions F1, F2, bounds
\ref{eqn:best} and Young's inequality implies that $F[\kappa] \in
L^p(X_{sr} \times \bR)$ for any $p \ge 1$. Moreover there exists
$C>0$ so that for $\log \kappa\in M$, $p \ge 1$,
\begin{equation}
  \label{eqn:fwdnorm}
  \|F[\kappa]\|_{L^p(X_{sr} \times \bR)} \le C \left\|\partial_t f\right\|_{L^p(\bR)}.
\end{equation}
This observation justfies posing FWI as a least-squares problem: given $d \in
L^2(X_{sr}\times \bR)$, $f \in H^1_0(\bR)$, and $\rho_s>0$, find $\kappa$ to minimize
\begin{equation}
  \label{eqn:fwi}
  J_{\rm FWI}[m;d]= \frac{1}{2}\|F[\kappa]-d\|_{L^2(X_{sr} \times \bR)},
\end{equation}

Practical versions of least-squares FWI include various types of regularization and
imposition of constraints on $m$, see the references cited in the
Introduction.
%As mentioned there, local iterative optimization methods
%are the only practical approach to minimizing $J_{\rm FWI}$ due to the
%computational cost of approximating the forward map $F$ and related
%operators, but these tend to stagnate at highly sub-optimal
%(``cycle-skipped'') estimates of $m$ unless the initial estimate is of
%quite high quality, often available only at considerable cost in data
%collection and processing.

\subsection{AWI via Optimization}

AWI is based an {\em extension} of the modeling operator $F$, that is,
an operator ${\bar F}$ with larger domain, coinciding with $F$ on its domain..
The extended modeling operator ${\bar F}$ maps extended models
$\bar{m}=(\kappa,\bar{w})$ incorporating extended sources
$\bar{w}({\bf x}_r,t;{\bf x}_s)$ to the same sampling of the pressure
field. Denote by $\bar{p}(\bx,t;\bx_s,\bx_r), \bar{\bv}(\bx,t;\bx_s,\bx_r)$
the solution of the acoustic system \ref{eqn:awe} with $w(t)$ in the
right-hand side of the first equation replaced by
$\bar{w}(\bx_r,\cdot;\bx_s)$. Then
$\bar{F}[\kappa,\bar{w}](\bx_r,t;\bx_s) =
\bar{p}(\bx_r,t;\bx_s,\bx_r)$. As was the case for $F$, the Hadamard
decomposition \ref{eqn:green} and conditions G1-G4, W1-W3 imply that
$\bar{F}[\kappa, \cdot]$ is a bounded map: $L^2(X_{sr} \times \bR)
\rightarrow L^2(X_{sr} \times \bR)$. If all  
of the wavelets are the same, that is, ${\bar w}({\bf  
  x}_r,t;{\bf x}_s) = w(t)$ is independent of source and receiver  
position, then ${\bar F}[\kappa,\bar{w}] = F[\kappa]$. That is, $F$ is a special
case, or restriction, of ${\bar F}$, so ${\bar F}$ is an extension of
$F$. This is the {\em source-receiver} extension, in the terminology of \cite{HuangSymes2015SEG}.

\cite{Warner:16} represent the extended sources $\bar{w}$ as time convolutions of the
(known) source $w$ with an {\em adaptive filter} $u \in L^2(X_{sr}
\times \bR)$, that is, $\bar{w}=u * w$.
Here, $u*w$ denotes the result of convolving each trace in $u$ by $w
\in L^2(\bR)$. I
will also use the asterisk to denote the trace-by-trace convolution
of two data sets with the same set of source and receiver coordinates,
that is, the data set consisting of time convolutions of traces with the same source
and receiver coordinates. The meaning will be clear from context.

For such extended sources,
\[
  \bar{F}[\kappa,u*w] = u*F[\kappa]
\]
Introduce the notation $S[\kappa]$ for the operator of convolution
with $F[\kappa]$, that is,
\begin{equation}
  \label{eqn:sdef}
  S[\kappa]u \equiv u*F[\kappa]
\end{equation}
Then $S[\kappa]$ is bounded on $L^2(X_{sr} \times \bR)$, uniformly in
$\log \kappa \in M$, thanks to
the bound \ref{eqn:fwdnorm} for $p=1$. $S$ also represents an
extension of $F$, in the sense that with the choice
$u(\bx_r,t;\bx_s)=\delta(t)$, the output of $F$ is recovered:
\begin{equation}
  \label{eqn:sconsist}
  S[\kappa]\delta = F[\kappa].
\end{equation}

%Since $S[\kappa]$ involves solution of the wave equation with point source
%wavelet $u*w$, application of the Hadamard decomposition \ref{eqn:green} requires a furnther
%constraint on ray geometry, expressed by condition G3. Specifically,
%\begin{itemize}
%\item[G4. ]$t_h > t_d + t_w + t_u$.
%\end{itemize}
%Then \ref{eqn:green} implies that
%\begin{equation}
%  \label{eqn:sop}
%  S[\kappa]u = w* a[\kappa] T[\kappa] ((\delta + b[\kappa]H)*u)
%\end{equation}
%in which $T[\kappa]$ is the time translation operator by $\tau[\kappa]$:
%\begin{equation}
%  \label{eqn:Tdef}
%  T[\kappa]u(\bx_r,t;\bx_s) = u(\bx_r,t-\tau[\kappa](\bx_s,\bx_r);\bx_s),
%\end{equation}
%Similarly,
%\begin{equation}
 % \label{eqn:fwdgreenop}
%  F[\kappa] = \phi a[\kappa] T[\kappa]((\delta + b[\kappa]H)*w) 
%\end{equation}

%Note that values of $b$ for $t > t_d + t_w + t_u$ do not affect the
%definitions of $S[\kappa]u$ or $F[\kappa]$. Therefore alter $b$ by
%multiplication by a suitable cutoff function so that its support lies in
%$(-\infty, t_h]$.

\cite{Warner:16} assume (implicitly) that there is always a unique
solution of the extended inversion problem, formulated in terms of
$S$: that is, that
for any $\kappa$ and any data $d$, there is an
adaptive filter $u[\kappa;d]$ for which
$S[\kappa]u[\kappa;d]= d$. If $d$ is noise-free, that is, $d=F[\kappa]$, then the consistency relation \ref{eqn:sconsist}
suggests that $u[\kappa;d](\bx_r,t;\bx_s) = \delta(t)$. This ideal adaptive filter is in the null
space of multiplication by $t$, that is, the operator
$u(\bx_r,t;\bx_s) \mapsto t u(\bx_r,t;\bx_s)$. This
observation motivates the definition of the AWI objective function
(\cite{Warner:16}, equation XXXX)::
\begin{equation}
  \label{eqn:awideforig}
  J_{\rm AWI}[m;d] = \sum_{\bx_s,\bx_r} \left(\frac{\int dt |tu[\kappa;d](\bx_r,t;\bx_s)|^2}{\int dt |u[\kappa;d](\bx_r,t;\bx_s)|^2}\right) 
\end{equation}
in which $u[\kappa;d]$ is as defined above, that is, the solution of $S[\kappa]u[\kappa;d]
= d$. \cite{Warner:16} also formulate
AWI in terms of the inverse filter, that is, $u$ for which
$u*d = F[\kappa]$, and in fact recommend that definition. The present
discussion uses only the forward filter.
Adaptive waveform inversion consists in iterative minimization of
$J_{\rm AWI}$: choose an initial estimate $\kappa_0$ of the
model, then update $\kappa$ repeatedly using a gradient
descent method. \cite{Warner:16} describes the computation of $J_{\rm AWI}$
and its gradient in detail, and their use in a quasi-Newton
optimization algorithm.

The same observation motivates the definition of the {\em Matched
  Source Waveform Inversion} (MSWI) objective studied by
\cite{HuangSymes2015SEG,HuangSymes:Geo17}:
\begin{equation}
  \label{eqn:mswideforig}
  J_{\rm MSWI}[m;d] = \sum_{\bx_s,\bx_r} \int dt |tu[\kappa;d](\bx_r,t;\bx_s)|^2
\end{equation}
That is, the two objectives differ only by the trace-by-trace
normalization featured in the definition of AWI (equation
\ref{eqn:awideforig}). Much of the analysis of $J_{\rm AWI}$ pertains to
$J_{\rm MSWI}$, as will be explained below.
The trace-by-trace normalization in the definition of $J_{\rm AWI}$
will prove quite consequential.

\section{Why it works}

It may not be obvious at first glance why AWI (local optimization of
$J_{\rm AWI}$) should be should be any more effective at estimating
$\kappa$ from data $d$ than is FWI (local
optimization of $J_{\rm FWI}$). Under the smoothness and ray geometry assumptions made above (equation
\ref{eqn:coeff}, conditions G1 - G4), an asymptotic relation
between AWI and travel time tomography emerges, justifying the notion
that AWI is more likely to succeed than is FWI.
%In addition all of the
%previously mentioned assumptions, I will assume that the data $d$ is
%{\em noise-free}, that is, $d = F[\kappa^*]$ for a target bulk modulus
%$\kappa^* \in M$.

\subsection{Tihonov Regularization}
Before establishing the relation with travel time tomography, several
defects in the definition \ref{eqn:awideforig} of $J_{\rm AWI}$ must be
remedied. First, the objective function is not well-defined for the
``ideal'' case, in which $d = F[\kappa]$ and $u[\kappa;d]=\delta$:
this adaptive filter is not square-integrable, so the denominator does
not make sense. Second, the equation $S[\kappa]u=d$ does not (in
general) have a solution $u \in L^2(X_{sr} \times \bR)$. Since
$S[\kappa]$ is a convolution operator, solving $S[\kappa]u=d$ for $u$
amounts to deconovolving $d$ by $F[\kappa]$..  Under the conditions
W1-W3 on the wavelet $w$, $F[\kappa]$ does not have a continuous
(stable) convolution inverse, and $S[\kappa]$ is not surjective.

In fact, these two defects are really symptoms of the instability of
deconvolution for kernels with the properties of $F[\kappa]$.  To
begin with, $S[\kappa] = F[\kappa]*$ defines a bounded map on
$L^2(X_{sr}\times \bR)$, since from \ref{eqn:fwdgreen} and the definition of
  $S[\kappa]$,
\[
  S[\kappa]u = \rho( a[\kappa]\delta    +  (b[\kappa]H)) *\delta_{\tau[\kappa]}*\partial_t f. *u,
\]
The bounds \ref{eqn:best} yield $bH \in
  L^1(X_{sr}\times \bR)$, and property F1, $\partial_t f \in
  L^1(\bR)$.

The conventional remedy for instability in a deconvolution problem of
this type is so-called ``pre-whitening'', as
mentioned by \cite{Warner:16} ``Pre-whitening'' is a version of
Tihonov regularization. It  amounts to replacement of the equation
$S[\kappa]u=d$ by the regularized least squares problem: 
\begin{equation}
  \label{eqn:reg}
  u_{\sigma}[\kappa;d] = \mbox{argmin}_{u \in L^2(X_{sr} \times \bR)} (\|S[\kappa]u-d\|^2 + \sigma\|u\|^2)
\end{equation}
For $\sigma > 0$, the Lax-Milgram Theorem (\cite{Yosida}, section
III.7) implies that this least-squares problem has a unique solution
$u_{\sigma}[\kappa;d] \in L^2(X_{sr} \times \bR)$. Define the modified
AWI objective $J_{\rm AWI,\sigma}$ by
replacing $u$ in the definition \ref{eqn:awideforig} with $u_{\sigma}$:
\begin{equation}
  \label{eqn:awidef}
  J_{\rm AWI,\sigma}[\kappa;d] = \sum_{\bx_s,\bx_r} \left(\frac{\int dt |tu_{\sigma}[\kappa;d](\bx_r,t;\bx_s)|^2}{\int dt |u_{\sigma}[\kappa;d](\bx_r,t;\bx_s)|^2}\right). 
\end{equation}
To make this definition a bit more concise, introduce the operator $T$ on
${\cal D}'(X_{sr} \times \bR)$ by
\[
  Tp(\bx_r,t;\bx_s) = tp(\bx_r,t;\bx_s).
\]
Then
\begin{equation}
  \label{eqn:awidefbis}
  J_{\rm AWI,\sigma}[\kappa;d] = \sum_{(\bx_s,\bx_r) \in X_{sr}}
  \frac{\|Tu_{\sigma}[\kappa;d](\bx_r,\cdot;\bx_s)\|^2}{\|u_{\sigma}[\kappa;d](\bx_r,\cdot;\bx_s)\|^2}.
\end{equation}
Here $\| \cdot \|$ is the norm in $L^2(\bR)$.

Define a regularized version of the MSWI objective function $J_{\rm
  MSWI}$ (definition \ref{eqn:mswideforig}) similarly:
\begin{equation}
  \label{eqn:mswidef}
  J_{\rm MSWI,\sigma}[\kappa;d] = \|Tu_{\sigma}[\kappa;d]\|^2.
\end{equation}

This definition has introduced a new defect: while the denominator is
well-defined for $\log \kappa \in M$, $d \times L^2(X_{sr} \times
\bR)$ for $\sigma > 0$, the numerator may not be, since there is no
guarantee that $Tu_{\sigma}[\kappa;d] \in L^2(X_{sr} \times
\bR)$. 

\begin{prop}
  \label{thm:tl2}
  Suppose that $d, Td \in L^2(X_{sr} \times \bR)$. Then
  $Tu_{\sigma}[\kappa;d]  \in L^2(X_{sr} \times \bR)$.
\end{prop}

\begin{proof}
  Since $\kappa$ is the argument of $S$ and $u_{\sigma}$ throughout, drop it
  temporarily, also from the ingredients $\tau, a, $ and $b$ of the
  Hadamard decomposition.
  
  Thus
  \[
    S^TS =
    \rho^2(a\delta  +  (\check{b}\check{H})) *\delta_{-\tau}*\partial_t
    f * (a\delta  +  (bH)) *\delta_{\tau}*\partial_t f*
  \]
  \[
    = \rho^2((a\delta  +  (\check{b}\check{H}))*(a\delta  +
    (bH))*\partial_t f *\partial_t f*
  \]
  \[
    [T,S^TS] = \rho^2((T\check{b}\check{H}) *(a\delta  +
    (bH))*+ (a\delta  +  (\check{b}\check{H}))*(TbH))*\partial_t f
    *\partial_t f*
  \]
  \begin{equation}
    \label{eqn:tsscomm}
    +2\rho^2((a\delta  +  (\check{b}\check{H}))*(a\delta  +
    (bH)))* \partial_t f  *T\partial_t f*.
  \end{equation}
  Referring to the exponential decay assumption \ref{eqn:best}
  and properties F1, F2 of $f$, 
  observe that all summands on the RHS of identity \ref{eqn:tsscomm} are in
  $L^1(X_{sr} \times \bR)$, so $[T,S^TS]$ restricts to a bounded
  operator on $L^2(X_{sr}\times \bR)$. A similar computation shows the
  same property for $[T,S^T]$. Since $u_{\sigma} \in L^2(X_{sr}\times
  \bR)$,
  \[
    T(S^TS+\sigma I)u_{\sigma} =  [T,(S^TS+\sigma I)]u_{\sigma} +
    (S^TS+\sigma I)Tu_{\sigma}
  \]
  \[
    = [T,S^T]d + S^TTd,
  \]
  that is,
  \[
    (S^TS+\sigma I)Tu_{\sigma} = e \in L^2(X_{sr} \times \bR).
  \]
  A theorem of von Neumann (\cite{Yosida}, VII.3, Theorem 2) implies
  that $S^TS+\sigma I$ has a bounded inverse, so the distribution
  $Tu_{\sigma} \in L^2(X_{sr} \times \bR)$.
\end{proof}

The following fact will be useful later on:

\begin{prop}
  \label{thm:noisefree}
  Suppose that $\kappa \in M$ and $d = F[\kappa]$. Then $Td
  \in L^2(X_{sr} \times \bR)$.
\end{prop}

\begin{proof}
  A simple estimate shows that $F[\kappa]$ satisfies an exponential
  decay estimate of the form G4, whence the conclusion follows.
\end{proof}

{\bf Remark:} It is conventional to call data in the range of the
modeling operator ``noise-free'', and I will adopt this usage.

%The second defect is insufficient constrain on the recording time
%$t_d$: so far, $t_d$ can be arbitrarily small, and the recorded data
%can contain arbitrarily little information about the material through%
%which it propagates. The Hadamard decomposition motivates additional
%constraints on $t_d$ which (in some cases) imply that the recorded
%data embody strong constraints on the material model $m$.
%These conditions on $t_d$  are posed in terms of the extreme
%traveltimes over $m \in M$. Since all derivatives of the log wave velocity $\log c =
%(\log \kappa - \log \rho )/2$ are bounded in the closure of $\Omega$,
%uniformly over $m \in M$, 
%the ray trace construction of travel time implies that
%$0 < \tau_{\rm min} < \tau_{\rm max}$ exist for which $\tau_{\rm min}
%\le \tau[\kappa](\bx_s,\bx_r) \le \tau_{\rm max}$ for $(\bx_s,\bx_r) \in
%X_{sr}$, $m \in M$.
%The additional constraints on $t_d$ and $\phi$ are:
%begin{itemize}
%\item[T1. ] $t_d >  \tau_{\rm max} + t_w + t_u $ (recording time is
% sufficient for first arrival generated from any source with
%waveform $u*w$ to be recorded at all receivers);
%\item[T2. ] $0 < \tau_{\rm min} - t_w - t_u$ (signal from earliest
%  part of wavelet $u*w$ arrives in positive time, for any
%  source-receiver pair);
%\item[T3. ] $\phi \equiv 1$ on $[\tau_{\rm min} - t_w - t_u,
%\tau_{\rm max} + t_w + t_u]$ (time cutoff $\phi$ does not modify
%leading term in Hadamard decomposition \ref{eqn:fwdgreen}, for any
%source-receiver pair).
%\end{itemize}
%These assumptions are physically natural, as
%they imply that the recording time interval $[0,t_d]$ contains the
%support of the extended source data, up to an error vanishing with
%source wavelength.

%The choice of $\lambda$ as the  the regularization weight might seem
%natural, as $\lambda$ is more-or-less the shortest wavelength
%controlled by solution of the least-squares deconvolution problem
%\ref{eqn:reg}. It will be rigorously justified {\em post-hoc} by the
%calculations to follow. The same conclusions can be reached if
%$\sigma$ in the definition \ref{eqn:reg}  is replaced by a function $\sigma(\lambda)$ with
%the constraint that $C_{\rm min} \lambda \le \sigma(\lambda)
%\le C_{\rm max}\lambda$, for suitable $C_{\rm min}, C_{\rm max}$ with
%$0 < C_{\rm min} \le C_{\rm max}$. For simplicity, I will use
%$\sigma(\lambda) = \lambda$.

\subsection{Source Characteristics and Asymptotics}

Note that in the representation \ref{eqn:fwdgreen}, only the
derivative $\partial_tf$ appears. For convenience, introduce
\begin{equation}
  \label{eqn:wavelet}
  w = \rho \partial_t f
\end{equation}
The properties F1 and F2 assumed for $f$ are equivalent to these
properties of $w$:
\begin{itemize}
\item[W1. ] $w \in C^{\infty}_0(\bR)$;;
\item[W2. ] $w$ is even;
\item[W3. ] $\int \, w = 0$.
\end{itemize}
I will also
refer to $w$ as ``the wavelet'', and view $F$ as parametrized by $w$.

Introduce asymptotics in this setting by choosing $w = \wl$
from a family $\{\wl:0 <\lambda \le 1\}$ indexed by the dimensionless parameter
$\lambda$, and defined by
\begin{equation}
  \label{eqn:wfam}
  \wl(t) = \lambda^{-1/2}w_1(t/\lambda).
\end{equation}
The  ``mother wavelet'' $w_1$ is assumed to have properties W1 - W3, and
all wavelets $\wl$ in this family
inherit these properties from $w_1$, along with its dimensions
(energy/time$^2$). It follows from these assumptions that the Fourier transform $\fwl$ is
real-valued, even, and real-analytic. The corresponding source waveform $f_{\lambda}$ is
given by
\begin{equation}
  \label{eqn:flambda}
  f_{\lambda}(t) =\frac{1}{\rho} \int_{-\infty}^t\wl =
  \lambda^{1/2}f_1\left(\frac{t}{\lambda}\right).
\end{equation}
Note that for $p \ge 1$,
\begin{equation}
  \label{eqn:lambdanorms}
  \|\wl\|_{L^p(\bR)} = \lambda ^{\frac{1}{p}-\frac{1}{2}}\|w_1\|_{L^p(\bR)},\,\|f_{\lambda}\|_{L^p(\bR)}
  = \lambda ^{\frac{1}{p}+\frac{1}{2}} \|f_1\|_{L^p(\bR)}.
\end{equation}

%and its product with frequency $|\omega|$ is also
%square-integrable.

The parameter $\lambda$ is connected to several frequency-related
characteristics of $\wl$. Suppose that $f \in L^2(\bR)$ is also square-integrable after
multiplication by $t$ (application of $T$, in the notation of the last
section - true if for instance $f$ has compact
support). The {\em pulse width} (or {\em RMS width} ) of $f$ is
\begin{equation}
  \label{eqn:pw}
  l(f) = \frac{\|Tf\|}{\|f\|}.
\end{equation}
The pulse width of $\wl$ is proportional to $\lambda$:
\begin{equation}
  \label{eqn:wpw}
  l(\wl) = \left(\frac{\int dt (t \wl(t))^2}{\int dt (\wl(t))^2}
  \right)^{1/2} = \lambda l(w_1).
\end{equation}

Note that the AWI objective defined in \ref{eqn:awideforig} and its
modification \ref{eqn:awidef} are actually
squared pulse widths of the adaptive filters $u[\kappa;d]$ and
$u_{\sigma}[\kappa;d]$ respectively.

The {\em RMS frequency} of $\wl$ is
\begin{equation}
  \label{eqn:freq}
  k(\wl) = \left(\frac{\int d\omega (\omega \fwl(\omega))^2}{\int dt (w(t))^2} \right)^{1/2} = \lambda^{-1}k(w_1).
\end{equation}
Thus the {\em RMS wavelength} $1/k(\wl)$ is proportional to $\lambda$.

The Heisenberg inequality (for example, \cite{Folland:07}, p. 255) bounds the RMS
wavelength, the reciprocal of the RMS frequency, in terms of the pulse
width:
\begin{equation}
  \label{eqn:heis}
  l(\wl) k(\wl) = l(w_1)k(w_1) \ge \frac{1}{4\pi}.
\end{equation}
Equality is attained for Gaussian $w_1$, and approximated for many other
commonly used wavelets (Ricker = second derivative of Gaussian,
trapezoidal bandpass,...).

The modeling operator with wavelet $\wl$ is denoted $\Fl$. From the
decomposition \ref{eqn:fwdgreen}, $\Fl$ is the sum of two terms:
\begin{equation}
  \label{eqn:Flamdecomp}
  \Fl = \Fla + \Flb.
\end{equation}
where
\begin{equation}
  \label{eqn:Fl0}
  \Fla[\kappa]= a[\kappa]\delta_{\tau[\kappa]}*\wl
\end{equation}
and
\[
\Flb[\kappa] = (b[\kappa]H) *\delta_{\tau[\kappa]}*\wl.
\]
Write $b_0[\kappa](\bx_s,\bx_r) = b[\kappa](\bx_r,0;\bx_s)$. Then
$\Flb$ can be rearranged as
\begin{equation}
  \label{eqn:Fl1}
  =\rho (b_0[\kappa]\delta + (\partial_t b H) * f_{\lambda} )*\delta_{\tau[\kappa]}.
\end{equation}
From the bounds \ref{eqn:best} and \ref{eqn:lambdanorms} and Young's
inequality, there exists $C \ge 0$ so that for all $\log \kappa \in M$, $p
\ge 1$,
\begin{equation}
  \label{eqn:Frembd}
 \|\Fla[\kappa]\|_{ L^p(X_{sr} \times \bR)} \le C\lambda^{\frac{1}{p}-\frac{1}{2}},\,\|\Flb[\kappa]\|_{ L^p(X_{sr} \times \bR)} \le C\lambda^{\frac{1}{p}+\frac{1}{2}}
\end{equation}

Similarly, define
\begin{eqnarray}
  \label{eqn:sdeflam}
  \Sl[\kappa]u &=& u *\Fl[\kappa],  \nonumber \\
  \Sla[\kappa]u &=& u *\Fla[\kappa], \nonumber\\
  \Slb[\kappa]u &=& u *\Flb[\kappa], 
\end{eqnarray}
so that
\[
  \Sl[\kappa] = \Sla[\kappa] + \Slb[\kappa].
\]
From the bound \ref{eqn:Frembd} for $p=1$, there is $C \ge 0$ so that
for all $\log \kappa \in M$, $u \in L^2(X_{sr} \times \bR)$,
\begin{eqnarray}
  \label{eqn:Srembd}
  \|\Sl[\kappa]u\| &\le&  C\lambda^{\frac{1}{2}}\|u\|,\nonumber \\
  \|\Slb[\kappa]u\| & \le& C\lambda^{\frac{3}{2}}\|u\|.
\end{eqnarray}

\subsection{Linking Regularization and Wavelength}
In the remainder of this section, I will examine the $\lambda$-parametrized families of
objective functions obtained by
\begin{itemize}
\item[1. ]replacing $S$ and $d$ in the definitions \ref{eqn:awidefbis}
  and \ref{eqn:mswidef} with $\Sl$ and $\dl =\Fl[\kappa^*]$, that is,
  noise-free data generated by a choice of $\kappa^* \in M$ and
  $\lambda \in (0,1]$, and
\item[2. ] choosing the Tihonov regularization weight
  $\sigma = \lambda$.
\end{itemize}
That is,
\begin{equation}
  \label{eqn:mswideflam}
  J_{\rm MSWI,\lambda}[\kappa;\dl] =  \|T\ul[\kappa;d]\|^2
\end{equation}
and 
\begin{equation}
  \label{eqn:awideflam}
  J_{\rm AWI,\lambda}[\kappa;\dl] = \sum_{(\bx_s,\bx_r) \in X_{sr}} 
  \frac{\|T\ul[\kappa;d](\bx_r,\cdot;\bx_s)\|^2}{\|\ul[\kappa;d](\bx_r,\cdot;\bx_s)\|^2}.
\end{equation}
In both definitions, $\ul$ solves the least-squares problem \ref{eqn:reg} with
$\sigma = \lambda$.

Justification for choosing the regularization weight $\sigma$ to be the same as
the wavelength proxy $\lambda$ will emerge from the analysis relating these
objective functions to the conventional objective function of travel
time tomography. It will be clear that choosing $\sigma$ a function of
$\lambda$ roughly proportional to $\lambda$, that is, $C_*\lambda \le
\sigma(\lambda) \le C^*\lambda$ for $C_*,C^*$ satisfying $0 < C_* \le
C^*$, would lead to the same asymptotic estimates for the relation
between $J_{\rm MSWI,\lambda}$,$ J_{\rm AWI,\lambda} $ and travel time
error. The argument is slightly cleaner if simply $\sigma=\lambda$.

\subsection{The leading term and travel time}

The relation between MSWI, AWI, and travel time emerges most clearly
through a further simplification:

\begin{prop} 
In the definitions of $J_{\rm MSWI,\sigma}$ and $J_{\rm AWI,\sigma}$, replace the data $d$ with
$\dla = \Fla[\kappa^*]$, $S[\kappa]$ with $\Sla[\kappa]$. That is,
\begin{eqnarray}
  \label{eqn:leading}
  \dla & = & \Fla[\kappa^*] = a[\kappa^*]\delta_{\tau[\kappa^*]}*\wl
             \nonumber \\
  \Sla[\kappa]u &=& a[\kappa]\delta_{\tau[\kappa^*]}*\wl *u\nonumber \\
  \uzl &=& \mbox{argmin}_{u \in L^2(X_{sr} \times \bR)} (\|\Sla[\kappa]u-\dla\|^2 + \sigma\|u\|^2)
\end{eqnarray}
and
\begin{eqnarray}
  \label{eqn:awileading}
  J^0_{\rm MSWI,\lambda}[\kappa;\dla] & = &
                                         \|T\uzl[\kappa;\dla]\|^2\nonumber\\
  J^0_{\rm AWI,\lambda}[\kappa;\dla] & = &
                                       \sum_{(\bx_s,\bx_r) \in X_{sr}} \frac{\|T\uzl[\kappa;\dla](\bx_r,\cdot;\bx_s)\|^2}{\|\uzl[\kappa;\dla](\bx_r,\cdot;\bx_s)\|^2}
\end{eqnarray}                 
Then there exists $C>0$ so that for
$\kappa \in M$, $0 < \lambda \le 1$, 
\begin{equation}
  \label{eqn:mswitt}
 | J^0_{\rm MSWI,\lambda}[\kappa;\dla] - 
  \frac{1}{\lambda} \sum_{{\bf x}_s,{\bf x}_r} \|u^0_1[\kappa](\bx_r,\cdot;\bx_s)\|^2(\tau[\kappa^*]({\bf
  x}_s,{\bf x}_r)-\tau[\kappa]({\bf x}_s,{\bf x}_r))^2 | \le C\lambda,
\end{equation}
and
\begin{equation}
  \label{eqn:awitt}
|J^0_{\rm AWI,\lambda}[\kappa;\dla] - \sum_{{\bf x}_s,{\bf x}_r} (\tau[\kappa^*]({\bf
  x}_s,{\bf x}_r)-\tau[\kappa]({\bf x}_s,{\bf x}_r))^2|  \le C\lambda^2.
\end{equation}
\end{prop}

\begin{proof}
The abbreviations
$a^*=a[\kappa^*], a=a[\kappa], \tau^*=\tau[\kappa^*], \tau=\tau[\kappa]$.
will be convenient in the computations to follow.

The right-hand side of the third equation in \ref{eqn:leading} is then
\[
= \mbox{argmin}_u (\|a \wl*u*\delta_{\tau}-a^*\wl*\delta_{\tau^*}\|^2 + \lambda\|u\|^2)
\]
which in terms of the Fourier transforms $\fwl$, $\fu$ is
\begin{equation}
  \label{eqn:leadingft}
= \mbox{argmin}_u \frac{1}{2\pi}(\|a \fwl \fu e^{-i\omega \tau}-a^*\fwl e^{-i\omega \tau^*}\|^2 + \lambda\|\fu\|^2).
\end{equation}
The normal equation for the least squares problem \ref{eqn:leadingft} is
\[
(a^2 |\fwl|^2 +\lambda)\hat{u} = a a^*|\fwl|^2e^{i\omega(\tau^*-\tau)}
\]
the solution of which is
\begin{equation}
  \label{eqn:hatuf}
\hat{u}_{0,\lambda}[\kappa;\dla] = \frac{a^*}{a}\hat{g}_{\lambda,\frac{\lambda}{a^2}} e^{i\omega(\tau^*-\tau)}
\end{equation}
where
\begin{equation}
  \label{eqn:hatgf}
\hat{g}_{\lambda,\lambda} = \frac{|\fwl|^2}{|\fwl|^2 + \lambda}.
\end{equation}
Since $\wl$ vanishes for large $|t|$, $\fwl$ is entire analytic hence
vanishes on a set of measure zero, whence $\hat{g}_{\lambda,\lambda}:$
tends to $1$ almost everywhere as $\lambda \rightarrow 0$. Hence the
inverse Fourier transform $g_{\lambda,\lambda}$ tends to $\delta$ in
the sense of distributions, and
\begin{equation}
  \label{eqn:u0s}
\uzl[\kappa,\dla](t) = \frac{a^*}{a}g_{\lambda,\frac{\lambda}{a^2}}(t-(\tau^*-\tau))
\end{equation}
to a multiple of a shifted $\delta$, in fact exactly the
non-square-integrable distribution
solution of $\bar{\Sla}[\kappa]u=\dla$. However for $\lambda>0$, $\uzl[\kappa;\dla]$ is square integrable.

Since $\fwl$ along with all of its derivatives
tends to zero faster than any polynomial at $\infty$, $\fwl' \in
L^2(\bR)$.
Therefore $\hat{g}_{\lambda,\lambda}$
and $\hat{g}_{\lambda,\lambda}'$ are also square-integrable, 
from equation \ref{eqn:hatuf} $\hat{u}_{0,\lambda}'$ is also square
integrable. Consequently $t u_{0\lambda}$ is square integrableTherefore
$J_{\rm MSWI,\lambda,\lambda}$, the result of substituting in the
right-hand side of the definition \ref{eqn:mswidef} $\uzl$ for
$u_{\lambda}$, is well-defined. \cite{HuangSymes2015SEG} presented an analysis of
$J_{\rm MSWI,\lambda,\lambda}$, as follows:
\[
J_{\rm MSWI,\lambda,\lambda}[\kappa;\dla] = \sum_{\bx_s,\bx_r}\int dt
|t\uzl[\kappa;\dla]|^2 =  \sum_{\bx_s,\bx_r}\frac{(a^*)^2}{a^2} \int dt\, t^2|g_{\lambda,\frac{\lambda}{a^2}}(t-(\tau^*-\tau))|^2
\]
\[
=\sum_{\bx_s,\bx_r}\frac{(a^*)^2}{a^2} \int dt\, (t+(\tau^*-\tau))^2|g_{\lambda,\frac{\lambda}{a^2}}(t)|^2
\]
\begin{equation}
  \label{eqn:MSWI}
  =\sum_{\bx_s,\bx_r}\frac{(a^*)^2}{a^2} \int dt\, (t^2 + 2t
  (\tau^*-\tau)+(\tau^*-\tau)^2)|g_{\lambda,\frac{\lambda}{a^2}}(t)|^2
\end{equation}

Since the Fourier transform $\fwl$ is even and real, so is
$\hat{g}_{\lambda,\lambda}$, therefore $g_{\lambda,\lambda}$ is even, and the linear term (in t) on the
right-hand side of \ref{eqn:MSWI} vanishes, So
\begin{equation}
  \label{eqn:MSWIasym}
  J_{\rm MSWI,\lambda,\lambda}[\kappa;\dla] =
\sum_{\bx_s,\bx_r}\frac{(a^*)^2}{a^2} \int dt\, t^2|g_{\lambda,\frac{\lambda}{a^2}}(t)|^2
+(\tau^*-\tau)^2\frac{(a^*)^2}{a^2}\|g_{\lambda,\frac{\lambda}{a^2}}\|^2
\end{equation}

Recalling the definition \ref{eqn:pw} of pulse width, \ref{eqn:MSWIasym} can be re-written as
\begin{equation}
  \label{eqn:MSWIasym2}
  J_{\rm MSWI,\lambda,\lambda}[\kappa;\dla] = 
  \sum_{\bx_s,\bx_r}\frac{(a^*)^2}{a^2}\|g_{\lambda,\frac{\lambda}{a^2}}\|^2\left(l(g_{\lambda,\frac{\lambda}{a^2}})^2  + (\tau^*-\tau)^2\right)
\end{equation}

The second term suggests that this function
is related to the travel-time error $\tau^*-\tau$. 
The relation is obscured by the presence of of the amplitudes
$a^*, a$ both in the multiplier and in the scaling of $\lambda$. Since the
amplitudes depend on $\kappa$, their presence may influence the location
(or even the presence) of stationary points of $J_{\rm
  MSWI,\lambda,\lambda}$ (or $J_{\rm MSWI}$, to which it is an approximation)  for
models differing in their travel time predictions. So stationary
points of the MSWI objective may not yield slowness models that match
travel-times with the data, even in the noise-free case (though
the examples reported by \cite{HuangSymes2015SEG, HuangSymes:Geo17}, and other works
cited there, suggest that sometimes this is the case).

The trace-by-trace normalization in the definition of
the AWI objective removes the model-dependent factors. Denote by
$J_{\rm AWI, \lambda,\lambda}$ the restult of subtituting
$\uzl$ for $u_{\lambda}$ in the definition \ref{eqn:awidef} of
$J_{\rm AWI}$:
\begin{equation}
  \label{eqn:AWIasym}
J_{\rm AWI,\lambda,\lambda}[\kappa;\dla] = \sum_{\bx_s,\bx_r}\frac{\int dt
(|t\uzl[\kappa;\dla])^2}{\|\uzl[\kappa;\dla]\|^2} 
\end{equation}
We have already analyzed the numerator, arriving at
equation \ref{eqn:MSWIasym2}. From the result \ref{eqn:u0s}, for each
$(\bx_s,\bx_r) \in X_{sr}$,
\begin{equation}
  \label{eqn:u0snorm}
  \|\uzl[\kappa;\dla]\|^2=
  \frac{(a^*)^2}{a^2}\|g_{\lambda,\frac{\lambda}{a^2}}\|^2.
\end{equation}
Combine equations \ref{eqn:AWIasym} and \ref{eqn:u0snorm} to obtain
\begin{equation}
  \label{eqn:AWIasym2}
  J_{\rm AWI,\lambda,\lambda}[\kappa;\dla] = \sum_{\bx_s,\bx_r} \left(l(g_{\lambda,\frac{\lambda}{a^2}})^2  + (\tau^*-\tau)^2\right)
 \end{equation}

So the leading-term modification of $J_{\rm AWI}$ differs from mean square
travel time error by the pulse width of
$g_{\lambda,\frac{\lambda}{a^2}}$. The analogous relation for $_{\rm
  MSWI}$ involves the norm-squared as well. To assess these
quantities, note first that
\[
  \|g_{\lambda,\frac{\lambda}{a^2}}\|^2 =\frac{1}{2\pi} \int d\omega\,
  \left(\frac{|\fwl|^2(\omega)}{|\fwl|^2(\omega) + \frac{\lambda}{a^2}}\right)^2
=\frac{1}{2\pi} \int d\omega\,
  \left(\frac{|\lambda^{1/2}\hat{w}_1(\lambda
      \omega)|^2(\omega)}{|\lambda^{1/2}\hat{w}_1(\lambda
      \omega)|^2 + \frac{\lambda}{a^2}}\right)^2
\]
\[
=\frac{1}{2\pi}\int d\omega\,
  \left(\frac{|\hat{w}_1(\lambda
      \omega)|^2}{|\hat{w}_1(\lambda
      \omega)|^2 + \frac{\lambda}{a^2\lambda}}\right)^2
=  \frac{1}{2\pi \lambda} \int d\omega_1\,
  \left(\frac{|\hat{w}_1(\omega_1)|^2}{|\hat{w}_1(\omega_1)|^2 + \frac{\lambda}{a^2\lambda}}\right)^2
\]
At this point, it is possible to identify an advantageous choice of
the Tihonov penalty weight $\lambda$: choose
$\lambda = \lambda$  and rearrange to obtain
\begin{equation}
  \label{eqn:g0}
  \|g_{\lambda,\frac{\lambda}{a^2}}\|^2 \approx \frac{1}{\lambda} \int d\omega\,
  \left(\frac{|a\hat{w}_1|^2}{|a\hat{w}_1|^2 + 1}\right)^2
\end{equation}
That is, with this choice $\lambda = \lambda$, the wavelength proxy
$\lambda$ appears only in the factor in front of the integral sign.

Similarly,
\[
  \int dt\, t^2|g_{\lambda,\frac{\lambda}{a^2}}(t)|^2 = \frac{1}{2\pi} \int d\omega
  \left|\frac{d\hat{g}_{\lambda,\frac{\lambda}{a^2}}}{d\omega}\right|^2
  =  \int d\omega  \left(\frac{2 \mbox{Re }\bar{\hat{w}}_{\lambda}\fwl' 
    \frac{\lambda}{a^2}}{(|\fwl|^2 + \frac{\lambda}{a^2})^2}\right)^2
\]
\[
  =\frac{1}{2\pi}\left(\frac{\lambda}{a^2}\right)^2 \int d\omega \frac{(2 \lambda^{1/2}\hat{w}_1(\lambda
    \omega)\lambda^{3/2}\hat{w}_1'(\lambda \omega))^2}{(\lambda|\hat{w}_1(\lambda
    \omega)|^2 + \frac{\lambda}{a^2})^4}
\]
\[
  =\frac{1}{2\pi\lambda}\left(\frac{\lambda}{a^2}\right)^2\int d\omega_1 \frac{(2 \lambda^2\hat{w}_1(\omega_1)\hat{w}_1'( \omega_1))^2}{(\lambda|\hat{w}_1(
    \omega_1)|^2 + \frac{\lambda}{a^2})^4}
  =\frac{1}{2 \pi \lambda}\left(\frac{\lambda}{a^2}\right)^2\int d\omega_1
  \frac{(2\hat{w}_1(\omega_1)\hat{w}_1'( \omega_1)
    )^2}{\left(|\hat{w}_1(\omega_1)|^2 + \frac{\lambda}{a^2\lambda}\right)^4}
\]
With the choice $\lambda = \lambda$, this becomes
\[
  =\frac{\lambda}{2\pi a^4}\int d\omega_1
  \frac{(2\hat{w}_1(\omega_1)\hat{w}_1'( \omega_1)
    )^2}{\left(|\hat{w}_1(\omega_1)|^2 + \frac{1}{a^2}\right)^4}
\]
\begin{equation}
  \label{eqn:g1}
 = \frac{\lambda}{2\pi} \int d\omega
  \frac{(2a\hat{w}_1a\hat{w}_1'
    )^2}{\left(|a\hat{w}_1|^2 + 1\right)^4}
\end{equation}

Combining the relations \ref{eqn:g0} and \ref{eqn:g1}, obtain
\begin{equation}
  \label{eqn:pwg}
  l(g_{\lambda,\frac{\lambda}{a^2}}) =\lambda W(m,w_1)
\end{equation}
in which $W(m,w_1)$ is bounded by the variation of the amplitude
$a[\kappa](\bx_r,\bx_s)$, hence uniformly for $\log \kappa \in M$ (property
H2), and the choice of ``mother wavelet'' $w_1$, but
independent of $\lambda$.

It follows that
\begin{equation}
  \label{eqn:mswitt}
  J_{\rm MSWI,\lambda,\lambda}[\kappa;\dla] =
  \frac{1}{\lambda} \sum_{{\bf x}_s,{\bf x}_r} C[\kappa](\bx_s,\bx_r)(\tau[\kappa^*]({\bf
  x}_s,{\bf x}_r)-\tau[\kappa]({\bf x}_s,{\bf x}_r))^2 + O(\lambda)
\end{equation}
  
\begin{equation}
  \label{eqn:awitt}
J_{\rm AWI,\lambda,\lambda}[\kappa;\dla] = \sum_{{\bf x}_s,{\bf x}_r} (\tau[\kappa^*]({\bf
  x}_s,{\bf x}_r)-\tau[\kappa]({\bf x}_s,{\bf x}_r))^2 + O(\lambda^2)
\end{equation}

\end{proof}

In particular, the modified AWI objective $J_{\rm AWI,\lambda}$ is {\em exactly} an approximation to
the travel-time tomography objective, with errors proportional to the squared
wavelength proxy $\lambda$.

\subsection{Estimating the Remainder}



%And that's ``why it works'' - when geometric asymptotics in the form
%given by equation \ref{eqn:hadamard} is an accurate approximation to
%the Green's function.

To evaluate the difference between $J_{\rm AWI}$ and
$J_{\rm AWI,\lambda}$, first estimate the norms of differences
$\|u_{\lambda} - \uzl\|$ and $\|T(u_{\lambda} -
\uzl)\|$.

Suppressing $ \bx_s,\bx_r$ and using the notation of the last
subsection, definitions \ref{eqn:Fl0}, \ref{eqn:Fl1} and
\ref{eqn:sdeflam} can be re-written
\[
  \Sl u = (a \wl*u + (bH) * \wl *u)(t-\tau)
\]
where $bH$ is an abbreviation for
$b[\kappa](\bx_r,t-\tau[\kappa];\bx_s)H(t-\tau[\kappa])$. Note that
$b$ decays exponetially, is smooth, and all derivatives have limits
at $t=0$. Similarly,
\[
  d = \Fl[\kappa^*] = (a^*\wl + (bH)  * \wl) (t-\tau).
\]
Recall (equations \ref{eqn:Fl0}, \ref{eqn:Fl1}, re-written as
\[
\Fla = a \wl * \delta_{\tau}, \,   \Flb  = (f_{\lambda}b(0) -
f_{\lambda}*(bH))*\delta_{\tau}.
\]
Since $S_{\lambda,i} u = F_{\lambda,i} * u, \,i=0,1,$ $\wl$ is even,
and $f_{\lambda}$ is odd,
\[
 ((\Sla)^T \Slb + (\Slb)^T\Sla)u = [\rho a b(0)( \check{\wl} *\delta_{-\tau}*f_{\lambda}*\delta_{\tau} + \wl
 *\delta_{\tau}*\check{f}_{\lambda}*\delta_{-\tau}) 
\]
\[
-
 \rho a(\check{\wl}*\delta_{-\tau} *f_{\lambda}*(bH)*\delta_{\tau} + \wl*\delta_{\tau}*\check{f}_{\lambda}*\check{b}\check{H)}*\delta_{-\tau}) ]*u
\]
\[
  = \rho a(\wl*f_{\lambda}*(\check{b}\check{H}-(bH)) *u
\]
Since $\wl = \rho \partial_t f_{\lambda}$ and $(\partial_t
(\check{b}\check{H}-bH))*u = (-2\partial_tb(0)u + (\partial_t\check{b}\check{H} -\partial_tb H))*u$, this is
\begin{equation}
  \label{eqn:symmprods0s1}
  = \rho^2 a(-2\partial_tb(0) \delta + (\partial_t\check{b}\check{H} -
  \partial_tb H)) * f_{\lambda}*f_{\lambda}*u_{\lambda}
\end{equation}
The bounds \ref{eqn:best}, \ref{eqn:lambdanorms} imply that
\begin{equation}
  \label{eqn:s0s1norm}
  \|(\Sla)^T\Slb + (\Slb)^T\Sla\| \le C \lambda^3,
\end{equation}
with $C$ uniform in $\log \kappa \in M$. From \ref{eqn:Srembd},
\begin{equation}
  \label{eqn:noperr}
  \|\Sl^T\Sl - (\Sla)^T\Sla\| \le C \lambda^3.
\end{equation}
A little algebra yields
\begin{equation}
  \label{eqn:step1}
  ((\Sla)^T\Sla + \lambda I) (u_{\lambda}-\uzl) = (\Slb)^T d_{\lambda} + (\Sla)^T(d_{\lambda}-d_{0,\lambda})- 
  ((\Sla)^T\Slb + (\Slb)^T\Sla + (\Slb)^T\Slb)u_{\lambda}
\end{equation}
Note that the bounds \ref{eqn:Srembd} imply
\[
  \|\Sl^T d\| \le C \lambda^{\frac{1}{2}}\|d\|,
\]
\[
  \|(\Sla)^T (d-d_0)\| = \|(\Sla)^T(f_{\lambda}b^*(0) -
f_{\lambda}*(b^*H)*\delta_{\tau^*}\| \le C \lambda^2,
\]
and
\[
 \| (\Slb)^Td \| \le C \lambda^{3/2}.
\]
Hence
\[
  \|u_{\lambda}\| \le C\lambda^{-\frac{1}{2}}\|d\|.
\]
Combine the preceding three bounds with identity \ref{eqn:step1} and
bound \ref{eqn:noperr} to obtain
\begin{equation}
  \label{eqn:step2}
  \|u_{\lambda}-\uzl\| \le C
  \lambda^{\frac{1}{2}}\|d_{\lambda}\|
\end{equation}

From identity \ref{eqn:step1},
\[
   ((\Sla)^T\Sla + \lambda I) T(\ul-\uzl) = -[T,
   (\Sla)^T\Sla](\ul-\uzl) - T \Slb)^T \dl 
 \]
\begin{equation}
  \label{eqn:step3}
 + T (\Sla)^T(\dl-\dzl)- 
  T((\Sla)^T\Slb + (\Slb)^T\Sla + (\Slb)^T\Slb)\ul)
\end{equation}

To estimate the first term on the right-hand side, note that
\[
  [T,\Sla] = a \tau \delta_{\tau} * \wl* + a \delta_{\tau} * (T\wl)*
\]
and
\[
  [T,(\Sla)^T] = -a \tau \delta_{-\tau} * \wl* + a 
  \delta_{-\tau}*(T\wl)*,
\]
so
\[
  [T,  (\Sla)^T\Sla] = 2 a^2 \wl*T\wl*.
\]
Since
\begin{equation}
  \label{eqn:twl}
  \|T\wl\|_{L^1(X_{sr}\times \bR)} =
  \|\partial_t(T\fl)-\fl\|_{L^1(\bR)} =\lambda^{\frac{3}{2}}\int dt_1
  |t_1||\partial f_1(t_1)| \le C \lambda^{\frac{3}{2}}
\end{equation}
the estimate \ref{eqn:step2} implies
\begin{equation}
  \label{step5}
  \|[T,  (\Sla)^T\Sla](\ul-\uzl)\| \le C \lambda^{2}\|\dl\|,
\end{equation}

For the second term, compute
\[
  [T, \Slb] = [T,bH*\delta_{\tau}*\wl*] = ((T+\tau)bH*\wl
  + bH*T\wl)*\delta_{\tau}
\]
so
\[
  [T,(\Slb)^T]  =  ((T - \tau) \check{b}\check{H})*\wl
  + \check{b}\check{H}*T\wl)*\delta_{-\tau}
\]
\[
= \check{b}\check{H}*\fl
-\tau)b(0)\delta *\fl + (T-\tau)
\partial_t\check{b}\check{h}*\fl +
\check{b}\check{H}*(\partial_t(T\fl)-\fl)
\]
Using \ref{eqn:twl}, conclude that
\begin{equation}
  \label{eqn:step6}
  [T, (\Slb)^T] \le C \lambda^{\frac{3}{2}}
\end{equation}

Observe that
\[
  T\dl=T(a^*\delta + b^*H)*\delta_{\tau^*}*\wl
\]
\[
  =Tb^*H *\delta_{\tau^*}*\wl+(a^*\delta+b^*H)\tau^*\delta_{\tau^*}\wl
  +(a^*\delta + b^*H)*\delta_{\tau^*}*T\wl,
\]
so the exponential decay of $b^*$ implies that $\|T\dl\| \le
C\|\dl\|$. Thus the estimate \ref{eqn:step6} implies
\begin{equation}
  \label{eqn:step7}
  \|T(\Slb)^T\dl\| \le C \lambda^{3/2}\|\dl\|.
\end{equation}
                
  

%%%%%%%%%%%%%%%

$(Tu)(t) = tu(t)$

$T^T = T$

$[T,\delta_{\tau}*] = \tau \delta_{\tau}*$

$[T,k *] = (Tk)*$ for any $k \in {\cal S}'(\bR)$

\[
  [T,\Sla] = a \tau \delta_{\tau} * \wl* + a \delta_{\tau} * (T\wl)*
\]
\[
  [T,(\Sla)^T] = -a \tau \delta_{-\tau} * \wl* + a \delta_{-\tau}*(T\wl) *
\]
\[
  [T,\Sl] = a (\tau  \wl +  (T\wl) ) \delta_{\tau} + [T,(bH)*\delta_{\tau}*\wl]
\]
\[
  =a (\tau  \wl +  (T\wl) ) \delta_{\tau}* + (Tb)H *\delta_{\tau}*\wl* +
  (bH)*(\tau \wl + T\wl)*\delta_{\tau}*
\]
\[
  = (a\delta + (bH))*(\tau \wl +
  T\wl)*\delta_{\tau}* + (Tb)H *\delta_{\tau}*\wl*
\]
\[
[T,\Sl^T] =   = (a\delta + (\check{b}\check{H}))*(-\tau \wl +
  T\wl)*\delta_{-\tau}* + (T\check{b})\check{H} *\delta_{-\tau}*\wl*
\]

\[
  [T,(\Sla)^T\Sla] = [T,(\Sla)^T]\Sla + (\Sla)^T [T,\Sla]
\]
\[
= (-a \tau \delta_{-\tau} * \wl + a \delta_{-\tau}*(T\wl)) *
a\delta_{\tau} * \wl
\]
\[
  + a \delta_{-\tau} * \wl *  ( a \tau \delta_{\tau} * \wl + a
  \delta_{\tau} * (T\wl))
\]
\[
  = 2 a^2 \wl * T\wl *
\]

\[
  [T,\Sl^T\Sl] = [T,\Sl^T]\Sl + \Sl^T [T,\Sl]
\]
\[
  =((a\delta + (\check{b}\check{H}))*(-\tau \wl +
  T\wl)*\delta_{-\tau}* + (T\check{b})\check{H}
  *\delta_{-\tau}*\wl*)(a \delta + (bH))*\wl*\delta_{\tau}
\]
\[
  +(a \delta + (\check{b}\check{H}))*\wl*\delta_{-\tau}*
  ((a\delta + (bH))*(\tau \wl +
  T\wl)*\delta_{\tau}* + (Tb)H *\delta_{\tau}*\wl*)
\]
\[
=(a\delta + (\check{b}\check{H}))*(-\tau \wl +
T\wl)*\delta_{-\tau}*(a \delta + (bH))*\wl*\delta_{\tau} +
\]
\[
  (a \delta + (\check{b}\check{H}))*\wl*\delta_{-\tau}*(a\delta + (bH))*(\tau \wl +
  T\wl)*\delta_{\tau}*
\]
\[
  ...
\]
\[
  = 2(a \delta + (\check{b}\check{H}))*(a\delta + (bH)*\wl*T\wl +
\]

From identity \ref{eqn:symmprods0s1},
\[
  [T,(\Sla)^T\Slb+(\Slb)^T\Sla] = \rho^2 a[T,(-2\partial_t b(0) \delta +
  (\partial_t\check{b}\check{H}-\partial_tb
  H))]*f_{\lambda}*f_{\lambda}*
\]
\[
  + 2\rho^2 a (-2\partial_t b(0) \delta +
  (\partial_t\check{b}\check{H}-\partial_tb
  H))]*f_{\lambda}*Tf_{\lambda}*
\]
\[
  =
  \rho^2 a[ T (\partial_t\check{b}\check{H}-\partial_tb
  H))]*f_{\lambda}*f_{\lambda}* +  2\rho^4 a (-2\partial_t b(0) \delta +
  (\partial_t\check{b}\check{H}-\partial_tb
  H))]*f_{\lambda}*Tf_{\lambda}*
\]
%\[
%  [T,\Slb] = ((Tb)H) *\wl * \delta_{\tau}* + (bH) * T\wl *
%  \delta{\tau} * + \tau (bH)* \wl * \delta_{\tau}
%\]
%\[
%[T,(\Slb)^T] = - [T,\Slb]^T =-(\checi((Tb)\check{H}*\wl*\delta{-\tau} +
%(\check{b}\check{H}) * (-Twl)*\delta{-\tau}^* + \tau
%(\check{b}\check{H))*\wl*\delta_{\-tau}]
%\]
%\begin{equation}
%  \label{eqn:step3}
%  ((\Sla)^T\Sla + \lambda I) (u_{\lambda}-\uzl) = (\Slb)^T d_{\lambda} + (\Sla)^T(d_{\lambda}-d_{0,\lambda})- 
%  ((\Sla)^T\Slb + (\Slb)^T\Sla + (\Slb)^T\Slb)u_{\lambda}
%\end{equation}





\section{Relation to Penalty Formulation}
Penalty functions provide a means to convert naively posed
inverse problems into well-posed problems, with solutions that exist
and depend stably on proper data. The ``pre-whitening'' construction
explained in the last section is a classic example, adding a scaled
square-integral of the adaptive filter to the residual square-integral to
produce an objective with a square-integrable (finite-energy)
minimizer. This penalty construction if often called {\em Tihonov
  regularization}. The MSWI and AWI objective functions have a deeper
relation to a penalty construction, however: they are
suitably defined limits of penalty functions.

This relation is abstract, and does not just pertain to the problems
discussed in this paper. To derive this abstract relation, suppose for
the moment that $S:U \rightarrow D$ is a bounded and coercive from a
domain Hilbert space $U$ to a range Hilbert space $D$. That is, for
real numbers $0 < C_* \le C^*$,
\[
  C_*\|u\|_U \le \|Su\|_D \le C^*\|u\|
\]
Suppose that $d \in D$ (``data''), and $T$ is another bounded operator $U \rightarrow E$, $E$
another Hilbert space. Define
\begin{equation}
  \label{eqn:eq1}
  J_{\alpha}(u) = \frac{1}{2}(\|Su-d\|_D + \alpha^2\|Tu\|_E^2).
\end{equation}
and
\begin{equation}
  \label{eqn:eq0}
  \tJa= \min_u J_{\alpha}(u) = J_{\alpha}(u_{\alpha})
\end{equation}
Since $S$ is coercive, the minimum is well-defined for any
$\alpha \ge 0$, and the minimizer $\ua \in U$ satisfies the normal
equation
\begin{equation}
  \label{eqn:eq0n}
  (S^TS + \alpha^2 T^TT) \ua = S^Td
\end{equation}

The claim to be established is that
\begin{equation}
  \label{eqn:eq4}
  \lim_{\alpha \rightarrow 0} \frac{1}{\alpha^2}  (\tJa-\tJz).= \frac{1}{2}\|T\uz\|^2
\end{equation}

To see this, use the normal equation to write
\[
  \ua = (S^TS + \alpha^2 T^TT)^{-1}T^Td = (S^TS + \alpha^2 T^TT)^{-1}S^TS \uz
\]
\begin{equation}
  \label{eqn:eq2}
  = \uz - \alpha^2 (S^TS + \alpha^2 T^TT)^{-1}T^TT\uz = \uz-\alpha^2 \va
\end{equation}
Note that $\va = (S^TS + \alpha^2 T^TT)^{-1}T^TT\uz$ is uniformly bounded in $\alpha \ge 0$.

Substitute the RHS of equation \ref{eqn:eq2} into the definition \ref{eqn:eq1} to obtain
\begin{equation}
  \label{eqn:eq3}
  \tJa = \frac{1}{2}(\|S\uz-d\|_d^2 - 2 \alpha^2\langle S\uz-d, S\va\rangle_d + \alpha^2\|T\uz\|_m^2 + O(\alpha^4))
\end{equation}
The second term on the RHS of equation \ref{eqn:eq3} vanishes thanks to the normal equation, and the first term is precisely $\tJz$. The conclusion \ref{eqn:eq4} follows.

To apply the assertion \ref{eqn:eq4} to the MSWI objective, make the
following choices, for each $m$ in the domain of $F$: $S$ is the regularized
modeling operator $(S_{\lambda}[\kappa],\lambda I)^T$.  Its domain is $U=L^2(\bR)$, of
which $u$ (the adaptive filter) is a member, and 
its range is $D = L^2(\bR) \oplus
L^2(\bR)$. The role of $d$ is played  the
augmented data $(d,0)^T$. $T$ is a modified multiply-by-$t$ operator
$Tu(t)=\max(|t|, t_d) \mbox{sgn}(t)u(t)$, with domain $U$ and
range $E=L^2(\bR)$. The truncation by $t_d > 0$ is necessary
to make $T$ bounded, but can be chosen large enough relative to the
support of the filter (necessarily bounded in practical calculation)
to have no effect. Then $\Ja$ becomes $J_{{\rm MSWI},\alpha,\sigma}[\kappa,u;d]$, where
\begin{equation}
  \label{eqn:eq5}
   J_{{\rm MSWI},\alpha,\sigma}[\kappa,u;d] = \frac{1}{2}\left(\|S_{\lambda}[\kappa]u-d\|^2 +
   \lambda\|u\|^2 + \alpha^2\|Tu\|^2\right)
 \end{equation}
The minimizer $\ua$ over $u$ of $ J_{{\rm MSWI}, \alpha,\sigma}[\kappa,u;d]$ is
$u_{{\rm MSWI},\alpha,\sigma}[\kappa;d]$, and $\tJa$ becomes
\begin{equation}
  \label{eqn:eq6}
  \tilde{J}_{{\rm MSWI},\alpha,\sigma}[\kappa;d] =
  J_{{\rm MSWI},\alpha,\sigma}[\kappa,u_{{\rm MSWI},\alpha,\sigma}[\kappa;d];d].
\end{equation}
Note that $u_0$ becomes $u_0[\kappa_{\lambda};d]$, and $\tilde{J}_0$ becomes
$\tilde{J}_{0,\sigma}[\kappa;d]$ which means exactly what
it did in the previous section. The subscript ${\rm MSWI}$ is left
off for $\alpha=0$, for reasons that will become apparent shortly.

With these correspondences, equation \ref{eqn:eq4} becomes
\begin{equation}
  \label{eqn:eq7}
  \lim_{\alpha \rightarrow 0}
  \frac{1}{\alpha^2}\left(\tilde{J}_{{\rm MSWI},\alpha,\sigma}[\kappa;d]-\tilde{J}_{0,\sigma}[\kappa;d]\right)=
  J_{\rm MSWI}[\kappa;d]
\end{equation}

To derive the analogous relation for AWI only requires re-definition
of the operator $T$: it becomes (taking explicitly into account
dependence on source and receiver coordinates)
\begin{equation}
  \label{eqn:eq8}
  T[\kappa;d] u(\bx_r,t;\bx_s) = \frac{\max(|t|, t_d)
    \mbox{sgn}(t)}{\|u_0[\kappa_{\lambda};d](\bx_r,\cdot;\bx_s)\|}
  u(\bx_r,t;\bx_s).
\end{equation}
That is, for AWI, $T$ depends on both model and data. Define
\begin{equation}
  \label{eqn:eq5a}
   J_{{\rm AWI},\alpha,\sigma}[\kappa,u;d] = \frac{1}{2}\left(\|S_{\lambda}[\kappa]u-d\|^2 +
   \lambda\|u\|^2 + \alpha^2\|T[\kappa;d]u\|^2\right)
 \end{equation}
The minimizer $\ua$ over $u$ of $ J_{{\rm AWI}, \alpha,\sigma}[\kappa,u;d]$ is
$u_{{\rm AWI},\alpha,\sigma}[\kappa;d]$, and $\tJa$ becomes
\begin{equation}
  \label{eqn:eq6a}
  \tilde{J}_{{\rm AWI},\alpha,\sigma}[\kappa;d] =
  J_{{\rm AWI};\alpha,\sigma}[\kappa,u_{{\rm AWI};\alpha,\sigma}[\kappa;d];d].
\end{equation}
Note that the definitions \ref{eqn:eq5} and \ref{eqn:eq6} are exactly
the same as the corresponding definition \ref{eqn:eq5a} and
\ref{eqn:eq6a} when $\sigma=0$, which is why the subscripts ${\rm
  MSWI}$ and ${\rm AWI}$ are left off in these cases.

With these choices, equation \ref{eqn:eq4} becomes 
\begin{equation}
  \label{eqn:eq9}
  \lim_{\alpha \rightarrow 0}
  \frac{1}{\alpha^2}\left(\tilde{J}_{{\rm AWI},\alpha,\sigma}[\kappa;d]-\tilde{J}_{0,\sigma}[\kappa;d]\right)=
  J_{\rm AWI}[\kappa;d]
\end{equation}

The intervention of the $\alpha=0$ limit $\tilde{J}_{0,\sigma}$ in the
relations \ref{eqn:eq7} and \ref{eqn:eq9} obscures the relation
between the MSWI and AWI objective functions and their penalty
counterparts. In fact, under the hypotheses used in deriving he
relations 
Recall that the wavelet $w$ is assumed essentially bandlimited, that is
$|\hat{w}(\omega)| \rightarrow 0$ as $\omega \rightarrow \infty$
faster than any negative power of $\omega$. It follows that $\hat{w}
\in L^p(\bR)$ for any $p \ge 1$ (and in fact the same is true of
$\hat{w}$ multiplied by any polynomial in $\omega$). 

Assume as in the last section, that is, that solutions
of the acoustic system \ref{eqn:awe} is well-approximated by the
geometric asymptotics as in equation \ref{eqn:hadamard}, and that the
data is noise-free ($d=F[\kappa^*]$). Together with the properties of $w$
just listed, these assumptions imply that
\begin{equation}
  \label{eqn:eq11}
  J_{0,\sigma}[\kappa;d] = O(\sigma) + O(\lambda(w)).
\end{equation}
To see this, begin with the definition:
\[
  \tilde{J}_{0,\sigma}[\kappa;d] = \frac{1}{2}
  (\|S_{\lambda}[\kappa]u_0[\kappa_{\lambda};d]-d\|^2 + \lambda \|u_0[\kappa_{\lambda};d]\|^2.
\]
Using the approximate identities \ref{eqn:hatuf} and \ref{eqn:hatgf} and
rearranging,
\[
\|S_{\lambda}[\kappa]u_0[\kappa_{\lambda};d]-d\|^2 
  \approx \int d\omega
  |a\hat{w}\hat{u_0[\kappa_{\lambda};d]}e^{i\omega\tau}-a^*\hat{w}e^{i\omega\tau^*}|^2
\]
\[
  = \int d\omega
  (a^*)^2|\hat{w}|^2\left(\frac{\lambda}{a^2|\hat{w}|^2 +
      \lambda}\right)^2   
\le \int d\omega (a^*)^2|\hat{w}|^2
  \left(\frac{\lambda}{a^2|\hat{w}|^2 + \lambda}\right)
\]
\begin{equation}
  \label{eqn:eq12}
  = \lim_{\Omega \rightarrow \infty} \int_{-\Omega}^{\Omega} d\omega \left(\frac{a^*}{a}\right)^2\lambda
  \left(\frac{a^2|\hat{w}|^2}{a^2|\hat{w}|^2 + \lambda}\right)
\end{equation}
For any $\Omega > 0$,
\[
\int_{-\Omega}^{\Omega} d\omega \left(\frac{a^*}{a}\right)^2\lambda\left(\frac{a^2|\hat{w}|^2}{a^2|\hat{w}|^2 + \lambda}\right)
\le \int_{-\Omega}^{\Omega} d\omega \left(\frac{a^*}{a}\right)^2 \lambda
  \left(\frac{ a|\hat{w}|}{(a^2|\hat{w}|^2 +
      \lambda)^{\frac{1}{2}}}\right)
\]
\[
  = \sigma \int_{-\Omega}^{\Omega} d\omega \left(\frac{a^*}{a}\right)^2 a|\hat{w}|
  \left(\frac{\sigma }{(a^2|\hat{w}|^2 +\lambda)^{\frac{1}{2}}}\right)
  \le \sigma \int_{-\Omega}^{\Omega} d\omega \frac{(a^*)^2}{a}
  |\hat{w}|
  \le \sigma \frac{(a^*)^2}{a} \|\hat{w}\|_{L^1(\bR)}.
\]
Together with the approximate inequality \ref{eqn:eq12}, this
inequality implies
that
\[
  \|S_{\lambda}[\kappa]u_0[\kappa_{\lambda};d] - d\|^2 = O(\sigma) + O(\lambda(w)).
\]
A similar argument leads to the conclusion that
\[
  \lambda\|u_0[\kappa_{\lambda};d]\|^2 = O(\sigma) + O(\lambda(w)).
\]
Together, these conclusions imply the assertion \ref{eqn:eq11}.


\section{Multiple Arrivals and Cycle-Skipping}
The geometric asymptotics approximation \ref{eqn:hadamard} holds only
in the ``single arrival'' case, that is, when sources and receivers
are close enough together relative to the length scale of significant
variation in the wave velocity $((\kappa\beta)^{1/2})$ that a unique ray of
geometric acoustics connects each source-receiver pair. At larger
distances, multiple raypaths typically exist, corresponding to
multiple idenfiable wavefront arrivals. Accordingly, in such settings
the analysis presented in the last section no longer applies, and the
relation established there between AWI and travel-time inversion is
case into doubt.

In fact, the behaviour inherited from travel-time inversion in the
single arrival case, namely the absence of spurious stationary points
(``cycle-skipping''), generally fails when multiple arrivals with
significant energy are present in the data. \cite{Symes:94c}
illustrated this phenomenon for a version of MSWI. Crosswell
source-receiver geometry often generates multiple arrivals,
particularly near low-velocity sedimentary layers that are natural
targets of seismic exploration and act as wave
guides. \cite{Plessix:00} successfully applied a version of MSWI to
crosswell waveform inversion by removing traces with close source and
receiver depths, thus leaving only data that tended to be dominated by
single arrivals. \cite{HuangSymes:Geo17} illustrate the failure of the
version of MSWI reported here to avoid cycle-skipping in inversion of
strongly refracting models, and ameliorate the this failure to some
extent by Tihonov regularization. They observe that the mapping from
adaptive filter or extended source to data (represented by the opertor
$S_{\lambda}[\kappa]$ in the discussion above) may not have a well-behaved inverse in
the presence of multiple wavefront arrivals. \cite{Yongetal:GJI23}
have shown that precisely the same issue afflicts AWI, and have
proposed a remedy by localizing the definition of the objective to
short time intervals, within which multiple wavefront arrrivals are
unlikely. \cite{Symes:23} constructs an extension based on surface
sources, as opposed to the source-receiver extension which is the
basis of MSWI and AWI. This surface source extension generates an
invertible map from extended source to data.

A simple calculation reveals the phenomenon discussed in all of these
works. The analogue of the geometric asymptotics for a source-receiver
pair with connected by multiple rays is (once again suppressing the
source and receiver coordinates from the notation):
\begin{equation}
  \label{eqn:multi}
  F[\kappa,w](t) \approx \sum_{i \ge 0} a_i H^{p_i}w(t-\tau_i).
\end{equation}
The amplitudes $\{a_i\}$ and arrival times $\{\tau_i\}$ depend on $m$, as
before, through ray-tracing. $H$ is the Hilbert transform, and $p_i$
counts the number of times the ray for arrival time $\tau_i$ has
touched a caustic (counted with multiplicity, in an appropriate
sense). The earliest arrival has not touched a caustic. If the
indexing is organized so that $\tau_0 < \tau_i$ for $i>0$, then
$p_0=0$. See \cite{Friedlander:75} for details.

The right-hand side of equation \ref{eqn:multi} can be expressed as
\[
  = (\delta + T) * a_0w(t-\tau_0),
\]
in which 
\[
  T = \sum_{i \ge 1}
  \frac{a_i}{a_0}H^{p_i}\delta(t-(\tau_i-\tau_0))
\]
Thus
\begin{equation}
  \label{eqn:unwrap}
  (\delta +T_1) * F[\kappa,w] \approx  a_0w(t-\tau_0)
\end{equation}
where $\delta + T_1$ is the convolution inverse of $\delta + T$,
defined by 
\[
  T_1 = \sum_{n=1}^{\infty} (-1)^n T^n.
\]
$T^n$ should be understood as the $n$th convolution power in this
expression, that is, $T*T*...*T$, $n$ times. $T_1$ is well-defined as a distribution.

Once again, it is possible to write the adaptive filter u for which $S_{\lambda}[\kappa]u = F[\kappa]*u = d =
F[\kappa^*]$ by inspection. Use the notations $a^*_i, t^*_i, p^*_i$ for
the analogous quantities computed with the ``true'' model $\kappa^*$, and
$T^*, T_1^*$ defined as above but with $a^*_i$ in place of $a_i$ and so
on. From the observation \ref{eqn:notl2},
\[
  d = F[\kappa_*]= S_{\lambda}[\kappa] u
\]
provided that we choose
\begin{equation}
  \label{eqn:notl2again}
u(t) \approx (\delta + T^*)*(\delta +
T_1)*\left(\frac{a^*_0}{a_0}\delta(t - (\tau^*_0-\tau_0))\right)
\end{equation}
Recalling the ordering of arrival times, 
\[
  T^* = \frac{a^*_1}{a^*_0}H^{p_1^*} \delta(t-(\tau^*_1-\tau^*_0)) + ...
\]
\[
  T_1 = -\frac{a_1}{a_0}H^{p_1} \delta(t-(\tau_1-\tau_0)) + ...
\]
where the elided terms all involve $\delta$s located at later times.
So
\[
  (\delta + T^*)*(\delta + T_1)(t) = \delta(t)  +  \frac{a^*_1}{a^*_0}
  H^{p_1^*}\delta(t-(\tau^*_1-\tau^*_0)) -\frac{a_1}{a_0}H^p_1
  \delta(t-(\tau_1-\tau_0)) + ...
\]
and
\begin{equation}
  \label{eqn:notl2explicit}
  u(t) = \frac{a^*_0}{a_0}\left(\delta(t - (\tau^*_0-\tau_0)) +
    \frac{a^*_1}{a^*_0}H^{p_1^*}\delta(t-(\tau^*_1-\tau_0)) -
    \frac{a_1}{a_0}H^{p_1}\delta(t-(\tau_1-\tau_0^*)) + ...\right)
\end{equation}
The elided terms involve only travel time differences other than the
ones appearing here, so nothing cancels.

Observe that the travel-time differences $\tau^*_1-\tau_0$,
$\tau_1-\tau_0^*$ do not tend to zero, though they do tend to the same
limit as $m \rightarrow \kappa^*$ (as do the amplitude quotients), and the
Hilbert transform powers $p_1, p_1^*$ are the same as soon as $m$ is
sufficiently close to $\kappa^*$.

Of courese, the adaptive filter just computed is not square
integrable, and must be regularized for use in a square-integral
construction like that of AWI or MSWI. That amounts essentially to
replacing the $\delta$s in the above formulae by a square-integrable
approximate Dirac function, of roughly a wavelength in width. In the
preceding section, this approximate Dirac was $g$, so use the same
notation here. The regularized version of $u(t)$ in
\ref{eqn:notl2explicit} contains summands with multiples of $g(t-(\tau^*_1-\tau_0))$
and $g(t-(\tau_1-\tau^*_0))$. These are approximately scaled by
$\tau^*_1-\tau_0$ and $\tau_1-\tau^*_0$ respectively in the
definitions \ref{eqn:MSWI} and \ref{eqn:tJz} of the MSWI and AWI
objectives. For other choices of objective, they contribute similarly
to the total. They will eventually begin to cancel as $m$ approaches
$\kappa^*$, however this cancellation happens only when the times are
within the width of the approximate 
Dirac waveform $g$ - that is, within a wavelength.

This argument shows that when multiple arrivals carry significant energy, any
inversion approach based on the source-receiver extension, similar to
AWI or MSWI, will cycle-skip, that is, change rapidly when the
estimated model approaches its optimal value.

It should be emphasized that this phenomenon is primarily a feature of the
source-receiver extension, rather than of the specific objective
function defined in terms of it. In that respect, the source-receiver
extension is similar to the common surface parameter extensions that
have been studied for inversion of reflection data. Inversion methods
based on these extensions also fail to avoid cycle-skipping in the presence of
geometrical ray complexity \cite[]{geoprosp:2008}.

\section{Conclusion}
In this paper, I have shown that AWI is a good approximation to least
squares travel time inversion of transmitted wave data, up to errors proportional to a typical
data wavelength and to a regularization (``pre-whitening'')
weight. However, the argument leading to this conclusion works only if
the material model is assumed slowly varying so that geometric
acoustics accurately describes the solution of the wave equation, and if
sources and receivers are sufficiently close to one another to be
connected by unique rays. The first condition may be ignored to some
extent - the conclusion of this paper likely holds if the data is
dominated by ballistic transmitted waves. The second condition
is essential: even for purely transmitted waves, the presence of
multiple arrivals breaks the connection
with travel time inversion, and AWI and similar algorithms
become essentially as prone to cycle-skipping as is FWI.

%%%%%%%%%%%%%%%%%%
Note that this main result effectively bounds the optimal mean-{\em
  square} traveltime error achieved by AWI by the pulse width,
hence the RMS traveltime error by the square-root of the pulse
width. Pulse width is related to RMS wavelength by the Heisenberg
inequality, and in many examples these differ by a modest factor.
Thus error estimate is suboptimal from the point of view
of FWI, which requires something like an initial model estimate
predicting a half-wavelength traveltime error. This suboptimality
arises from the use of pre-whitening (Tihonov regularization) in the
definition of the AWI objecive function \cite[]{Warner:16}. It is
possible that a different regularization strategy or assumptions on
the wavelet could yield a more optimal error estimate.


The analysis presented here has nothing to say about AWI applied to
reflected wave data. \cite{Warner:16} and others have
suggested that AWI is effective in that context. If so, the underlying
mechanism remains to be explained. 

\bibliographystyle{seg}
\bibliography{../../bib/masterref}

