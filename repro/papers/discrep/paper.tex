\title{A Discrepancy-Based Algorithm for Extended Inversion}
\author{William W. Symes, Orcas Island, Huiyi Chen* and Susan E. Minkoff, The University of Texas at Dallas}

\lefthead{Symes}

\righthead{Discrepancy}

\maketitle
\begin{abstract}
  Study of an extremely simple single-trace transmission example shows
  how an extended source formulation of full waveform inversion can
  produce an optimization problem without spurious local minima
  (``cycle skipping''), hence efficiently solvable via Newton-like
  local optimization methods. The data consist of a single trace
  recorded at a given distance from a point source. The velocity or
  slowness is presumed homogeneous, and the target source wavelet is
  presumed quasi-impulsive or localized near zero time lag. The source
  is extended by permitting energy to spread in time, and the spread
  is controlled by adding a weighted mean square of the extended
  source wavelet to the data misfit, to produce the extended inversion
  objective. The objective function and its gradient can be computed
  explicitly, and it is easily seen that the error in any local
  minimizer is bounded by a multiple of the permitted energy spread,
  which may be interpreted as a wavelength. The derivation shows several
  important features of all similar extended source algorithms. For
  example, nested optimization, with the source estimation in the
  inner optimization (variable projection method), is essential. The
  choice of the penalty operator, controlling the extended source
  degrees of freedom, is critical: in order to produce an objective
  immune from cycle-skipping, the penalty operator must be
  (pseudo-)differential. The penalty weight is chosen dynamically,
  according to a version of the Discrepancy Principle, in order to
  accelerate convergence to a near-global minimum. Like many other
  extended formulations, the example problem considered here permits
  the discrepancy criterion to be satisfied with zero penalty
  weight. This unusual feature removes a common obstacle to the use of
  the discrepancy principle, namely the determination of a
  satisfactory estimate of the penalty weight: the initial weight may
  be set to zero, and the dynamic algorithm updates it until
  convergence is achieved.

\end{abstract}

\section{Introduction}
Full Waveform Inversion (FWI), or estimation of earth structure by
model-driven least squares data fitting, is now well-established as a
useful tool for probing the earth's subsurface
\cite[]{VirieuxOperto:09,Fichtner:10}. However, so-called ``cycle-skipping'', the tendency of iterative FWI
algorithms to stagnate at suboptimal and geologically uninformative
earth models, still impedes its use. Because the computational size of field inversion tasks
is very large, only iterative local (descent) minimization of the data
misfit function is computationally feasible. However local
descent methods avoid suboptimal stagnation only if initial models are
already quite close to optimal, in the sense of predicting the arrival
times of seismic events to within a small multiple of a dominant
wavelength \cite[]{GauTarVir:86,Plessix:10}.

This paper concerns one of the several ideas that have been advanced to
overcome cycle-skipping, namely so-called extended inversion
\cite[]{geoprosp:2008}. ``Extended'' signifies that addional degrees
of freedom are provided to the modeling process, in the hope of
opening up more effective routes to geologically informative models
with acceptable data fit. Since these extended degrees of freedom are
not part of the basic physics chosen to model the data acquisition
process, they should be suppressed in the eventual solution. Extended
inversion methods differ by the choice of additional degrees of
freedom, and by choice of penalty applied to eliminate them in the final
result.

Many of these extended inversion concepts sound plausible, and appear
to work at least to some extent as one might hope from their heuristic
justifications. However very few of these approaches have been
underwritten by mathematical argument: in essence, they are mostly
justified only ``in the rear-view mirror'', with no assurance that
failure is not just around the corner, at the next example. On top of
that, some of these approaches, for example those based on the
computationally attractive Variable Projection Method (``VPM'') of
\cite{GolubPereyra:03}, are cast in such form that the reasons for
success are not readily apparent.

This note shows exactly how extended inversion leads to successful solution
of a very simple wave propagation inverse
problem, which asks that a homogeneous velocity field be deduced from
one trace at known offset. I put forward this inverse problem and
extension-based solution not because there are not simpler
ways of answering the question it poses - there certainly are - but
because the formal ingredients of waveform-based velocity estimation
in this very simple setting are common to many similar extended
inversion algorithms, and because in this case every computation can
be done analytically, nearly to completion. In particular, it becomes
clear why the VPM gradient formula produces a constructive update,
with no possibility of stagnation away from the global minimum.

The extended inversion approach
developed here uses a {\em source extension}, in which source
parameters form the additional degrees of freedom. This type of
extension presumes that the actual or target source is constrained in
some way; the extended source is allowed to violate the
constraint. For recent overview of source extension methods, see
\cite{HuangNammourSymesDollizal:SEG19}. Source
extension methods have computational complexity approximately the same
as that of FWI, a signal advantage over the alterantive medium
extension class described for instance in \cite[]{geoprosp:2008}.

For the problem considered here, the source model amounts to a
wavelet, and the target wavelet is assumed to be non-zero only in a
short time interval (an approximate impulse, perhaps as the result of
signature deconvolution). The extension consists in permitting energy
to spread in time at intermediate iterations of the inversion. A
penalty for energy spread drives the extended source towards a focused
source approximately satisfying the assumed constraint. Not all
penalties are created equal. Two plausible penalty operators are
studied here, both multiplication operators: one by the characteristic
function of the target support, the other by a smooth function of
time. Only the smooth multiplier avoids cycle-skip. This distinction
is quite general, as will be explained in the Discussion section.

%despite not incorporating explicitly the extension measure
%(``annihilator'') central to the approach.

I begin with a quick sketch of constant density acoustics, and
describe the single-trace transmission inverse problem.. For
completeness, I show how the standard FWI approach to this problem,
with the small support of the source imposed as a hard constraint,
generates multiple local minima that will be found by any descent
method unless the initial slowness estimate predicts travel time from
source to receiver with an error on the order of a wavelength. The
next section describes the extended source objective, and the reduced
objective produced by VPM, in general terms. As VPM eliminates the
extended source, this function depends only on the velocity, just as
does the FWI objective.  Of course, it also depends on the operator
used to define the penalty (``annihilator''). I propose two choices: a
soft version of the support constraint, and an even softer constraint,
namely multiplication by time. A nearly-explicit calculation of the
VPM gradient for these two choices shows that the first leads again to
multiple local minima far from the target solution, whereas the second
does not. In fact, in the second case, the only stationary points are
``within a wavelength'' of the correct velocity, used to build the
data: that is, cycle-skipping cannot occur. For this second, more
successful penalty, I introduce the discrepancy based dynamic
weighting algorithm, and show how it accelerates the convergence
towards a local minimizer, which has been shown to be an acceptable
approximation to a global minimizer. The paper ends with a discussion
of the parallels between the calculations presented here and the
structure of other extended inversion methods applicable to
field-scale velocity estimation, and the critical role that the
differential nature of the extension penalty plays in the success of
this and other extension methods.

I shall use the abbreviations ${\cal B}(X,Y)$ and ${\cal I}(X,Y)$ for
the algebra of bounded linear operators from the Hilbert space $X$ to
the Hilbert space $Y$, and its subalgebra of invertible operators.

%\section{Theory}

%\cite{FuSymes2017discrepancy} introduce a version of the discrepancy
%principle and its application to separable nonlinear least squares
%problems in penalty form \ref{eqn:basic}. The algorithm developed
%there is incomplete, however: it lacks  termination criteria. In this
%section, I review the discrepancy-based algorithm and introduce an
%appropriate stopping rule, thus completing the development of the
%algorithm presented in \cite{FuSymes2017discrepancy}.

\section{An Example: Single Trace Modeling}
Assume small amplitude (linearized) acoustic propagation, constant
density, and isotropic point source and receiver. Denote by $m(\bx)$
the slowness (reciprocal velocity) at spatial position $\bx$, $w(t)$
the time dependence of the point source (``wavelet'') at location
$\bx=\bx_s$. Then the (excess) pressure field $p(\bx,t)$ obeys a
scalar wave equation:
\begin{eqnarray}
  \label{eqn:awe}
  \left(m(\bx)^2\frac{\partial^2 p}{\partial t^2} - \nabla^2\right) p(\bx,t) &=&
                                                                         w(t)\delta(\bx-\bx_s) \nonumber\\
  p(\bx,t)&=&0, t\ll 0
\end{eqnarray}
Suppose that a single trace is recorded, at distance $r>0$ from the
source position $\bx_s$. The dominant information in a single
trace is the transient signal time of arrival, constraining only the mean slowness in the
region
between source and receiver, so assume that the
slowness is constant, that is, independent of position $\bx$. The pressure field is simply the  the source
wavelet $w(t)$ convolved with the
acoustic Green's function, for which an analytic expression is
available in the constant $m$ case \cite[]{CourHil:62}:
\begin{equation}
  \label{eqn:homsol}
  p(\bx,t) = \frac{1}{4\pi |\bx-\bx_s|}w\left(t-m|\bx-\bx_s|\right).
\end{equation}

The receiver location $\bx_r$ lies at distance $r$ from the source
location $\bx_s$, that is, $|\bx_r-\bx_s|=r$. The predicted signal at
$p(\bx_r,t)$ depends nonlinearly on the slowness $m$ and linearly on the
source wavelet $w$. Therefore it is naturally represented as the
action of a $m$-dependent linear operator $S[m]$ on $w$:
\begin{equation}
\label{eqn:mod}
F[m]w(t)= p(\bx_r,t) = \frac{1}{4\pi r}w\left(t-mr\right).
\end{equation}

Ignoring amplitude, this map implements a $m$-dependent time
shift. This time shift operator is the basis of many descriptions of
the cycle-skipping phenomenon (for example, \cite{VirieuxOperto:09},
Figure 7), so it is unsurprising that an analysis of cycle-skipping
can be based on the simple modeling operator described above, which
amounts essentially to a time shift.

The slowness $m$ must be positive, as follows from basic acoustics,
and in fact reside in a range $M=(m_{\rm min}, m_{\rm max})$ characteristic of the
material model: for crustal rock, a reasonable choice would be
$m_{\rm min}=0.125, m_{\rm max}=0.6$ s/km. The wavelet $w$ is
naturally assumed to be 
square-integrable, corresponding to finite power transfer from the
source process to the propagating wavefield $p$
\cite[]{SantosaSymes:00}. The predicted data $F[m]w$ is then also
square-integrable.

FIeld recording takes place over finite time intervals (and with
finite sample rate, a detail I shall ignore in this paper). Denote by
$[t_{\rm min},t_{\rm max}]$ the recording interval for the
hypothetical single trace data. Then natural choices for domain and
range of $F[m]$ are $W = L^2(\bR)$ and $D=L^2([t_{\rm min},t_{\rm
  max}])$ respectively (that is, interpret the definition \ref{eqn:mod}
as holding for $t \in [t_{\rm min},t_{\rm  max}]$). So
\begin{equation}
  \label{eqn:mapprop}
  \mbox{for }m \in M, F[m] \in {\cal B}(W,D),\mbox{ and }\|F[m]\| =
  \frac{1}{4\pi r}.
\end{equation}

Note also that $F[m]$ is surjective for every $m \in M$.

In computational practice, $W$ will have to be replaced by a
finite-dimensional subspace of $L^2(\bR)$. Many such choices will
implicitly limit the support of $w \in W$ to a bounded interval, say
$[T_{\rm min},T_{\rm max}]$. To maintain the surjective property,
these bounds should be chosen so that $[t_{\rm min}, t_{\rm max}]
\subset [T_{\rm min}+mr,T_{\rm max}+mr]$ for all $m \in M$, that is,
\begin{eqnarray}
  \label{eqn:dombounds}
  t_{\rm min} &\ge & T_{\rm min}+m_{\rm max}r,\nonumber\\
  t_{\rm max} &\le & T_{\rm max}+m_{\rm min}r.
\end{eqnarray}
We will ignore these computational necessities in this work,
maintaining the defintion $W=L^2(\bR)$.

\section{The Inverse Problem}

Part of the definition of an inverse problem based on the model
definition \ref{eqn:mod} is that observed data $d \in D$ be fit: that
is, $m, w$ must be found so that $F[m]w \approx d$. However this
formulation does not provide enough information to determine the model
$(m,w) \in M \times W$: indeed, the definition
\ref{eqn:mod} shows that $F[m]$ is surjective for any $m \in M$, so
the data can always be fit precisely by appropriate choice of $w$, and
$m$ is completely
unconstrained.  In order that data fit constrain $m$, it is necessary
to further constrain $w$ {\em a priori} - that is, to add information
beyond the trace data $d$.

One natural constraint is to assume that the support of $w$ is
limited. An assumption of small support may be justified as
follows. In practical seismic data processing, actual sources may act
over considerable time intervals - for example, the most commonly used
marine source (airgun) produces an oscillating pulse that dies away
slowly. It is commonplace to estimate this pulse then deconvolve it
from the data by safeguarded Fourier division or other means. This
so-called signature deconvolution results in modified data
corresponding to a source equal to signature deconvolution of the
pulse estimate itself. The deconvolved source pulse approximates an
impulse ($\delta(t)$): it cannot have point support, due to the finite
frequency nature of seismic data, hence the bandlimited nature of the
pulse estimate, but its support is ultimately much smaller than that
of the original pulse estimate.

Therefore, supplement the observed data $d \in D$ with a presumed 
source support radius $\lambda$. Define 
$\lW=\{w \in W: \mbox{ supp }w \subset [-\lambda,\lambda]\}$, and 
denote by $\lF$ the restriction of $F$, defined in equation 
\ref{eqn:mod}, to $M \times \lW$.

Note that for $w \in \lW$, $m \in M$,
$\mbox{supp }F[m]w \subset [ m_{\rm min}r-\lambda,
m_{\rm max}r+\lambda] \cap
[t_{\rm min},t_{\rm max}]$. Also, $\lF[m]$ is coercive for all $m \in
M$ (so that $w$ is uniquely determined) only if
$[ m_{\rm min}r-\lambda,m_{\rm max}r+\lambda] \subset
[t_{\rm min},t_{\rm max}]$. Define $\lambda_{\rm max}
>0$, so that $M, r,$ and $T$ by
\begin{equation}
  \label{eqn:fullrec}
  [ m_{\rm min}r-\lambda_{\rm max}, m_{\rm max}r+\lambda_{\rm max}]
  \subset [t_{\rm min},t_{\rm max}].
\end{equation}
Then for 
$m \in M$, $0 <\lambda \le \lambda_{\rm max}$, $\lF[m] \in {\cal B}(\lW,D)$ is coercive.

Note that condition \ref{eqn:fullrec} implies
\begin{eqnarray}
  \lambda_{\rm max} & \le & t_{\rm max}-m_{\rm max}r,\nonumber \\
  t_{\rm min}-m_{\rm min}r & \le & -\lambda_{\rm max} \label{eqn:lminc}
\end{eqnarray}
hence for any $m \in M$,
\begin{equation}
  [-\lambda_{\rm max}, -\lambda_{\rm max}] \subset[ t_{\rm min}-mr ,
  t_{\rm max}-mr].
  \label{eqn:zeroinc}
\end{equation}

In terms of this infrastructure, the inverse problem studied in 
this paper is: 

\noindent {\bf Problem Statement:} given data $d \in D$, relative 
error level $\epsilon >0$, and support radius $0 <\lambda \le 
\lambda_{\rm max}$, find 
$(m,w) \in M \times \lW$ for which 
\begin{equation}
  \label{eqn:probstat0}
  \|\lF[m]w-d\| \le \epsilon\|d\|. 
\end{equation}

\noindent{\bf Remark:} The support constraint is closely linked to the
folk theorem about FWI noted many times in the literature: convergence
of a descent method requires that the initial slowness must be known
to ``within a (fraction of a) wavelength''. The relation is a
consequence of Heisenberg's inequality, and will be reviewed later in
the paper.

The best case for data fitting
is clearly the one in which the data can be fit precisely: that is,
there exists $(m_*,w_*) \in M\times \lW$ so that
\begin{equation}
  \label{eqn:defdatanonoise}
  d=\lF[m_*]w_*.
\end{equation}
Such data $d$ is {\em noise-free}, in the range of the map $\lF$, and a solution of the Problem Statement
exists with arbitrarily small $\epsilon>0$. I will relax the noise-free assumption
later in the paper: until further notice, assume that $d \in
D$ satisfies the condition \ref{eqn:defdatanonoise}.

\section{Full Waveform Inversion: hard constraint}
A natural formulation of the Problem Statement \ref{eqn:probstat0} is
via least squares: that is, given $d \in D$, $\lambda >0$, find $(m,w)
\in M \times \lW$ to minimize $e: M \times W
 \rightarrow \bR$, defined by
\begin{equation}
  \label{eqn:FWI}
  e[m,w;d] = \frac{1}{2}\|\lF[m]w-d\|^2.
\end{equation}
A pair $(m,w) \in M\times \lW$
satisfies the condition \ref{eqn:probstat0} iff $e[m,w;d] \le \epsilon^2/2$, so a solution exists iff a 
global minimizer of $e$ satisfies this inequality.

In this discussion, I will presume that $d$ is 
noise-free, as defined at the end of the last section (equation
\ref{eqn:defdatanonoise}). Thus {\em a priori} a global minimizer of
$e$ is a solution of the Problem Statement for any
$\epsilon > 0$. The only
question is how to find such a global minimizer. As mentioned in the
introduction, the simple model problem studied here is a proxy for
large-scale data fitting problems for which the only computationally
feastible approach to minimization of analogs of $e$
is some form of descent-based iterative local optimization. Any such
algorithm requires the specification of an initial iterate
$(m_0,w_0)$. Therefore the key question is: what
constraints must be imposed on $(m_0,w_0)$ in order that the resulting
sequence of iterates convertes to a global minimizer?  

To answer this question, note that 
\[
 e[m,w;d] =  \frac{1}{32\pi^2
    r^2}\int_0^T\,dt\,\left|w\left(t-mr\right)-w_*\left(t-m_*r\right)\right|^2
\]
Since $w_*, w$ vanish for $|t|>\lambda$,
$\lF[m_*]w_*(t)$ vanishes if $|t-m_*r|>\lambda$ and $\lF[m]w$ vanishes if $|t-mr|>\lambda$. So if $|mr-m_*r|
= |m-m_*|r > 2\lambda$, then $|t-mr|+|t-m_*r| \ge |mr-m_*r| >
2\lambda$ so either $|t-mr|>\lambda$ or $|t-m_*r|>\lambda$, that is,
either $\lF[m]w(t)=0$ or $\lF[m_*]w_*=0$. Therefore $\lF[m]w$ and
$\lF[m_*]w_*$ are orthogonal in the sense of the $L^2$ inner product
$\langle \cdot,\cdot \rangle_D$ on $D$:
\begin{equation}
  \label{eqn:ortho}
  |m- m_*|r > 2\lambda \,\, \Rightarrow \,\, \langle F[m]w,
  F[m_*]w_*\rangle_D = 0
\end{equation}
But $d = \lF[m_*]w_*$, so this is the same as saying that $d$ is
orthogonal to $F[m]w$. So conclude after a minor manipulation that
\begin{equation}
  \label{eqn:iso}
  |m- m_*|r > 2\lambda \,\, \Rightarrow \,\, e[m,w;d]=\frac{1}{32\pi^2
    r^2}(\|w\|^2 + \|w_*\|^2).
\end{equation}
That is, for slowness $m$ in error by more than $2\lambda/r$ from the 
target slowness $m_*$, the means square error (FWI objective) $e$ is independent of
$m$, and its minimum over $w$ is attained for $w=0$:
\begin{theorem}
  \label{thm:fwi}
  Suppose that $0 <\lambda\le \lambda_{\rm max}$,  $m_* \in M, w_*
  \in \lW, d=\lF[m_*]w_*$ is noise-free data per definition \ref{eqn:defdatanonoise},
  Under assumption \ref{eqn:fullrec}, for any $m \in M$ with $r|m-m_*|>2\lambda$,
\begin{equation}
  \label{eqn:isovpm}
 \min_w e[m,w;d]=e[m,0,d]=\frac{1}{32\pi^2 r^2}\|w_*\|^2,
\end{equation}
and any such $(m,0)$ is a stationary point of $e$.
\end{theorem}

Therefore local minimizers of $e$ abound, as far as you like from the
global minimizer $(m_*,w_*)$. Local exploration of the FWI objective
$e$ gives no useful information whatever about constructive search
directions, and descent-based optimization tends to fail if the
initial estimate $m_0$ is in error by more than $2\lambda/r$
(``further than a multiple of a wavelength'', per discussion
above). In fact the actual behaviour of FWI itererations is worse
(failure if $m_0$ is in error by ``half a wavelength''), as follows
from a more refined analysis of the ``cycle-skipping'' local behaviour of $e$ near its
global minimizer.

\section{Soft constraints and penalty formulation}

For inverse problems based on wave propagation, enforcing hard
constraints on the solution ($w \in \lW$) leads to the pathology
similar to that described in the last
section for the single-trace transmission problem, and to failure of
local optimization methods to converge to a useful solution. A soft
constraint or penalty approach can avoid this obstacle.

This
discussion presumes that the penalty is quadratic, constrains only
$w\in W$
(as it is a soft version of the condition $w \in \lW$), and is
$m-$independent. Therefore the penalty function takes the form
\begin{equation}
\label{eqn:pen}
p[w] = \frac{1}{2}\|Aw\|_Y^2 
\end{equation}
where $A \in {\cal B}(W,Y)$, $Y$ being another suitable Hilbert
space.

The penalty approach asks that a linear combination of $e,p$,
depending on a relative weight $\alpha$:
\begin{equation}
\Ja[m, w;d] = e[m,w;d] + \alpha^2 p[w] = \frac{1}{2}(\|F[m]w-d\|^2 + \alpha^2 \|Aw\|^2).
\label{eqn:pen}
\end{equation}
be minimized over $m\in M$ and $w\in W$.

The choice of penalty weight $\alpha$ has a profound influence on the
character of this optimization problem. For the moment, I will mandate
only that $\alpha \ge 0$. A complete description of an algorithm using
the penalty function \ref{eqn:pen} to solve the inverse problem
\ref{eqn:probstat0} must include a selection principle for $\alpha$. I
shall describe one such principle below.  

While minimization of $\Ja$ might be tackled directly - by
alternately minimizations between $m$ and $w$, or by computing updates
for $m$ and $w$ simultaneously - such joint mimization performs
poorly, as \cite{YinHuang:16} has shown. The reason for this poor
performance is that $\Ja$ has dramatically different
sensitivity to $m$ versus $w$,
%especially for high frequency $f$, as
%the reader will see below.
as will be established below.
Instead, a nested approach, in which $w$ is
eliminated in an inner optimization,
generally gives far better numerical performance.  This {\em Variable
Projection Method} (VPM) \cite[]{GolubPereyra:03} takes advantage of
$\Ja$ being quadratic in
$w$ to eliminate $w$ from the problem statement, as described in the
following theorem.

In stating this result, I will make use of the the Hessian operator of
$\Ja[m,\cdot]$, which is $F[m]^TF[m] + \alpha^2 A^TA$. The
superscript $T$ denotes the adjoint with respect to the inner products
in $W=L^2(\bR)$ and $D=L^2([t_{\rm min},t_{\rm max}])$.

\begin{theorem}
\label{thm:red}
Assume that
\begin{equation}
\label{eqn:norminv}
F[m]^TF[m] +\alpha^2 A^TA \in {\cal I}(W,W).
\end{equation}
Then the unique stationary point $\aw[m;d] \in W$ of
$\Ja[m,\cdot;d]$ is the solution of the {\em normal equation}:
\begin{equation}
  \label{eqn:norm}
  (F[m]^TF[m]+\alpha^2A^TA)w= F[m]^Td,
\end{equation}
Define the reduced or variable projection objective by
\begin{equation}
  \label{eqn:redexp}
  \tJa[m;d] =\Ja[m,w_{\alpha}[m;d];d] = \min_w \Ja[m,w;d]
\end{equation}
Then
\begin{equation}
  \label{eqn:objexp}
  \tilde{J}_{\alpha}[m;d] =\langle d, (I-F[m]
  (F[m]^TF[m]+\alpha^2A^TA)^{-1}F[m]^T)d\rangle_D
\end{equation}
\end{theorem}

\begin{proof}
Substitute $w_{\alpha}[m;d]$ for $w$ in the definition \ref{eqn:pen}
of $J_{\alpha}$, using the normal equation \ref{eqn:norm} and 
rearranging, obtain the result.
\end{proof}

\section{All stationary points are (near) global minimizers -
  sometimes}
It is not immediately apparent why use of a soft constraint should
produce an optimization problem more amenable to descent methods than
does the hard-constrained inverse problem (FWI). In this section, I
will show that the penalty formulation can yield a reduced problem
with for which any stationary point is close to a global minimizer,
{\em for some choices of penalty operator, but not for other choices
  that appear equally or even more suitable}. 

For the single-trace transmission inverse problem, $\tilde{J}_{\alpha}$ is explicitly
computable. First observe that apart from amplitude, $F[m]$ is
unitary: for $g \in D$,
\begin{equation}
\label{eqn:tran}
F[m]^T g (t) =
\left\{
  \begin{array}{c}
    \frac{1}{4\pi r}g\left(t+mr\right), \, t \in [t_{\rm min}-mr,
    t_{\rm max}-mr],\\
    0, \mbox{ else.}
  \end{array}
\right.
\end{equation}
so
\begin{equation}
  \label{eqn:unit}
  F[m]^TF[m] = \frac{1}{(4\pi r)^2}{\bf 1}_{[t_{\rm min}-mr,  
    t_{\rm max}-mr]}
\end{equation}
in which ${\bf 1}_{S}$ denotes
multiplication by the characteristic function of a measurable 
$S \subset \bR$.

Therefore the normal equation for the minimizer on the RHS of equation \ref{eqn:red} is
\begin{equation}
  \label{eqn:norm1}
  \left(\frac{1}{(4\pi r)^2} {\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]} + \alpha^2 A^TA\right)w= F[m]^Td.
\end{equation}

At this point I have to come clean about the actual choice of
$A$. The role of $A$ is to penalize distance from the constrained
subspace $\lW$, in some sense. All choices for $A$ considered here are scalar 
multiplication operators defined by a choice of $a \in L^{\infty}(\bR)$:
\begin{equation}
  \label{eqn:annmult}
  A w(t)= a(t)w(t), \, t\in \bR.
\end{equation}


With these choices, the normal equation \ref{eqn:norm1} becomes
\begin{equation}
\label{eqn:norm2}
\left(\frac{1}{(4\pi r)^2}  {\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]} + \alpha^2a^2\right)w= F[m]^Td.
\end{equation}

\begin{theorem}
  \label{thm:norminv}
  Assume the conditions \ref{eqn:mod}, \ref{eqn:fullrec},
  \ref{eqn:annmult}. Also assume that $\lambda \in (0,\lambda_{\rm
    max}], \alpha > 0,$ and that $a \in L^{\rm infty}(\bR)$
  mentioned in condition \ref{eqn:annmult} satisfies
  \begin{equation}
    \label{eqn:abnd}
    |t| > \lambda \Rightarrow a(t) \ge C,
  \end{equation}
  in which $C>0$ may depend on $\lambda$.
  Then
  \begin{itemize}
  \item[1. ]the normal operator $F[m]^TF[m] + \alpha^2A^TA$ is
    invertible for any $m \in M$, $\alpha > 0$;
  \item[2. ]for any $d \in D$, the normal equation \ref{eqn:norm} has
    a unique solution $\aw[m;d]\in W$, and 
    \begin{equation}
      \label{eqn:normsol}
      \aw[m;d](t) = \left\{
        \begin{array}{c}
          \left(\frac{1}{(4\pi r)^2} + \alpha^2
          a^2(t)\right)^{-1}\frac{1}{4 \pi r}d(t+mr), t \in [t_{\rm
          min}-mr, t_{\rm max}-mr];\\
          0, \mbox{ else;}
        \end{array}
      \right.
    \end{equation}
  \item[3. ]if in addition $d=F[m_*]w_*, w_* \in \lW$ is noise-free, as in equation
    \ref{eqn:defdatanonoise},
    \begin{equation}
      \label{eqn:solnnongen}
      \aw[m,d](t)= \left(1+ (4\pi r)^2\alpha^2 a(t)^2\right)^{-1}w_*\left(t+(m-m_*)r\right).
    \end{equation}
  \end{itemize}
\end{theorem}

\begin{proof}
  \begin{itemize}
  \item[1. ]Note that thanks to \ref{eqn:zeroinc}, if $|t|\le
    \lambda \le \lambda_{\rm max}$, then ${\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]}(t) = 1$, whereas if $|t|>\lambda$,
    then $a(t) \ge C$, whence
    \[
      \frac{1}{(4\pi r)^2}  {\bf 1}_{[t_{\rm min}-mr,  
        t_{\rm max}-mr]} + \alpha^2a^2  \ge \min\{(4\pi r)^2,
      \alpha^2\min\{1/(4\pi r)^2,C^2\} \}> 0.
    \]
    Therefore the normal operator is invertible under the stated
    conditions.

  \item[2. ]From the identity \ref{eqn:tran}.
    \[
      \mbox{supp }F[m]^Td \subset [t_{\rm min}-mr,t_{\rm max}-mr].
    \]
    Define $w_{\rm tmp}$ to be the right-hand side of equation \ref{eqn:normsol}. Then
    from the previous observation and identity \ref{eqn:tran},
    \[
      \mbox{supp }w_{\rm tmp} \subset [t_{\rm min}-mr,t_{\rm max}-mr].
    \]
    From the identity \ref{eqn:unit}, for any $w \in W$,
    \[
      t \in [t_{\rm min}-mr,t_{\rm max}-mr] \Rightarrow F[m]^TF[m]w(t)
      = \frac{1}{(4 \pi r)^2}w(t).
    \]
    It follows from this and the previous two observations that
    $w_{\rm tmp}$ solves the normal equation \ref{eqn:norm}, and
    therefore that $\aw[m;d]=w_{\rm tmp}$.

  \item[3. ]Follows by inserting the definition
    \ref{eqn:defdatanonoise} of $d$ in \ref{eqn:normsol} and
    rearranging.
  \end{itemize}
\end{proof}

Since the normal operator is invertible, the reduced
objective $\tJa$ is given by the expression \ref{eqn:objexp}.
The results of the previous theorem imply those of

\begin{theorem}
  \label{thm:epjgen}
  Assume the hypotheses of Theorem \ref{thm:norminv}. Then
  \begin{equation}
  \label{eqn:residnormgen}
  e[m,\aw[m,d];d] = \frac{1}{2}\int \,dt\,(4\pi r \alpha a(t-mr))^4(1 +
  (4\pi r \alpha a(t-mr))^2)^{-2}d(t)^2
\end{equation}
\begin{equation}
  \label{eqn:anninormgen}
  p[m,\aw[m,d];d] = \frac{1}{2}\int \,dt\,(4\pi r a(t-mr))^2(1 +
  (4\pi r \alpha a(t-mr))^2)^{-2}d(t)
\end{equation}

\begin{equation}
  \label{eqn:expjgen}
\tJa[m;d] = \frac{1}{2}\int\,dt\,(4\pi r \alpha a(t-mr))^2(1+(4\pi r \alpha 
a(t-mr))^2)^{-1}d(t)^2. 
\end{equation}
Finally, if $a \in W^{1,1}(\bR)$ (distributions with integrable 
derivatives), then $\tJa[\cdot;d]$ is differentiable, and 
\begin{equation}
  \label{eqn:dexpjgen}
  \frac{d}{dm}\tJa[m;d] = -(4 \pi r \alpha)^2 \int \,dt \, 
  \left(a\frac{da}{dt}\right)(t-mr)(1+(4\pi r \alpha 
  a(t-mr))^2)^{-2}d(t)^2. 
\end{equation}
\end{theorem}

\begin{corollary}
  \label{thm:epjnonoise}
  Assume the hypotheses of Theorem \ref{thm:norminv}, item 3. Then 
\begin{equation}
  \label{eqn:residnorm}
  e[m,\aw[m,d];d] 
= 8 \pi^2 r^2 \alpha^4\int\,dt\,a(t-(m-m_*)r)^4(1+(4\pi r)^2 \alpha^2 
    a(t-(m-m^*)r)^2)^{-2}w_*(t)^2.
\end{equation}
\begin{equation}
  \label{eqn:anninorm}
  p[m,\aw[m,d];d] = \frac{1}{2}\int \,dt\,  a(t-(m-m_*)r)^2 \left(1+ (4\pi r)^2\alpha^2
    a(t-(m-m_*)r)^2\right)^{-2}w_*(t)^2.
\end{equation}
so 
\begin{equation}
\label{eqn:expjnonoise}
\tJa[m;d] = \frac{\alpha^2}{2}\int\,dt\,a(t-(m-m_*)r)^2(1+(4\pi r)^2 \alpha^2 
  a(t-(m-m_*)r)^2)^{-1}w_*(t)^2. 
\end{equation}
Finally, if $a \in W^{1,1}(\bR)$, then $\tJa[\cdot;d]$ is differentiable, and 
\begin{equation}
  \label{eqn:dexpjnonoise}
  \frac{d}{dm}\tJa[m;d] = -r \alpha^2 \int \,dt \, 
  \left(a\frac{da}{dt}\right)(t-(m-m_*)r)(1+(4\pi r)^2 \alpha^2 
  a(t-(m-m_*)r)^2)^{-2}w_*(t)^2. 
\end{equation}
\end{corollary}

I willl now examine two possible choices of penalty multipler $a$ in
definition \ref{eqn:annmult}.

A penalty operator $A$ of which $\lW$ is the null space would be a
natural choice. Such operators have come to be called
``annihilators'', since they map all members of the constraint
subspace $\lW$ to zero.  A simple example is
\begin{eqnarray}
  A = E^c_{\lambda}&=&I - E_{\lambda},\mbox{ where } \nonumber \\
  E_{\lambda}w(t) &=&{\bf 1}_{[-\lambda,\lambda]}(t)w(t). 
                      \label{eqn:ann0}
\end{eqnarray}
That is, $E_{\lambda}$ is the orthogonal projector onto $\lW$,
and $E_{\lambda}^c$ is the orthogonal projector onto its
orthocomplement, an operator of the form \ref{eqn:annmult} with $a
= 1 - {\bf 1}_{[-\lambda,\lambda]}$. Note that this choice of $a$
satisfies condition \ref{eqn:abnd} with $C \equiv 1$.

\begin{theorem}
  \label{thm:boxcarbad}
  For $A=E^c_{\lambda}$, that is, $a=1-{\bf 1}_{-\lambda,\lambda]}$ in
  the definition \ref{eqn:expjgen}. Then if $|m-m_*| >  2\lambda/r$,
  \[
    \tJa[m;d] = \frac{\alpha^2}{2(1+(4 \pi r)^2 \alpha^2)}\|w_*\|^2.
  \]
\end{theorem}

\begin{proof}
  This is clear, as the supports of $w_*$ and $t \rightarrow {\bf
    1}_{[-\lambda,\lambda]}(t-(m-m_*))$ are disjoint under this
  condition.
\end{proof}

\noindent {\bf Remark.}
One might have thought that $A=E^c_{\lambda}$ would be a better choice
of annihilator, as for noise-free
data, the solution set defined by the problem statement
\ref{eqn:probstat0} is the same as the set of global minimizers of
$\tJa$ in this case. However, for this choice of annihilator,
$\tJa$ exhibits the same feature as the mean square error $e$, namely
a continuum of stationary points at any distance from the global
minimizer $m_*$ greater than a multiple of $\lambda$. Therefore the
extended inversion with this choice of annihilator is no more amenable
to local optimization than is FWI. 

A second possible 
penalty operator penalizes energy away from 
$t=0$: choose $\tau > 0$ and set 
\begin{equation}
  \label{eqn:ann}
  a(t) = \min(|t|, \tau). 
\end{equation}
The cutoff $\tau$ will be chosen large enough to be effectively inactive: 
specifically, hindsight suggests 
\begin{equation}
  \label{eqn:taudef}
  \tau = \max\{|t_{\rm min}-m_{\rm min}r|,|t_{\rm min}-m_{\rm max}r|, |t_{\rm max}-m_{\rm min}r|, |t_{\rm max}-m_{\rm max}r|\}. 
\end{equation}
This 
particular annihilator has been employed in earlier papers on extended 
source inversion 
\cite[]{Plessix:00a,LuoSava:11,Warner:14,HuangSymes:SEG15a,Warner:16,HuangSymes:GEO17}. It
satisfies condition \ref{eqn:abnd} with $C=\lambda$, and is a member
of $W^{1,1}(\bR)$, so that the resulting objective $\tJa$ is
differentiable, with derivative given by equation \ref{eqn:dexpgen},
or \ref{eqn:dexpjnonoise} in the noise-free case. 

Surprisingly, the choice \ref{eqn:ann} yields an optimization
problem with better global convergence properties:

\begin{theorem}
  \label{thm:rampgood}
  For $a(t)=\min\{|t|,\tau\}$ in the definition \ref{eqn:annmult},
  with $\tau$ given by equation \ref{eqn:taudef}, all
  stationary points of $\tJa$ lie in the interval
  \[
    [m_*-\lambda/r, m_*+\lambda/r].
  \]
\end{theorem}
\begin{proof}
  As observed before, $\mbox{supp }\aw[m;d] \subset [t_{\rm
    min}-mr,t_{\rm max}-mr]\subset [-\tau,\tau]$, with $\tau$ defined
  in \ref{eqn:taudef}. Therefore, $a(t) = |t|$ $a a'(t) = t$ in the
  support of the integrand on the RHS of equation
  \ref{eqn:depjnonoise}, which therefore 
  becomes (after change of integration variable)
  \begin{equation}
    \label{eqn:gradfinal}
    \frac{d}{dm}\tJa[m;d] = -r \alpha^2 \int \,dt \, 
  t(1+(4\pi r)^2 \alpha^2 
  t^2)^{-2}w_*(t+(m-m_*))^2.
  \end{equation}
  Recall that $w_*(t+(m-m_*)r)$
  vanishes if $|t+(m-m_*)r| > \lambda$. Therefore the integral on the
  RHS of equation \ref{eqn:gradfinal} can be re-written
  \[
    = -r\alpha^2\int_{-(m-m_*)r-\lambda}^{-(m-m_*)r+\lambda}
    \,dt\, \frac{t}{(1+(4\pi r)^2\alpha^2 t^2)^2}w_*\left(t+(m-m_*)r\right)^2
  \]
  Suppose that $\mu < \lambda$ and $w_* \in W_{\mu}$. Then for
  $t+(m-m_*)r \in \mbox{supp }w_*$, $t\le\mu-(m-m_*)r$.
  If $m > m_*+\lambda/r$, then $t >\mu-\lambda<0$, so the support of
  the $[-\mu-(m-m_*)r, \mu-(m-m_*)r]$ is a proper
  subset of the negative half-axis. Consequently the first factor in the
  integrand satisfies
  \[
    \frac{t}{(1+(4\pi r)^2\alpha^2 t^2)^2}< 0
  \]
  over the interval of integration. The integral is
  negative, so the derivative
  is positive. In fact, the quantity displayed above is
  \[
    \le 
  \]
  Similar reasoning applies to the case $m <
  m_*-\lambda/r$. Thus
  \begin{eqnarray}
    m > m_*+\lambda/r & \Rightarrow & \frac{\partial \tJa}{\partial
                                      m}[m;d]> 0; \nonumber\\
    m < m_*-\lambda/r & \Rightarrow & \frac{\partial \tJa}{\partial
                                      m}[m;d]< 0.
                                      \label{eqn:leftright}
  \end{eqnarray}
\end{proof}

\noindent {\bf Remark.} That is, {\em with the choice of penalty
  multiplier $a$ given in equation \ref{eqn:ann}, $\tJa$ has no local minima
  further than  $O(\lambda)$ from the global minimum:} the gradient
has the correct sign and slowness updates computed from it will be
constructive, unless the slowness estimate is already ``within a
wavelength'' of being correct.

\noindent {\bf Remark.} The estimate $|m-m_*|r<\lambda$ for stationary
points $m$ of $\tJa$ is sharp: it is possible to choose $w_* \in \lW$
so that $\lambda - |m-m_*|r$ is as small as you like. In particular,
the ``exact'' or ``true'' slowness $m_*$ is not necessarily the only 
stationary point, or even a stationary point at all.

HERE INSERT RESULT: TJA DIFFBL RIGHTARROW A DIFFBLE - relate in
Discussion to Stolk 03

\section{Extended Inversion}
The preceding theorem established that a proper choice of annihilator
leads to a reduced penalty objective all of whose stationary points
are within $O(\lambda)$ of the target slowness $m_*$, provided that
the data are noise-free in the sense of equation
\ref{eqn:defdatanonoise}. This result leaves open two questions:
\begin{itemize}
\item how does one use this reduced penalty minimization to produce
  a solution of the inverse problem as in problem statement
  \ref{eqn:probstat0}? 
\item how does one answer the same question for noisy data?
\end{itemize}

\subsection{Noise-free Data}
\begin{theorem}
  \label{thm:ipnonoisesuf}
  Suppose that $a$ is given by definition \ref{eqn:ann}, $\alpha$,
  $\mu \in (0,\lambda_{\rm max}]$,
  $d$ is given by
  \ref{eqn:defdatanonoise} with $w_* \in W_{\mu}$, and  $m_{\infty}$ is a stationary
  point of $\tJa[\cdot;d]$. Then $(m_{\infty},\aw[m_{\infty};d])$ is a
  solution of the inverse problem \ref{eqn:probstat0} for any $\lambda
  \ge 2\mu$ and $\epsilon \ge (4\pi r \lambda \alpha)^2$.
\end{theorem}

\begin{proof}
From the
inequalities \ref{eqn:leftright}, $|(m-m_*)r|\le \mu$. From the
identity \ref{eqn:solnnon}, $\mbox{supp }\aw[m_{\infty};d] \subset
[(m_{\infty}-m_*)r-\mu,(m_{\infty}-m_*)r+\mu] \subset
[-2\mu,2\mu]$. Because of the support limitation, $a(t)=|t|$
in the interval of integration appearing in \ref{eqn:residnorm}, so
\[
  e[m_{\infty},\aw[m_{\infty},d];d] 
= 8 \pi^2 r^2 \alpha^4\int^{\mu}_{-\mu}\,dt\,|t-(m_{\infty}-m_*)r|^4(1+(4\pi r)^2 \alpha^2 
|t-(m_{\infty}-m^*)r|^2)^{-2}w_*(t)^2.
\]
and therefore
\begin{equation}
  \label{eqn:estresidnorm}
e[m_{\infty},\aw[m_{\infty};d];d] \le 8 \pi^2 r^2 (2\mu\alpha)^4 \|w_*\|^2 =
\frac{1}{2}(8 \pi r \mu \alpha)^4 \|d\|^2
\end{equation}
\end{proof}

The inequality \ref{eqn:estresidnorm} can be interpreted as a bound 
on $\alpha$, given $\epsilon$ and $\lambda$, for a
stationary point of $\tJa$ to yield a solution of the inverse
problem: one obtains a solution, provided that $\alpha$ is
sufficiently small. On the other hand, it is clear that $\alpha$
cannot be too large if stationary points of $\tJa$ are to yield
solutions: the integrand in \ref{eqn:residnorm} is increasing in
$\alpha$ for every $t$ and $m$, and the multiplier
\[
t \mapsto (4\pi r \alpha(t-(m_{\infty}-m_*)r))^4(1+(4\pi r)^2 \alpha^2 
|t-(m_{\infty}-m^*)r|^2)^{-2}
\]
tends monotonically to $1$ as $\alpha \rightarrow \infty$, uniformly
on the complement of any open interval containing
$t=(m_{\infty}-m_*)r$. Therefore
\begin{equation}
  \label{eqn:elimit}
  \lim_{\alpha \rightarrow \infty}e[m_{\infty},\aw[m_{\infty};d];d] =
  \frac{1}{2}\frac{1}{(4 \pi r)^2}\|w_*\|^2 = \frac{1}{2}\|d\|^2.
\end{equation}
Consequently, there exists $\alpha_{\rm max}(\epsilon,\lambda,d)$ so
that
\[
  e[m_{\infty},\aw[m_{\infty};d];d]  \le \frac{1}{2}\epsilon^2\|d\|^2
  \Rightarrow \alpha \le \alpha_{\rm max}(\epsilon,\lambda,d).
\]
The existence of this limiting penalty weight has been inferred
indirectly; the next section is devoted to a constructive algorithm
for its approximation.

[THE NEXT BIT MAY OR MAY NOT BE WORTH KEEPING - IT WAS INTERESTING TO
WORK OUT]

Without further contraints on data or solution, nothing more can be
said about bounds on $\alpha$. If the data $d$ (and the target wavelet
$w_*$) are assumed to have a square integrable derivative, then a
necessary condition follows from the Heisenberg
inequality (see for example \cite{Folland:07}, p. 255). To formulate
this result in its most general form, introduce the Hilbert subspaces
$V^0 \subset L^2(\bR), V^1 \subset H^1(\bR)$:
\begin{eqnarray}
  V^0 & = & \{ f \in L^2(\bR): Af \in L^2(\bR)\}, \nonumber\\
  V^1 & = & V^0 \cap H^1(\bR).
            \label{eqn:vdef}
\end{eqnarray}
$V^0$ is the domain of $A$, and equipped with the graph norm of $A$. A
natural norm in $V^1$ is
\[
  \|f\|^2_{V^1} = \|f\|_{V^0}^2 + \|f\|_{H^1}^2.
\]
$V^j$ is the completion of $C_0^{\infty}(\bR)$ in the corresponding
norm, j=0,1.

\begin{proposition}
  \label{thm:heis}
For $w \in V^1$,
  \begin{equation}
    \label{eqn:heis}
    \|Aw\|\|w'\| \ge \frac{1}{2}\|w\|^2
  \end{equation}
\end{proposition}

\begin{proof}
  For $w \in C_0^{\infty}(\bR)$,
  \[
    \int w^2 = \left|\int\,dt\, t (w(t)^2)' \right|= \left|2\int\,dt\,tw(t)w'(t)\right| \le
    2\|Aw\| \|w'\|
  \]
  by the Cauchy-Schwarz inequality. Since $C_0^{\infty}(\bR)$ is dense
  in $V^1$, the conclusion follows by continuity.
\end{proof}

In the conventional formulation of the Heisenberg inequality, the $L^2$ norm of
$w'$ is replaced by the its equivalent in terms of the Fourier
transform $\hat{w}$. Adopting temporarily the use of dummy variables
in the expression of functions, the identity \ref{eqn:heis} turns into
the usual form of the Heisenberg inequality: for $w \in V^1$,
\begin{equation}
\label{eqn:fheis}
\|tw(t)\|\|k\hat{w}(k)\| \ge \frac{1}{4\pi}\|w\|^2.
\end{equation}

Define $\krms[w]$, the root mean square estimator of frequency of $w
\in V^1$, by
\begin{equation}
  \label{eqn:krms}
  \krms[w]=\frac{1}{2\pi}\frac{\|w'\|}{\|w\|} = \left(\int
    \,dk\,\frac{|\hat{w}(k)|^2}{\|\hat{w}\|^2} k^2\right)^{1/2}.
\end{equation}
Then the inequalities \ref{eqn:heis}, \ref{eqn:fheis} can be rewritten as
\begin{equation}
  \label{eqn:frmsheis}
  \|Aw\| \ge \frac{\|w\|}{4\pi \krms[w]}.
\end{equation}

For $\lambda >0$, define
\begin{equation}
  \label{eqn:wlam1}
  \lW^1 = \lW \cap H^1(\bR).
\end{equation}
Note that $\lW^1 \subset V^1$ is a closed subspace for any
$\lambda>0$.

\begin{proposition}
  \label{thm:klam}
  For $w \in \lW^1$,
  \[
    \krms[w] \ge \frac{1}{4 \pi \lambda}
  \]
\end{proposition}

\begin{proof}
  Follows directly from inequality \ref{eqn:frmsheis} and the obvious bound
  $\|A|_{\lW}\|\le \lambda$.
\end{proof}

\noindent{\bf Remark:} This result is the link mentioned earlier between the support
constraint and the well-known frequency-based criteria for success of
FWI. The result of Theorem \ref{thm:fwi} can be rephrased as showing the
existence of many stationary points of the mean-square error function
for which the travel time is in error by more than
$\lambda_{\krms[w]|}$. In fact, the usual error criterion mentioned in the
literature is that the initial estimate of travel time must be in error by at most ``half a
wavelength'' if FWI is to converge reliably. This is correct in some
circumstances, depending on features of the target wavelet $w_*$ of
which the arguments in this paper do not take account.

\begin{proposition}
  \label{thm:heis2}
  For $\lambda>0, w \in \lW^1$,
  \begin{equation}
    \label{eqn:heis2}
    \|A^2w\| \ge \frac{\|w\|}{24 \lambda (2\pi\krms[w])^3}.
  \end{equation}
\end{proposition}

\begin{proof}
  Since $A$  preserves $\lW^1$, inequality \ref{eqn:heis} implies
  \begin{equation}
    \label{eqn:heissq}
    \|A^2w\| \ge \frac{\|Aw\|^2}{2\|(Aw)'\|}.
  \end{equation}
  From the definition of $A$, $(Aw)'=w +Aw'$, so for $w \in \lW^1$,
  \[
    \|(Aw)'\| \le \|w\|+ \lambda\|w'\|.
  \]
  Applying \ref{eqn:heis} again,
  \[
    \|w\|^2 \le 2\|w'\|\|Aw\|\le 2\lambda \|w'\|\|w\|,
  \]
  so obtain the 1D Poincar\'{e} inequality: for $w \in \lW^1$,
  \[
    \|w\| \le 2\lambda\|w'\|.
  \]
  Thus
  \[
    \|(Aw)'\| \le 3\lambda\|w'\|.
  \]
  Apply this estimate together with the basic Heisenberg estimate
  \ref{eqn:heis} to the inequality \ref{eqn:heissq} to obtain
  \[
    \|A^2w\| \ge \frac{\|w\|^4}{24\lambda \|w'\|^3}
  \]
  Rearranging and using the definition \ref{eqn:krms} of $\krms[w]$,
  arrive at inequality \ref{eqn:heis2}.
\end{proof}

\begin{theorem}
  \label{thm:ipnonoisenec}
  Suppose that $a$ is given by definition \ref{eqn:ann}, $\alpha$,
  $\mu \in (0,\lambda_{\rm max}/2]$, $d$ is given by
  \ref{eqn:defdatanonoise} with $w_* \in W_{\mu}^1$, and that a
  stationary point $m_{\infty}$ of $\tJa[\cdot;d]$ yields a solution
  of the inverse problem \ref{eqn:probstat0} for $\lambda \ge 2\mu$,
  $\epsilon>0$. Then
\begin{equation}
  \label{eqn:epsalpha}
  3 (4\pi \lambda \krms[w_*])^3 \epsilon \ge \frac{(4\pi  r\alpha\lambda)^2}
  {(1+(4\pi r \alpha \lambda)^2)}
\end{equation}
\end{theorem}

\begin{proof}
  Rearranging the identity \ref{eqn:residnorm}, and observing as in
  the proof of Theorem \ref{thm:ipnonoisesuf} that the support of the
  integrand is contained in $[-2\mu, 2\mu] \subset [\lambda,\lambda]$,
  \[
  e[m_{\infty},\aw[m_{\infty},d];d] 
= 8 \pi^2 r^2 \alpha^4\int\,dt\,t^4(1+(4\pi r)^2 \alpha^2 t^2)^{-2}w_*(t+(m_{\infty}-m_*))^2
\]
\[
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\int\,dt\,t^4w_*(t+(m_{\infty}-m_*))^2
\]
\begin{equation}
  \label{eqn:residnormbis}
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\|A^2w_*(\cdot + (m_{\infty}-m_*)r)\|^2.
 \end{equation}
Since $\krms[w]$ is invariant under translation of $w \in V^1$,
Proposition \ref{thm:heis2} implies that this is
\[
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\frac{\|w_*\|^2}{(24)^2 \lambda^2 (2\pi\krms[w_*])^6}
\]
\[
  = \frac{1}{2}\frac{(4\pi r\alpha\lambda)^4}{(1+(4\pi r \alpha
    \lambda)^2)^{2}}\frac{\|d\|^2}{(24)^2 (2\pi
    \lambda \krms[w_*])^6}
\]
The pair $(m_{\infty}, \aw[m_{\infty},d])$ is presumed to solve the inverse
problem as stated in \ref{eqn:probstat0}, in particular
\[
  \epsilon \ge (2 e[m_{\infty}, \aw[m_{\infty},d],d])^{1/2}/\|d\|
\]
the inequality \ref{eqn:epsalpha} follows.

\end{proof}

Inequality \ref{eqn:epsalpha} couples the dimensionless quantites
$\epsilon$,  $4\pi r \lambda \alpha$, and $4 \pi \lambda
krms[w_*]$. Proposition \ref{thm:klam}, implies that the left
hand side is $\ge 3\epsilon$. Since the right hand side is $\le 1$,
the inequality implies no limitation on $\alpha$ if the left hand side
is $\ge 1$. For small $\epsilon$, the the largest permissible $\alpha$
is $O(\sqrt{\epsilon})$. The permissible range of $\alpha$ increases with
nondimensionalized RMS frequency $4\pi \lambda \krms[w_*]$. Since
$\krms$ is invariant under translation and scaling of its argument,
$\krms[w_*]=\krms[d]$, that is, the nondimensionalized RMS frequency
is an observable property of the data, in the noise-free case at
least.

\subsection{The Effect of Noise}
Suppose that the data trace $d$ takes the form
\begin{equation}
  \label{eqn:defdatanoisy}
  d = F[m_*]w_* + n = d_*+n,
\end{equation}
with $m_* \in M, w_* \in \lW$ as before, and noise trace $n \in
D$. Since no support assumptions can be made about $n$, equation
\ref{eqn:normsol} implies that $\aw[m;d] \notin \lW$ for any value of
$\alpha$.  Therefore minimization of $\tJa$ cannot by itself yield a
solution of the inverse problem as defined in the problem statement
\ref{eqn:probstat0}.

Recall that $F[m]$ is surjective for every $m \in M$. Define for each
$m \in M$
\begin{equation}
  \label{eqn:defnoisewavelet}
  w_{n}[m] = \mbox{arg}\min_w\{\|w\|: F[m]w=n\}.
\end{equation}
That is,
\[
  w_n[m](t) =
  \left\{
    \begin{array}{c}
      4\pi r n(t+mr),\,t \in [t_{\rm min}-mr,t_{\rm max}-mr];\\
      0, \mbox{ else.}
    \end{array}
  \right.
\]
Set
\begin{equation}
  \label{eqn:defwflat}
  w_{\flat} = w_* + w_n[m_*];
\end{equation}
then
\begin{equation}
  \label{eqn:datacomplete}
  d = F[m_*]w_{\flat}.
\end{equation}
That is, we can treat the data as if it were noise-free, for the price
of altering the wavelet. The altered wavelet is of course not a member
of $\lW$.

From the proof of Theorem \ref{thm:norminv}, for the choice
$a(t)=\min(|t|,\tau)$ with $\tau$ defined in \ref{eqn:taudef},
\begin{equation}
  \label{eqn:solnoise}
  \aw[m,d](t)= \left(1+ (4\pi r)^2\alpha^2 t^2\right)^{-1}w_{\flat}\left(t+(m-m_*)r\right).
\end{equation}
and from Theorem \ref{thm:epjnonoise}
\begin{equation}
\label{eqn:expjgen}
\tJa[m;d] = =\frac{\alpha^2}{2}\int\,dt\,(t-(m-m_*)r)^2(1+(4\pi r)^2 \alpha^2 
  (t-(m-m^*)r)^2)^{-1}w_{\flat}(t)^2.
\end{equation}
whence via the same short computation as before
\[
  \frac{\partial}{\partial m}\tJa[m;d] = -r\alpha^2\int \,dt\,\frac{t-(m-m_*)r}{(1+(4\pi r)^2\alpha^2 (t-(m-m_*)r)^2)^2}w_{\flat}(t)^2. 
\]
\[
  = -r\alpha^2 \int \, dt \, (t-(m-m_*)r)\aw[m;d](t)^2 
\]  
\[
  = -r\alpha^2\left\{ \int_{-\lambda}^{\lambda} \, dt \, 
    (t-(m-m_*)r)(w_*(t)^2 +2w_*(t)w_n[m_*](t))\right\}
\]
\begin{equation}
  \label{eqn:gradnoise}
-r\alpha^2\int_{-\infty}^{\infty} \, dt \, a(t-(m-m_*)r) w_n[m_*](t)^2.
\end{equation}
The last line recalls the definition of $a$ for this example
(\ref{eqn:deftau}).

We will follow the pattern laid down in the proof of Theorem
\ref{thm:rampgood}. However to obtain an estimate depending on some
measure of noise, it is necessary to explicitly quantify the bounds
within which stationary points of $\tJa$ are located. Pick $\dlam >0$,
and suppose that $|m-m_*|r > \lambda + \dlam$. Then for $t \in
[-\lambda,\lambda]$,
\begin{eqnarray*}
  (m-m_*)r >   \lambda +\dlam & \Rightarrow & t-(m-m_*)r < -\dl, \\
  (m-m_*)r < -\lambda +\dlam & \Rightarrow & t-(m-m_*)r >   \dl.
\end{eqnarray*}
In the first case, 

Recall that $\lE$ is the projection of $W$ onto $\lW$
(i.e. multiplication by ${\bf 1}_{[-\lambda,\lambda]}$.

\begin{theorem}
  \label{thm:ipnoisesuf}
  Suppose that $a$ is given by definition \ref{eqn:ann}, $\alpha$,
  $\mu \in (0,\lambda_{\rm max}]$, $d$ is given by
  \ref{eqn:defdatanoise} with $w_* \in W_{\mu}$, $n \in D$, and
  $m_{\infty}$ is a stationary point of $\tJa[\cdot;d]$. Then
  $(m_{\infty},\lE\aw[m_{\infty};d])$ is a solution of the inverse
  problem \ref{eqn:probstat0} for any $\lambda \ge 2\mu$ and
  $\epsilon \ge (4\pi r \lambda \alpha)^2 + C\|n\|/\alpha$.
\end{theorem}

\section{The Discrepancy Principle}
As described in the introduction, the discrepancy principle (in one of
its guises) involves setting an acceptable range of data misfit
$[e_-,e_+]$, and adjusting the penalty weight $\alpha$ in the reduced
objective definition \ref{eqn:red} so that the data error term at the
inner solution, $e[m,\aw[m;d]]$, lies in this range. Since updating
$m$ changes the inner problem, the appropriate condition for such
separable problems is that $e$ {\em stays} in the range $[e_-,e_+]$ as
$m$ is updated. In this section, we examine the dependence of $e$
on $\alpha$ with a view to understanding how to reset $\alpha$ when
$m$ changes.

Accordingly, regard $m$ as fixed and suppress it from the notation for the remainder of this section, and introduce the abbreviations
\begin{eqnarray}
\label{eqn:erralph}
e(\alpha) &=& e[m,\aw[m;d]]\\
\label{eqn:penalph}
p(\alpha) &=& p[m,\aw[m;d]]
\end{eqnarray}
Since the normal matrix $F^T F + \alpha^2 A^T A$ has been assumed
invertible (condition \ref{eqn:nopco}), the solution of $\aw$ of the
normal equation \ref{eqn:condition} is smooth in $\alpha^2$.
Differentiating equation \ref{eqn:condition} with respect to  $\alpha^2$ leads to the relation
\begin{equation}
(F^T F + \alpha^2 A^T A ) \frac{dw}{d\alpha^2} = -A^T A w
\label{eqn:dnorm}
\end{equation}
whence
\begin{align}
\frac{de}{d\alpha^2} 
&=\left\langle\frac{dw}{d\alpha^2},F^T(Fw-d) \right\rangle \nonumber \\
&=-\alpha^2\left\langle\frac{dw}{d\alpha^2},A^TAw\right\rangle \nonumber \\ 
&=\alpha^2 \langle A^TAw,(F^TF + \alpha^2 A^TA)^{-1}A^TAw\rangle \nonumber \\
&\ge 0
\label{eqn:de}
\end{align}
Note that the inequality in equation \ref{eqn:de} is {\em strict}  if $p > 0$ hence $A^TAw \ne 0$, since the normal operator is assumed to be positive definite.
The derivative of $p(\alpha^2)$ with respect to $\alpha^2$ is
\begin{align}
\frac{dp}{d\alpha^2} &=  -\langle A^T Aw,(F^TF + \alpha^2 A^TA)^{-1}A^TAw \rangle \nonumber \\
&\leq 0
\label{eqn:dp}
\end{align}
similarly a strict inequality if $p > 0$.

Equations \ref{eqn:de} and \ref{eqn:dp} show that increasing $\alpha^2$ implies increasing $e$ while decreasing $p$, and
\begin{align}
&\langle A^TA w,(F^TF + \alpha^2 A^TA)^{-1} A^TA w \rangle \nonumber \\ 
=& \langle (A^TA)^{1/2}w,[(A^TA)^{-1/2}F^TF(A^TA)^{-1/2} + \alpha^2 I]^{-1}(A^TA)^{1/2}w \rangle \nonumber \\ 
\le& \frac{1}{\alpha^2} \langle A^TA w, w\rangle = \frac{2}{\alpha^2}p
\end{align}
In view of equation \ref{eqn:de},
\begin{equation}
\label{eqn:lep}
\frac{de}{d\alpha^2}  \le 2p.
\end{equation}
with this inequality also being strict if $p > 0$.

Suppose the current weight is $\alpha^2_c$ and denote a candidate for an updated weight by $\alpha^2_+$. Then from inequality \ref{eqn:lep},
\begin{equation}
e(\alpha^2_+)-e(\alpha^2_c) \le \int_{\alpha^2_c}^{\alpha^2_+} 2p d\alpha^2
\end{equation}
If $\alpha^2_+ \ge \alpha^2_c$, then in view of inequality \ref{eqn:dp}, the above is
\begin{equation}
\label{eqn:basic}
\le 2 p(\alpha^2_c) (\alpha^2_+-\alpha^2_c)  
\end{equation}

Let us suppose that $e(\alpha^2_c) < e_{+}$. Then setting 
\begin{equation}
\label{eqn:alphasecant}
\alpha^2_+ = \alpha^2_c + \frac{e_{+}-e(\alpha^2_c)}{2p(\alpha^2_c)} 
\end{equation}
implies via inequality \ref{eqn:basic} that 
\begin{equation}
e(\alpha^2_+)-e(\alpha^2_c) \le e_{+}-e(\alpha^2_c)
\end{equation}

Assuming that $p(\alpha^2_+) > 0$, hence $p(\alpha^2)>0$ for $\alpha^2_c \le \alpha^2 \le \alpha^2_+$,  we conclude that if $\alpha^2_+$ is given by the rule \ref{eqn:alphasecant},
then
\begin{equation}
\label{eqn:assert}
e(\alpha^2_c) < e(\alpha^2_+) \le e_{+}
\end{equation}
That is, unless $p(\alpha^2_+) = 0$ (in which case a physical solution of the inverse problem has been reached),  $e(\alpha^2_+)$ is larger than $e(\alpha^2_c)$ but in any case does not exceed $e_+$. The rule \ref{eqn:alphasecant} therefore provides a feasible updated $\alpha^2$ consistent with the upper bound in the discrepancy principle.


\section{stuff}
In many cases, the constraint posed by membership of the second component $w$ in the subspace $\lW$ is an approximation to an idealized limit $\lambda \rightarrow 0$.
It may be that $W_0 = \cap_{\lambda > 0}\lW$ is non-trivial, in which
case $F$ may be viewed as an extension of $F_0 = F|_{ M \times
  W_0}$. In that case, $F_0$ and $F$ realize the idealized extension
structure discussed in many prior works on extended inversion (for
example, \cite[]{geoprosp:2008}): $F_0$ is the physical modeling
operator, and $W_0$ consists of second components of physical or
feasible models. However, in many of these examples, $W_0$ is not
computationally accessible, or - even if it is - gives results that
closely approximate those obtained with $\lW$ for small $\lambda$. For
other examples, for instance that  discussed later in this paper (and
implicitly in \cite[]{Warner:16}, for instance), $W_0=\{0\}$ and the
family $\{\lW: \lambda > 0\}$ actually embodies all of the necessary
physics.


\bibliographystyle{seg}
\bibliography{../../bib/masterref}