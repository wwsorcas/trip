\title{Implementation of Adaptive Waveform Inversion via a Penalty Function}
\author{Huiyi Chen, Susan E. Minkoff, and William W. Symes}

\begin{abstract}
goes here.
\end{abstract}

\section{Introduction}

\section{Preliminaries}


$Q \subset \bR^3 \times \bR^3$ is the finite ``acquisition'' set of source-receiver location pairs $(\bx_r,\bx_s)$, assumed disjoint from the diagonal (sources and receivers non-coincident). $Q_s$ denotes projection onto second factor (source locations occurring in $Q$).

%$P = \bR^{|Q|}$ = real functions on $Q$, with Euclidean norm (no attempt to scale for geometry of acquisition).

The data to be inverted consists of $|Q|$ functions of time (``traces''). The time interval of recording is assumed to be the same for all traces, namely $[0,t_D]$. Traces are assumed square-integrable, members of $ L^2([0,t_D])$ - indeed the square of the trace $L^2$ norm is essentially the energy transferred from the surrounding acoustic fluid to the recording instrument \cite[]{SantosaSymes:00}. The set of traces forms the Hilbert space $D$ = data Hilbert space = $\bigoplus_{(\bx_r,\bx_s) \in Q} L^2([0,t_D])$. Note that no attempt is made to weight the traces for the spatial distribution of the acquisition coordinates.

The pressure field $p$, sampled at the receiver locations, is a component of the solution ${p,\bv}$ of the linear acoustic wave equation
\begin{eqnarray}
  \label{eqn:awe}
  \frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv +
                                      w_*(t) \delta(\bx-\bx_s); \\
  \frac{\partial \bv}{\partial t} & = & - \beta \nabla p; \\
  p, \bv & = & 0 \mbox{ for }  t \ll 0.
\end{eqnarray}
The right-hand side is localized in space at a source position $\bx_s \in Q_s$, with time dependence given by a {source wavelet} $w_* \in C_0^{\infty}(\bR)$. Details about the source mechanism are in fact difficult to measure directly and could justifiably be included in the parameters to be estimated in the solution of the inverse problem, but for the purposes of this discussion the wavelet $w_*$ is assumed known.

The acoustic parameter fields, bulk modulus ($\kappa$) and buoyancy ($\beta$) appearing in the system \ref{eqn:awe} are assumed smooth and uniformly bounded.
Since the speed of propagation is bounded as is the set of source locations $Q_s$ and the interval of propagation $[0,t_D]$, the values of $\kappa(\bx)$ and $\beta(\bx)$ for sufficiently lartge $|\bx|$ have no effect on the solution of the system \ref{eqn:awe}, so $\log \kappa$ and $\log \beta$ can be assumed square-integrable without loss of generality. The set of feasible models $M$ is a  bounded open subset of ${\cal M} = C^{\infty}(\bR^3) \times C^{\infty}(\bR^3) \cap L^{\infty}(\bR^3) \times L^{\infty}(\bR^3) \cap L^2(\bR^3) \times L^2(\bR^3)$.

Under these conditions, the following conclusions may be drawn about the system \ref{eqn:awe}:
\begin{itemize}
  \item[A1. ] For $m = (\log \kappa, \log \beta) \in M$, shot location ${\bf x}_s\in Q_s$, there exist distributions (pressure and velocity fields) $p(\cdot,\cdot;\bx_s)$, ${\bf v}(\cdot,\cdot;\bx_s)$ satisfying the system \ref{eqn:awe};
  \item[A2. ] $p(\cdot,\cdot,t;\bx_s)$, ${\bf v}(\cdot,\cdot;\bx_s)$ are smooth in the punctured space-time $\bR^4 \setminus \{(\bx_s,t): t \in \bR\}$;
    \item[A3. ] For $(\log \kappa, \log \beta) \in M$, define 
\[
  F[m](\bx_r,t;\bx_s) = p(\bx_r,t;\bx_s), \, (\bx_r,\bx_s) \in Q, \, t \in [0,t_D],
\]
for the pressure field $p$ solving \ref{eqn:awe}. The mapping ({\em forward map}, {\em modeling operator},...) $F: M \rightarrow D$ so defined satisfies
\begin{itemize}
  \item[A3.1 ] $\|F[m](\bx_r,\cdot;\bx_s)\|_{L^2([0,t_D])}$ is uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$.
  \item[A3.2 ] $F$ has a directional derivative $DF[m]: {\cal M} \rightarrow D$ for every $m \in M$;
  \item[A3.3 ] for every $m \in M$, $DF[m]$ extends to a bounded map on ${\cal H} = L^2(\bR^3) \times L^2(\bR^3)$.
  \end{itemize}
\end{itemize}
See \cite{Symes:23a} for detailed statements and justifications of these assertions.

{\bf Note:}
$F$ is the mapping implemented (in finite difference approximation) in {\tt asg.fsbop}, with the additional constraint that the buoyancy field is fixed so that the computational version of ${\cal M}$ consists of (gridded) bulk modulus fields.

Adaptive filters are collections of traces defined on a (possibly)
different time interval, usually conveniently chosen to be
time-symmetric, that is, $[-t_U,t_U]$. The trace geometry of adaptive
filters is otherwise the same as that of the data traces. $U$ will denote the  adaptive
filter Hilbert space: $U= \bigoplus_{(\bx_r,\bx_s) \in Q} L^2([-t_U,t_U])$.

The central object in AWI is the adaptive-filter-to-data operator $S:
M \rightarrow {\cal B}(U,D)$, computed by trace-by-trace convolution
with predicted data: $S[m]u = F[m] * u$.
This notation for convolution in time hides a number of important details: none of the arguments are defined on the entire real line, so extension and restriction must be involved in the definition. Define $\Pi_D: \bigoplus_{(\bx_r,\bx_s) \in Q} L^2(\bR) \rightarrow D$ to be the restriction operator: that is, for $t \in [0,t_D]$, $\Pi_Df(\bx_r,t;\bx_s) = f(\bx_r,t;\bx_s)$. Then its adjoint $\Pi_D^T$ is the zero extension operator: that is, $\Pi_D^Tf(\bx_r,t;\bx_s) = f(\bx_r,t;\bx_s)$ if $t \in [0,t_D]$, $= 0$ else. Define $\Pi_U:  \bigoplus_{(\bx_r,\bx_s) \in Q} L^2(\bR) \rightarrow U$ similarly.

The meaning of convolution in the definition of $S$ is: extension followed by (trace-by-trace) convolution followed by restriction. That is, for $u \in U, f \in D$,
\begin{equation}
  \label{eqn:convdef}
  K[u]f(\bx_r,t;\bx_s) = \Pi_D\left(\int\,ds\,(\Pi_U^Tu)(\bx_rt-s;\bx_s) (\Pi_D^Tf)(\bx_r,s;\bx_s)\right)
\end{equation}
So defined, $K:U \rightarrow {\cal B}(D,D)$ is continuous and bilinear thanks to Young's inequality.

Convolution is commutative, so the right-hand side of the definition \ref{eqn:convdef} could also be viewed as defining $L: D \rightarrow {\cal B}(U,D)$, for $u \in U, f \in D$,
\begin{equation}
  \label{eqn:altconvdef}
  L[f]u = K[u]f
\end{equation}

The convolution operator implemented in the module {\tt segyvc} actually implements a time-discrete version of $K$ (or $L$). Note that the transpose $K[u]^T$ is a restricted version of cross-correlation, implemented as the transpose of {\tt segyvc.ConvolutionOperator}.

A proper definition of $S[m]$ is thus
\begin{equation}
  \label{eqn:sdef}
  S[m]u = K[u]F[m] = L[F[m]]u.
\end{equation}
$S$ has a directional partial derivative with respect to its first argument, given by
\begin{equation}
  \label{eqn:dsdef}
  D_m(S[m]u) \delta m = K[u]DF[m] \delta m,\,m\in M, u \in U, \delta m \in {\cal M}.
\end{equation}
Since $DF[m]$ extends to a continuous linear map: ${\cal H} \rightarrow D$, the same is true of $D_mS[m]u$, $m \in M, u \in U$..

Since operation is trace-by-trace, that is block-diagonal, $S[m] = \mbox{diag}_{(\bx_r,\bx_s) \in Q}S[m]_{\bx_r,\bx_s}$. Fact A3.1 implies that $\|S[m]_{\bx_r,\bx_s}\|_{{\cal B}(L^2([-t_U,t_U]),L^2([0,t_D]))}$ is also uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$. A similar statement applies to the derivative.

No-zero-traces Assumption: there exists $C>0$ so that for all $(\bx_r,\bx_s) \in Q$,
$\|d(\bx_r,\cdot;\bx_s)\|_{L^2([0,t_D])} \ge C.$

Observable Data Assumption: there exists $C>0$ so that for all $m \in M, (\bx_r,\bx_s) \in Q$,
\begin{equation}
  \label{eqn:obsdata}
  \|S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)\|_{L^2[-t_U,t_U]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}
\end{equation}.
That is, the projection of each trace onto the range of convolution of the corresponding predicted data trace is coercive.  

$\sigma > 0$ = Tihonov regularization weight

$\alpha \ge 0$ = penalty weight

Definition of AWI penalty objective divides into two parts:

\subsection{ Unpenalized adaptive filter}
The unpenalized filter $u_{0,\sigma}[m]$ solves the regularized least squares data fitting problem:  given $m \in M$, minimizes
\[
 J_{0,\sigma}[u,m] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \sigma^2 \|u\|^2_U).
\]
Since $\sigma > 0$, the normal operator is uniformly bounded below and of class $C^1$ in $M$ and $D$. So a unique solution $u_{0,\sigma}[m] \in C^1(M, U)$ exists.

Also, the normal operator is block-diagonal, so each trace solves a least-squares problem:
\begin{equation}
  \label{eqn:blocknormal}
  (S[m]_{\bx_r,\bx_s}^TS[m]_{\bx_r,\bx_s} + \sigma^2 I)u_{0,\sigma}[m](\bx_r,\cdot; \bx_s)= S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)
\end{equation}

According to Fact A3.1 and the Observable Data Assumption, the left-hand side of equation \ref{eqn:blocknormal} is bounded by a multiple of $\|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]}$, uniformly in $m \in M, (\bx_r,\bx_s) \in Q$. According to the Observable Data Assumption, the right-hand side is bounded below by a multiple of $\|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}$, uniform in the same sense. Conclude that there exists $C>0$ so that
\begin{equation}
  \label{eqn:u0lower}
  \|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}
\end{equation}
uniformly in $m \in M, (\bx_r,\bx_s) \in Q$.

\subsection{AWI Penalty Operator}
The penalty operator is also block diagonal, that is, operates independently on each trace. For the trace at $(\bx_r,\bx_s) \in Q$, 
\begin{equation}
  \label{eqn:ppf}
  T_{\sigma}[m]u (\bx_r,t;\bx_s) = \frac{t u(\bx_r,t;\bx_s)}{\|u_{0,\sigma}[m](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]} }
\end{equation}
That is, this penalty operator scales the trace by time, so that the output is small if the nonzero samples are concentrated near $t=0$. The second scaling, by the reciprocal of norm of the trace $u_{0,\sigma}[m](\bx_r,\cdot;\bx_s)$, plays an essential role in linking the AWI objective to the travel-time residual, as is explained by \cite{HCSMWS:23a}
This operator is well-defined and $C^1$ as function of $m$ due to the
bound \ref{eqn:u0lower} and the the No-zero-trace assumption.

To express the penalty operator $T_{\sigma}[m]$ conveniently, along
with similar operators appearing in the expression for the gradient
developed below, introduce functions
\begin{enumerate}
\item $R: U \rightarrow ({\bf R}^{+})^{|Q|}$,
  \[
    Ru((\bx_r,\bx_s)) = \|u(\bx_r, \cdot;\bx_s)\|
  \]
\item $W: {\bf R}^{|Q|} \times {\bf R} \times {\bf R}^{+}
  \rightarrow {\cal B}(D,D)$,
  \[
    W(q,p_s, p_t)f (\bx_r,t;\bx_s) = t^{p_t}
    q(\bx_r,\bx_s)^{p_s}f(\bx_r,t;\bx_s)
  \]
\end{enumerate}
Use the same notation for the similar function with $D$ replaced by
$U$. Note that $W$ is only well-defined if either $p_s \ge 0$ or $q>0$.

With these notations, $T_{\sigma}[m]$ may be expressed as
\begin{equation}
  \label{eqn:talt}
  T_{\sigma}[m] = W(Ru_{0,\sigma}[m],-1, 1)
\end{equation}

{\bf Note:} IWAVE implements the mappings $R$ and $W$ in the form of
the commands {\tt iwave/trace/main/rms.x} and {\tt
  iwave/trace/main/txgain.x}. The natural python implementation of $R$
would output the vector of trace norms as a python array (or a NumPy
ndarray). However that would require extracting the number of traces
from the input, and checking that the inputs are compatible. This is
already implicitly done in the IWAVE manipulations of SU trace
data. So {\tt iwave/trace/main/rms.x} stores the vector of trace norms
as the 0th data sample in a set of traces, one for each trace of the
input data: all header words are the same except for {\tt ns}, which
is set to 1.

The python functions {\tt awi.setrms} and {\tt awi.applytxgain} call
the two IWAVE commands, manipulating the data as SU data files, rather
than as VCL vectors. That is, these are simply functions, not {\tt
  vcl.Function} objects. Since VCL vectors defined on a {\tt segyvc.Space}
have access to their data files (keyword {\tt data}), these are
convenient to use to express the action of $R$ and $W$ in AWI penalty operator,
gradient, and so on.

\section{Objectives}
Define for $\alpha \ge 0$
\[
 J_{\alpha,\sigma}[u,m,d] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \alpha^2\|T_\sigma[m]u\|_U^2 + \sigma^2 \|u\|^2_U).
\]
Note that the notation is consistent, that is, reduces to $J_{0,\sigma}$ as defined above for $\alpha=0$.

The variable projection reduction of $J_{\alpha,\sigma}$ is
\begin{equation}
  \label{eqn:jtilde}
 \tilde{J}_{\alpha,\sigma}[m,d] = \frac{1}{2}(\|S[m]u_{\alpha,\sigma}[m,d] - d\|_D^2 + \alpha^2\|T_{\sigma}[m]u_{\alpha,\sigma}[m,d]\|_U^2 + \sigma^2 \|u_{\alpha,\sigma}\|^2_U).
\end{equation}
in which $u_{\alpha,\sigma}[m,d]$ is the minimizer of $J_{\alpha\sigma}[u,m,d]$ over $u \in U$, that is, the solution of the normal equation
\begin{equation}
  \label{eqn:normal}
  (S[m]^TS[m] + \alpha^2T_{\sigma}[m]^TT_{\sigma}[m] + \sigma^2I)u_{\alpha,\sigma}[m] = S[m]^Td
\end{equation}
Note that $u_{0,\sigma}[m]$ solves the above system for $\alpha=0$, and that the left-hand side is block-diagonal, similar to the left-hand side of equation \ref{eqn:blocknormal}.

Computing $\tilde{J}_{\alpha,\sigma}$ requires the following steps:
\begin{enumerate}
\item given $m$, $d$, $\alpha$, and $\sigma$, compute $u_{0,\sigma}[m]$ by solving the normal equations \ref{eqn:normal} for $\alpha=0$;
\item use $u_{0,\sigma}[m]$ to construct $T_{\sigma}[m]$ by means of
  equation \ref{eqn:talt};
\item use $S[m]$, $T_{\sigma}[m]$ to compute $u_{\alpha,\sigma}[m]$ by solving the normal equations \ref{eqn:normal} with the given value of $\alpha$;
\item assemble $\tilde{J}_{\alpha,\sigma}[m]$ according to equation \ref{eqn:jtilde}.
\end{enumerate}

Therefore evaluating $\tilde{J}_{\alpha,\sigma}$ involves solution of
two least-squares problems, in sequence.

{\bf Note:} The implementation {\tt vcalg.conjgrad} of the Conjugate Gradient algorithm for the normal equations optionally returns the residual vector (parameter {\tt e}). The obvious way to use this is to create a 3-component product space for the output of the block-column operator $(S[m],\alpha T[m], \sigma I)^T$, and set the right-hand side vector (parameter {\tt b})
to $(d,0,0)^T$. Then on return one-half the norm-squared of {\tt e} is precisely the (approximate) value of $\tilde{J}_{\alpha,\sigma}[m]$.

\section{Derivatives}
The fundamental result of variable projection theory
\cite[]{GolubPereyra:73} applies under the conditions presented here:
the stationary points of $J_{\alpha,\sigma}$ and
$\tilde{J}_{\alpha,\sigma}$ are in bijective correspondence. Moreover,
\[
D_m\tilde{J}_{\alpha,\sigma}[m]\delta m D_mJ[u,m]\delta m|_{u =
  u_{\alpha,\sigma}[m,d]}
\]
\begin{equation}
= \langle (D_mS[m]u\delta m)_{u =
  u_{\alpha,\sigma}[m]} (S[m]u_{\alpha,\sigma}[m]-d \rangle + \alpha^2 \langle D_m(T_{\sigma}[m]u)\delta m)_{u=u_{\alpha,\sigma}[m]},T_{\sigma}[m]u_{\alpha,\sigma}[m]\rangle_U.
\label{eqn:basederiv}
\end{equation}
Note that the regularization term does not appear in the VPM gradient expression, since it does not depend explicitly on $m$.

This section presents the derivative of $\tilde{J}_{\alpha,\sigma}$, expressed in terms of
\begin{itemize}
  \item trace-by-trace convolution and cross-correlation operators,
  \item the derivative $DF[m]$ and its adjoint, and
  \item trace-by-trace scaling via the operators $R$ and $W$.
\end{itemize}
These the first two items are common components of typical FWI implementations. The
third is necessary to express the AWI penalty operator.

\subsection{Single Trace}

Since both $S$ and $T$ are block-diagonal, acting trace-by-trace, I
will first compute the derivatives in the single-trace case. For the
remainder of this section, $F$, $S$, $T$, $d$, $u$, and $u_0$ denote
the restriction of the corresponding operators and vectors to a trace subspace, that is, for some $(\bx_r,\bx_s) \in Q$,
\begin{eqnarray*}
  F[m] = F[m]_{\bx_r,\bx_s}&: & M \rightarrow D_{\bx_r,\bx_s} =
                 L^2[0,t_D] \\
  S[m] = S[m]_{\bx_r,\bx_s}&: & L^2[-t_U,t_U] \rightarrow L^2[0,t_D] \\
  T[m] = T[m]_{\bx_r,\bx_s}&: & L^2[-t_U,t_U] \rightarrow L^2[-t_U,t_U] \\
  u_{0,\sigma}[m] &=& u_{0,\sigma}[m](\bx_r,\cdot;\bx_s).
\end{eqnarray*}
Inner products and transposes are implicit in these definitions.

The derivative of $S$ is related to the derivative of $F$ by
convolution (definition \ref{eqn:sdef}), so the first term in the
right-hand side of equation \ref{eqn:basederiv} is
\[
  \langle D_m(S[m]u)\delta m, S[m]u-d \rangle = \langle
  K[u]DF[m]\delta m, S[m]u-d \rangle
\]
\begin{equation}
  \label{eqn:resderiv}
  = \langle DF[m]\delta m, K[u]^T(S[m]u-d) \rangle
\end{equation}

With the notational conventions listed above, the normal equation \ref{eqn:blocknormal} for $u_{0,\sigma}[m]$ implies
\[
  D_mu_{0,\sigma}[m] \delta m = (S[m]^TS[m] + \sigma^2I)^{-1}
\]
\begin{equation}
  \label{eqn:blockderiv}
  \times ((DS[m]^{T}\delta m) (d - S[m]u_{0,\sigma}[m]) - S[m]^T(DS[m]\delta) m u_{0,\sigma}[m])
\end{equation}
The single-trace version of the definition \ref{eqn:ppf} is
\[
  T_{\sigma}[m]u)(t) = \frac{t u(t)}{\|u_{0,\sigma}[m]\|},\,u \in L^2[-t_U,t_U],
\]
so
\[
D(T_{\sigma}[m] u)\delta m = -\frac{t u}{\|u_{0,\sigma}[m]\|^3}\langle D_mu_{0,\sigma}[m]\delta u, u_{0,\sigma}[m]\rangle.
\]
\[
  =-\frac{t u}{\|u_{0,\sigma}[m]\|^3}\langle  (S[m]^TS[m] +
  \sigma^2I)^{-1}((DS[m]^{T}\delta m) (d - S[m]u_{0,\sigma}[m])
\]
\[
   - S[m]^T(DS[m]\delta m) u_{0,\sigma}[m]), u_{0,\sigma}[m] \rangle
\]
Define
\begin{equation}
  \label{eqn:defvsigma}
  v_{\sigma} = (S[m]^TS[m]+ \sigma^2I)^{-1} u_{0,\sigma}[m] \in
  L^2[-t_U,t_U].
\end{equation}
Then
\[
  D(T_{\sigma}[m] u)\delta m =
  \frac{t u}{\|u_{0,\sigma}[m]\|^3}[\langle (DS[m]\delta
  m)v_{\sigma}[m], S[m]u_{0,\sigma}[m]-d\rangle
\]
\[+
  \langle S[m]v_{\sigma}[m], (DS[m]\delta m) u_{0,\sigma}[m] \rangle ]
\]
and
\[
  \langle D(T_{\sigma}[m]u)\delta m, T_{\sigma}[m]u\rangle =
\frac{\langle t u, tu \rangle}{\|u_{0,\sigma}[m]\|^4}
\]
\[
\times [\langle (DS[m]\delta
  m)v_{\sigma}[m], S[m]u_{0,\sigma}[m]-d\rangle
+
  \langle S[m]v_{\sigma}[m], (DS[m]\delta m) u_{0,\sigma}[m] \rangle ]
\]
\[
  = \frac {\|t u\|^2}{\|u_{0,\sigma}[m]\|^4}
\]
\[
  \times [\langle DF[m]\delta m, K[v_{\sigma}[m]]^T(S[m]u_{0,\sigma}[m]-d)
+
K[u_{0,\sigma}[m]]^T S[m]v_{\sigma}[m]\rangle ]
\]

\[
  = \langle DF[m]\delta m, W(RT_{\sigma}u,2,0)W(Ru_{0,\sigma}[m],-2,0)
\]
\begin{equation}
  \label{eqn:penderiv}
  \times ( K[v_{\sigma}[m]]^T(S[m]u_{0,\sigma}[m]-d)
+
K[u_{0,\sigma}[m]]^T S[m]v_{\sigma}[m])\rangle 
\end{equation}

Combine equations \ref{eqn:resderiv} and \ref{eqn:penderiv} with
equation \ref{eqn:basederiv} to obtain
\[
  \nabla \tilde{J}_{\alpha,\sigma}[m] =
  DF[m]^T\large[ K[u_{\alpha,\sigma}[m]]^T (S[m]u_{\alpha,\sigma}[m]-d)
\]
\[
  + \alpha^2 W(RT_{\sigma}[m]u_{\alpha,\sigma}[m],2,0)W(Ru_{0,\sigma}[m],-2,0) (
  K[v_{\sigma}[m]]^T(S[m]u_{0,\sigma}[m]-d)
\]
\begin{equation}
  \label{eqn:deriv}
+ K[u_{0,\sigma}[m]]^T S[m]v_{\sigma}[m]) \large]
\end{equation}
with $v_{\sigma}$ defined in equation \ref{eqn:defvsigma}.

\noindent{\bf Remark:} ``Gradient'' should really be in quotes, as $F$ is not actually differentiable, or even well-defined, in any open subset of ${\cal H}$. It is however well-defined in the intersection of $M$ with any ${\cal H}$-closed subspace of ${\cal M}$. The latter is necessarily finite-dimensional, but all feasible calculations are carried out in finite-dimensional settings, so that's OK.

\subsection{Multiple Traces}
The general case follows immediately from the single trace case, since
every operator ($S[m]$, $K[...]$, $W$, $R$) ,appearing in the right-hand side of expression
\ref{eqn:deriv} is block-diagonal (trace-by-trace), with the exception
of $DF[m]^T$. This latter is a reduction operation, i.e. a block-row
operation, so its expression is also the same. That is, equation
\ref{eqn:deriv} also correctly expresses the gradient in the
multi-trace case.

The gradient is unlikely to be needed without the value
of $\tilde{J}_{\alpha,\sigma}$, so 
$T_{\sigma}[m]u_{\alpha,\sigma}$ will have been computed and can be
cached. The same is true of the vector of trace norms
$Ru_{0,\sigma}[m]$. So all ingredient for application of
$W(Ru_{0,\sigma}[m],-2,0)W(RT_{\sigma}[m] u_{\alpha,\sigma}[m],2,0)$

\begin{enumerate}
\item given $m$, $d$, $\alpha$, and $\sigma$, compute
  $u_{0,\sigma}[m]$ by solving the normal equations \ref{eqn:normal}
  for $\alpha=0$, and $T_{\sigma}[m]=W(Ru_{0,\sigma},-1,1)$ (this has
  likely already been done to compute the value of $\tilde{J}_{\alpha,\sigma}$;
\item use $S[m]$, $T_{\sigma}[m]$ to compute $u_{\alpha,\sigma}[m]$ by
  solving the normal equations \ref{eqn:normal}, and record
  $T_{\sigma}[m]u_{\alpha,\sigma}[m]$ as a by-product;
\item compute $v_{0\sigma} \in U$ by solving $(S[m]^TS[m] + \sigma^2 I)v_{0,\sigma} = u_{0,\sigma}[m]$
\item compute $r_0 = S[m]u_{0,\sigma}[m]-d,\,r_{\alpha} = S[m]u_{\alpha,\sigma}[m]-d$ (these may be by-products of steps 1 and 3;
\item compute four vectors in $D$:
  \begin{enumerate}
  \item $G_1 = K[u_{\alpha,\sigma}[m]]^T r_{\alpha}$
  \item $G_2 = K[v_{\sigma}[m]]^Tr_0$
  \item $G_3 = K[u_{0,\sigma}[m]]^T S[m]v_{\sigma}[m]$
  \item $G_0 = G_1 + \alpha^2  W(Ru_{0,\sigma}[m],-2,0)W(RT_{\sigma}[m]u_{\alpha,\sigma}[m],2,0) (G_2 + G_3)$
  \end{enumerate}
\item assemble
  $\nabla \tilde{J}_{\alpha,\sigma}[m] = DF[m]^TG_0$
\end{enumerate}

\subsection{Warner-Guasch AWI}
The (``forward'') AWI objective defined in \cite{Warner:16} is
the limit
\[
  \tilde{J}_{\sigma}[m] = \lim_{\alpha \rightarrow 0}
  \frac{1}{\alpha^2}(\tilde{J}_{\alpha,\sigma}[m]-\tilde{J}_{0,\sigma}[m])
\]
\begin{equation}
  \label{eqn:wg}
  = \frac{1}{2} \|T_{\sigma}[m]u_{0,\sigma}\|^2
\end{equation}

\cite{Warner:16} presume that $S[m]u_{0,\sigma} = d$, which is
manifestly not the case for $\sigma > 0$, however for small $\sigma$
the error is presumably negligible. Neglecting it, the gradient becomes
\[
  \nabla \tilde{J}_{\sigma}[m] = DF[m]^T
    W(Ru_{0,\sigma}[m],-2,0)W(RT_{\sigma}[m]u_{0,\sigma}[m],2,0)
\]
\begin{equation}
  \label{eqn:wggrad}
\times K[u_{0,\sigma}[m]]^T S[m](S[m]^TS[m] + \sigma^2I)^{-1}u_{0,\sigma}[m]).
\end{equation}

\bibliographystyle{seg}
\bibliography{../../bib/masterref}