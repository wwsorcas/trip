\title{Computation of the Reduced AWI Penalty Function and its Gradient}
\author{William W. Symes}

\begin{abstract}
goes here.
\end{abstract}

\section{Definitions}
Hilbert space $X$ has inner product $\langle \cdot,\cdot \rangle_X$ and norm $\|\cdot\|_X$.

$Q \subset \bR^3 \times \bR^3$ = finite ``acquisition'' set of source-receiver location pairs $(\bx_r,\bx_s)$, assumed disjoint from the diagonal (sources and receivers non-coincident). $Q_s$ = projection onto second factor (source locations occurring in $Q$).

$P = \bR^{|Q|}$ = real functions on $Q$, with Euclidean norm (no attempt to scale for geometry of acquisition).

$[0,t_d]$ = acquisition time interval

$[-t_U,t_U]$ = adaptive filter time interval

$D$ = data Hilbert space = $\bigoplus_{(\bx_r,\bx_s) \in Q} L^2([0,t_D])$ (again, no attempt to scale for geometry of acquisition).

$U$ = adaptive filter Hilbert space = $\bigoplus_{(\bx_r,\bx_s) \in Q} L^2([-t_U,t_U])$

$w_* \in C_0^{\infty}(\bR)$ = known point source wavelet

$M = $ model set, bounded open subset of $ C^{\infty}(\bR^3) \times C^{\infty}(\bR^3) \cap L^{\infty}(\bR^3) \times L^{\infty}(\bR^3)$.

\begin{theorem}
  \label{thm:eu}
For $(\log \kappa, \log \beta) \in M$, shot location ${\bf x}_s\in Q_s$, there exist distributions (pressure and velocity fields) $p(\cdot,\cdot,t;\bx_s)$, ${\bf v}(\cdot,\cdot;\bx_s)$ satisfying
\begin{itemize}
\item
  \begin{eqnarray}
    \label{eqn:awe}
    \frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv +
                                        w(t;\bx_s) \delta(\bx-\bx_s); \\
    \frac{\partial \bv}{\partial t} & = & - \beta \nabla p; \\
    p, \bv & = & 0 \mbox{ for }  t \ll 0.
  \end{eqnarray}
\item
  $p, \bv$ are smooth in the punctured space-time $\bR^4 \setminus \{(\bx_s,t): t \in \bR\}$.
\end{itemize}
\end{theorem}
\begin{proof}
  See Appendix A.
\end{proof}

Define $F: M \rightarrow D$ by
\[
  F[m](\bx_r,t;\bx_s) = p(\bx_r,t;\bx_s), \, (\bx_r,\bx_s) \in Q, \, t \in [0,t_D],
\]
for the pressure field $p$ solving \ref{eqn:awe}.

\begin{theorem}
  \label{thm:mbd}
  $\|F[m](\bx_r,\cdot;\bx_s)\|_{L^2([0,t_D])}$ is uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$.
\end{theorem}
\begin{proof}
  See Appendix A.
\end{proof}

$S: M \rightarrow {\cal B}(U,D)$ = trace-by-trace convolution with predicted data: $S[m]u = F[m] * u$. This notation for convolution in time hides a number of important details: none of the arguments are defined on the entire real line, so extension and truncation are involved in the definition. I make these details explicit later in the paper.

Due to smoothness of $F$, $S \in C^1(M,{\cal B}(U,D))$. Operation is trace-by-trace, that is block-diagonal: $S[m] = \bigoplus_{(\bx_r,\bx_s) \in Q}S[m]_{\bx_r,\bx_s}$. Theorem \ref{thm:mbd} implies that $\|S[m](\bx_r,\cdot;\bx_s)\|_{{\cal B}(L^2([-t_U,t_U]),L^2([0,t_D]))}$ is also uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$.

No-zero-traces Assumption: there exists $C>0$ so that for all $(\bx_r,\bx_s) \in Q$,
$\|d(\bx_r,\cdot;\bx_s)\|_{L^2([0,t_D])} \ge C.$

Observable Data Assumption: there exists $C>0$ so that for all $m \in M, (\bx_r,\bx_s) \in Q$,
\begin{equation}
  \label{eqn:obsdata}
  \|S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)\|_{L^2[-t_U,t_U]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}
\end{equation}.
That is, the projection of each trace onto the range of convolution of the corresponding predicted data trace is coercive.  

$\sigma > 0$ = Tihonov regularization weight

$\alpha \ge 0$ = penalty weight

Definition of AWI penalty objective divides into two parts:

Part 1: Unpenalized adaptive filter $u_{0,\sigma}[m,d]$ solving regularized least squares data fitting problem:  given $m \in M$, minimizes
\[
 J_{0,\sigma}[u,m,d] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \sigma^2 \|u\|^2_U).
\]
Since $\sigma > 0$, normal operator is uniformly bounded below and of class $C^1$ in $M$ and $D$. So unique solution exists, and $u_{0,\sigma} \in C^1(M \times D, U)$.

Also, the normal operator is block-diagonal, so each trace solves a least-squares problem:
\begin{equation}
  \label{eqn:blocknormal}
  (S[m]_{\bx_r,\bx_s}^TS[m]_{\bx_r,\bx_s} + \sigma^2 I)u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)= S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)
\end{equation}
According to Theorem \ref{thm:mbd} and the Observable Data Assumption, the left-hand side is bounded by a multiple of $\|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]}$, uniformly in $m \in M, (\bx_r,\bx_s) \in Q$. According to the Observable Data Assumption, the right-hand side is bounded below by a multiple of $\|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}$, uniform in the same sense. Conclude that there exists $C>0$ so that
\begin{equation}
  \label{eqn:u0lower}
  \|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_D]}
\end{equation}
uniformly in $m \in M, (\bx_r,\bx_s) \in Q$.

Part 2:
Preconditioned penalty operator: block diagonal, 
\begin{equation}
  \label{eqn:ppf}
  T_{\sigma}[m]u (\bx_r,t;\bx_s) = \frac{t u(\bx_r,t;\bx_s)}{\|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_U,t_U]} }
\end{equation}
Well-defined and $C^1$ as function of $m$ due to the bound \ref{eqn:u0lower} and the the No-zero-trace assumption.

\section{Objectives}
Define for $\alpha \ge 0$
\[
 J_{\alpha,\sigma}[u,m,d] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \alpha^2\|T_\sigma[m]u\|_U^2 + \sigma^2 \|u\|^2_U).
\]
Note that the notation is consistent, that is, reduces to $J_{0,\sigma}$ as defined above for $\alpha=0$.

The variable projection reduction of $J_{\alpha,\sigma}$ is
\begin{equation}
  \label{eqn:jtilde}
 \tilde{J}_{\alpha,\sigma}[m,d] = \frac{1}{2}(\|S[m]u_{\alpha,\sigma}[m,d] - d\|_D^2 + \alpha^2\|T_\sigma[m]u_{\alpha,\sigma}[m,d]\|_U^2 + \sigma^2 \|u_{\alpha,\sigma}\|^2_U).
\end{equation}
in which $u_{\alpha,\sigma}[m,d]$ is the minimizer of $J_{\alpha\sigma}[u,m,d]$ over $u \in U$. The fundamental result of variable projection theory \cite[]{GolubPeyreyra:73} applies under the conditions presented here: the stationary points of $J_{\alpha,\sigma}$ and $\tilde{J}_{\alpha,\sigma}$ are in bijective correspondence.

Computing $\tilde{J}_{\alpha,\sigma}$ takes place in two stages, corresponding to the two-stage construction of $J_{\alpha,\sigma}$:
\begin{itemize}
\item[1. ] minimize $J_{0,\sigma}$ to compute $u_{0,\sigma}$;
\item[2. ] use $u_{0,\sigma}$ to compute $u \mapsto T_{\sigma}[m]u$, and compute $u_{\alpha,\sigma}$ by minimizing $J_{\alpha,\sigma}$.
\end{itemize}
Therefore evaluating $\tilde{J}_{\alpha,\sigma}$ involves solution of two least-squares problems, in sequence.

\section{Derivatives}
This section presents the derivative of $\tilde{J}_{\alpha,\sigma}$, expressed in terms of
\begin{itemize}
  \item trace-by-trace convolution and cross-correlation operators,
  \item the derivative $D_mF[m]$ and its adjoint, and
  \item trace-by-trace scaling
\end{itemize}
These the first two items are common components of typical FWI implementations. The
third is necessary to express the AWI penalty operator.

As pointed out by \cite{GolubPeyreyra:73}, the directional derivative $D_m\tilde{J}_{\alpha,\sigma}[m,d]\delta m$ is the same as the partial directional derivative $D_mJ[u,m,d]\delta m$ with $u = u_{\alpha,\sigma}[m,d]$. Since 
\begin{equation}
D_mJ[u,m,d]\delta m = \langle D_m(S[m]u)\delta m, S[m]u-d \rangle + \alpha^2 \langle D_m(T_{\sigma}[m]u)\delta m,T_{\sigma}[m]u\rangle_U,
\label{eqn:basederiv}
\end{equation}
the key calculations are $D_mS^T$ and $D_m T^T$. 

Note that the regularization term does not appear in the VPM gradient expression, since it does not depend explicitly on $m$.

\subsection{Single Trace}

Since both $S$ and $T$ are block-diagonal, acting trace-by-trace, I will first compute the derivatives in the single-trace case. For the remainder of this section, $S$, $T$, and $u_0$ denote the restriction of the corresponding operators to a trace subspace, that is, for some $(\bx_r,\bx_s) \in Q$,
\begin{eqnarray*}
  S[m] = S[m]_{\bx_r,\bx_s}&: & L^2[-t_U,t_U] \rightarrow L^2[0,t_D] \\
  T[m] = T[m]_{\bx_r,\bx_s}&: & L^2[-t_U,t_U] \rightarrow L^2[-t_U,t_U] \\
  u_{0,\sigma} &=& u_{0,\sigma}(\bx_r,\cdot;\bx_s).
\end{eqnarray*}
Inner products and transposes are implicit in these definitions.

With these notational conventions, the normal equation \ref{eqn:blocknormal} for $u_{0,\sigma}$ implies
\begin{equation}
  \label{eqn:blocksingle}
  u_{0,\sigma}[m] = (S[m]^TS[m] + \sigma^2I)^{-1}S[m]^Td.
\end{equation}
So
\begin{equation}
  \label{eqn:blockderiv}
  D_mu_{0,\sigma}[m] \delta m = (S[m]^TS[m] + \sigma^2I)^{-1}((DS[m]^{T}\delta m) (d - S[m]u_{0,\sigma}[m]) - S[m]^T(DS[m]\delta) m u_{0,\sigma}[m])
\end{equation}

The derivative of $S$ is related to the derivative of $F$ by convolution. The relation with $T$ is more complicated.
The single-trace version of the definition \ref{eqn:ppf} is
\[
  (T_{\sigma}[m]u)(t) = \frac{t u(t)}{\|u_{0,\sigma}[m]\|},\,u \in L^2[-t_U,t_U],
\]
so
\[
D(T_{\sigma}[m] u)\delta m = -\frac{t u}{\|u_{0,\sigma}[m]\|^3}\langle D_mu_{0,\sigma}[m]\delta u, u_{0,\sigma}[m]\rangle.
\]
\[
  =-\langle  (S[m]^TS[m] + \sigma^2I)^{-1}((DS[m]^{T}\delta m) (d - S[m]u_{0,\sigma}[m]) - S[m]^T(DS[m]\delta m) u_{0,\sigma}[m]),
\]
\[ u_{0,\sigma}[m] \rangle tu \|u_{0,\sigma}[m] \|^{-3}.
\]
Since $\|tu\|^2\| u_{0,\sigma}[m]\|^{-4} = \|T_{\sigma}[m]u\|^2\| u_{0,\sigma}[m]\|^{-2}$,
\[
  D_m \frac{\alpha^2}{2}\|T_{\sigma}[m]u\|\delta m =-\alpha^2\left( \langle d-S[m]u_{0,\sigma}[m], DS[m]\delta m (S[m]^TS[m] + \sigma^2I)^{-1}u_{0,\sigma}[m]\rangle \right.
\]
\[
  \left. - \langle (DS[m]\delta m)u_{0,\sigma}[m],S[m](S[m]^TS[m] + \sigma^2I)^{-1}u_{0,\sigma}[m]\rangle \right)
\]
\[
  \times  \|T_{\sigma}[m]u\|^2\| u_{0,\sigma}[m]\|^{-2}.
\]

Recall that $S[m]u$ is the convolution of the pertinant trace of $u \in U$ with the corresponding trace of $F[m]$. These are functions defined on bounded time intervals: in particular, $F:M \rightarrow L^2([0,t_D])$. So restriction and extension must also be involved. Define $\Pi_D: L^2(\bR) \rightarrow L^2([0,t_D])$ to be the restriction operator. Then its adjoint $\Pi_D^T$ is the zero extension operator: that is, $\Pi_D^Tf(t) = f(t)$ if $t \in [0,t_D]$, $= 0$ else. Define $\Pi_U:  L^2(\bR) \rightarrow L^2([-t_U,t_U])$ similarly. Then
\begin{equation}
  \label{eqn:sdefsingle}
  S[m]u = \Pi_D(\Pi_D^TF[m] * \Pi_U^Tu)
\end{equation}

The first term in the expression \ref{eqn:basederiv} for the derivative of $\tilde{J}_{\alpha,\sigma}$ can then be re-written
\[
  \langle DS[m]u, S[m]u-d\rangle = \langle \Pi_D(\Pi_D^TDF[m]\delta m * \Pi_U^Tu), S[m]u-d\rangle
\]
\[
  = \langle \Pi_D^TDF[m]\delta m * \Pi_U^Tu, \Pi_D^T (S[m]u-d) \rangle
\]
\[
  = \langle \Pi_D^TDF[m]\delta m,  R(\Pi_U^Tu)*\Pi_D^T (S[m]u-d) \rangle
\]
\begin{equation}
  \label{eqn:firstsingle}
  = \langle DF[m]\delta m, \Pi_D(R(\Pi_U^Tu)*\Pi_D^T (S[m]u-d))\rangle
\end{equation}
In the previous two expressions, $R: L^2(\bR) \rightarrow L^2(\bR)$ is the time-reversal operator: $Rf(t) = f(-t)$.

