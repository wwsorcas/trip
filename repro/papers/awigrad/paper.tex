\title{Computation of the Reduced AWI Penalty Function and its Gradient}
\author{William W. Symes}

\begin{abstract}
goes here.
\end{abstract}

\section{Definitions}
Hilbert space $X$ has inner product $\langle \cdot,\cdot \rangle_X$ and norm $\|\cdot\|_X$.

$M$ = model set, an open set in a Banach space also pre-Hilbert structure 

$Q$ = finite ``acquisition'' set of source-receiver location pairs

$P = \bR^{|Q|}$ = real functions on $Q$, with Euclidean norm (no attempt to scale for geometry of acquisition).

$[0,t_d]$ = acquisition time interval

$[-t_u,t_u]$ = adaptive filter time interval

$D$ = data Hilbert space = $\bigoplus_{(\bx_r,\bx_s) \in Q} L^2([0,t_d])$ (again, no attempt to scale for geometry of acquisition).

$U$ = adaptive filter Hilbert space = $\bigoplus_{(\bx_r,\bx_s) \in Q} L^2([-t_u,t_u])$

$w_* \in C_0^{\infty}(\bR)$ = known point source wavelet

$F: M \rightarrow D$ = forward modeling operator, use of $w_*$ implicit. Since $w_*\in C_0^{\infty}(\bR)$  and distance from source points to receiver points is bounded below, $F \in C^1(M,D)$.

Modeling Bound Assumption: $\|F[m](\bx_r,\cdot;\bx_s)\|_{L^2([0,t_d])}$ is uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$.

$S: M \rightarrow {\cal B}(U,D)$ = trace-by-trace convolution with predicted data: $S[m]u = F[m] * u$. Due to smoothness of $F$, $S \in C^1(M,{\cal B}(U,D))$. Operation is trace-by-trace, that is block-diagonal: $S[m] = \bigoplus_{(\bx_r,\bx_s) \in Q}S[m]_{\bx_r,\bx_s}$. The Modeling Bound Assumption implies that $\|S[m](\bx_r,\cdot;\bx_s)\|_{L^2([0,t_d])}$ is also uniformly bounded over $m \in M, (\bx_r,\bx_s) \in Q$.

No-zero-traces Assumption: there exists $C>0$ so that for all $(\bx_r,\bx_s) \in Q$,
$\|d(\bx_r,\cdot;\bx_s)\|_{L^2([0,t_d])} \ge C.$

Observable Data Assumption: there exists $C>0$ so that for all $m \in M, (\bx_r,\bx_s) \in Q$,
\begin{equation}
  \label{eqn:obsdata}
  \|S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)\|_{L^2[-t_u,t_u]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_d]}
\end{equation}.
That is, the projection of each trace onto the range of convolution of the corresponding predicted data trace is coercive.  

$\sigma > 0$ = Tihonov regularization weight

$\alpha \ge 0$ = penalty weight

Definition of AWI penalty objective divides into two parts:

Part 1: Unpenalized adaptive filter $u_{0,\sigma}[m,d]$ solving regularized least squares data fitting problem:  given $m \in M$, minimizes
\[
 J_{0,\sigma}[u,m,d] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \sigma^2 \|u\|^2_U).
\]
Since $\sigma > 0$, normal operator is uniformly bounded below and of class $C^1$ in $M$ and $D$. So unique solution exists, and $u_{0,\sigma} \in C^1(M \times D, U)$.

Also, the normal operator is block-diagonal, so each trace solves a least-squares problem, and
\[
  (S[m]_{\bx_r,\bx_s}^TS[m]_{\bx_r,\bx_s} + \sigma^2 I)u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)= S[m]_{\bx_r,\bx_s}^Td(\bx_r,\cdot;\bx_s)
\]
According to the Modeling Bound Assumption, observable data assumption, the left-hand side is bounded by a multiple of $\|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_u,t_u]}$, uniformly in $m \in M, (\bx_r,\bx_s) \in Q$. According to the Observable Data Assumption, the right-hand side is bounded below by a multiple of $\|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_d]}$, uniform in the same sense. Conclude that there exists $C>0$ so that
\begin{equation}
  \label{eqn:u0lower}
  \|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_u,t_u]} \ge C \|d(\bx_r,\cdot;\bx_s)\|_{L^2[0,t_d]}
\end{equation}
uniformly in $m \in M, (\bx_r,\bx_s) \in Q$.

Part 2:
Preconditioned penalty operator: block diagonal, 
\begin{equation}
  \label{eqn:ppf}
  T_{\sigma}[m]u (\bx_r,t;\bx_s) = \frac{u(\bx_r,t;\bx_s)}{\|u_{0,\sigma}[m,d](\bx_r,\cdot; \bx_s)\|_{L^2[-t_u,t_u]} }
\end{equation}
Well-defined and $C^1$ as function of $m$ due to the bound \ref{eqn:u0lower} and the the No-zero-trace assumption.

Define for $\alpha \ge 0$
\[
 J_{\alpha,\sigma}[u,m,d] = \frac{1}{2}(\|S[m]u - d\|_D^2 + \alpha^2\|T_\sigma[m]u\|_U^2 + \sigma^2 \|u\|^2_U).
\]
Note that the notation is consistent, that is, reduces to $J_{0,\sigma}$ as defined above for $\alpha=0$.


\subsection{Computing $D_mA_{\alpha,\epsilon}[m]^T$}
The main goal of this and the next two subsections is to express the
operator in the subsection title in terms of
\begin{itemize}
  \item trace-by-trace convolution and cross-correlation operators,
  \item the derivative $D_mF[m]w_*$ and its adjoint, and
  \item trace-by-trace scaling.
\end{itemize}
These the first two bullets describe common components of FWI.The
third is necessary to express the AWI penalty operator.

Note that the second-mentioned operator describes the derivative of
the acoustic forward modeling operator for a single point source per
receiver gather, not the extended modeling operator $\bar{F}$. The
latter is not a modular component of FWI, and a self-contained
implementation is not likely to be available.

Write $\nabla \tilde{J}_{\alpha,\epsilon}$ in terms of its components:
\begin{equation}
= D_mS[m]^T e_0 + \alpha D_mT_{\epsilon}[m]^Te_2
\label{eqn:tildejgrad}
\end{equation}
where $e=(e_0,e_1,e_2)^T$ is the residual vector:
$$
e = \left[
\begin{array}{c}
S[m]u-d\\
\epsilon u\\
\alpha T[m]u
\end{array}
\right],\, u=\tilde{u}_{\alpha,\epsilon}[m].
$$
Note that the regularization term does not appear in the VPM gradient expression, since it does not depend explicitly on $m$.

Evidently the key calculations are $D_mS^T$ and $D_m T^T$. 

\subsection{Convolution Revisited}

To keep track of the data flow in subsequent calculations, it is convenient to introduce a notation for the operator of convolution by $u$. This notation must include the time ranges for input and output traces, as these define the index limits in the sums that implement the convolution operator. Looked at another way, these ranges define the domain and range of the operator, and that is how the definition in *segyvc.ConvolutionOperator* is arranged: the arguments to the class constructor are the domain and range (as *segyvc.Spaces*, type-checked) and the filename for the convolution kernel. Domain, range, and kernel must share spatial geometry (source and receiver positions, in particular number of traces, and time step; these conditions are checked. In this section, the convolution operator with compatible trace space domain $X$ and range $Y$, and kernel $z \in Z$ is denoted $C[X,Y,Z,z]$. The kernel space $Z$ is also compatible with $X$ and $Y$.

The domain, range, and kernel spaces considered here are all coordinate spaces, defined by finite time index ranges (or, in the continuum case, bounded time ranges). For such a coordinate space $X$, denote by $\Pi_X$ the orthogonal projection of the ambient space $l^2$ (or $L^2({\bf R})$ onto $X$. In the discrete case, this projection $\Pi_X$ simply extracts the values inside the characterizing index range $I_X$:
$$
(\Pi_X f)_i = f_i, \, i \in I_X,
$$
and similarly for the continuum case. The transpose projection $\Pi_X^T$ injects the array of values within the range into $l^2$, with zeros outside. So $\Pi_X^T\Pi_X$ is the projection onto the subspace of $l^2$ (or $L^2({\bf R})$) corresponding to $X$, i.e. the range of $\Pi_X^T$, and $\Pi_X \Pi_X^T = I_X$.

Define convolution $f*g$ by the usual sum or integral, for arguments $f$ and $g$ of bounded support. With these conventions,
$$
C[X,Y,Z,z]x = \Pi_Y(\Pi_Z^T z * \Pi_X^T x).
$$
The class *segyvc.ConvolutionOperator* implements precisely the discrete version of this definition.

Both discrete and continuous convolution are commutative. The same is true of the discrete, finite version of convolution implemented in *segyvc.ConvolutionOperator*, and can be expressed as the follow obvious consequence of the definition: if $X, Y, Z$ are compatible trace spaces, and $z \in Z, x \in X$, then
\begin{equation}
C[X,Y,Z,z]x = C[Z,Y,X,x]z
\label{eqn:convcomm}
\end{equation}
The transpose of convolution is expressed via the time reversal operator $f \mapsto \check{f}\, \check{f}_i = f_{-i}$:  
$$
\langle f*g,h \rangle = \langle g, \check{f}*h \rangle.
$$ 
The right-hand side defines the cross-correlation of $f$ and $h$. From the definition of $C$,
$$
\langle C[X,Y,Z,z]x, y\rangle_Y = \langle \Pi_Y(\Pi_Z^T z * \Pi_X^T x), y \rangle_Y
$$
$$
= \langle \Pi_Z^T z * \Pi_X^T x, \Pi_y^T y \rangle = \langle \Pi_X^T x, \check{(\Pi_Z^T z)}* \Pi_Y^T y \rangle
$$
Denote by $\check{Z}$ the coordinate space produced by time reversal applied to $Z$, that is, $I_{\check{Z}}$ consists of the negatives of members of $I_Z$. Then $\check{(\Pi_Z^T z)}=\Pi_{\check{Z}}\check{z}$, so the above is
$$
=\langle \Pi_X^T x, \Pi_{\check{Z}}^T \check{z} * \Pi_Y^T y \rangle =\langle x, \Pi_X(\Pi_{\check{Z}}^T \check{z} * \Pi_Y^T y) \rangle_X
$$
$$
= \langle x, C[Y,X,\check{Z},\check{z}]y \rangle_X.
$$
That is, $C[X,Y,Z,z]^T = C[Y,X,\check{Z},\check{z}]$.

Combining the commutativity relation $\ref{eqn:convcomm}$ with this identity yields the useful identity:
\begin{equation}
C[X,Y,Z,z]^Ty = C[Y,X,\check{Z},\check{z}]y =
C[\check{Z},X,Y,y]\check{z} =
C[X,\check{Z},\check{Y},\check{y}]^T\check{z}.
\label{eqn:convcommtransp}
\end{equation}

\subsection{Back to AWI penalty gradient}

The SEGY trace spaces involved in the AWI calculation are the data space $D$ and the adaptive filter space $U$. Minimization of $J_{\alpha,\epsilon}$ amounts to solution of a least squares problem with operator $C[U,D,D,F[m]w]$, that is, convolution with the predicted data:
$$
S[m]u = C[U,D,D,F[m]w_*]u = C[D,D,U,u]F[m]w_*
$$
The second expression, equivalent by the commutativity relation \ref{eqn:convcomm} to the first, is more convenient for computing the derivative with respect to $m$ of $S[m]u$:
$$
D_m(S[m]u) \delta m = C[D,D,U,u](D_mF[m]\delta m)
$$
whence ($e_0 = S[m]u_{\alpha,\epsilon}[m]-d \in D$) the first summand in the gradient of $\tilde{J}_{\alpha,\epsilon}$ is
$$
D_m(S[m]u)^T e_0 = D_mF[m]^TC[D,D,U,u]^Te_0.
$$
%The derivative $D_mF[m]$ and its adjoint are defined in the *vcl.Function* subclass *asg.fsbop*, and the convolution operator $C$ and its transpose in *segyvc.ConvolutionOperator*.

Introduce the set $SR$ of source-receiver pairs active in the data
$D$, and scale factor space $P = \{f:SR \rightarrow \bR\}$. Write
\[
  T[m]u = B_2 \circ B_1 \circ B_0[m],
\]
where 
\[
B_2: P \times U \rightarrow U, \, B_2[f,u](bx_r,t;\bx_s) = f(\bx_r,\bx_s) tu(\bx_r,t;\bx_s)
\]
\[
B_1: U \rightarrow P, \,B_1[u](\bx_r,t,\bx_s) = \|u(\bx_r,\cdot; \bx_s)\|^{-1}
\]
\[
B_0: M \rightarrow U, \, B_0[m] = u_{0,\epsilon}[m]
\]
Then
\[
 \frac{\alpha^2}{2} D_m(\|T[m]u\|_U^2)\delta m = \alpha^2\langle D_m(
 T[m]u)\delta m, T[m]u \rangle_U
\]
\[
  =\alpha^2 \langle DB_2(B_1\circ B_0[m],u)DB_1(B_0[m]) DB_0[m]\delta m,
  T[m]u\rangle_U
\]
\begin{equation}
  \label{eqn:compgrad}
  =\alpha^2 \langle DB_0[m]\delta m, DB_1(B_0[m])^TDB_2(B_1\circ
  B_0[m],u)^TT[m]u\rangle_U
\end{equation}
Next display the derivative and adjoint derivative of $B_1$ and $B_2$:
$$
(D_fB_2[f,u]\delta f)(\bx_r,t;\bx_s) = tu(\bx_r,t;\bx_s)\delta f(\bx_r,\bx_s)
$$
$$
D_f(B_2[f,u])^Te_2(\bx_r,\bx_s) = \int dt\,te_2(\bx_r,t;\bx_s)u(\bx_r,t;\bx_S);
$$
$$
D_u B_1[u]\delta u(\bx_r,\bx_s) = -\|u(\bx_r,\cdot;\bx_s)\|^{-3}\langle u(bx_r,\cdot;\bx_s), \delta u(\bx_r,\cdot;\bx_s) \rangle
$$
$$
(D_u B_1[u])^Tf(\bx_r,t;\bx_s) =
-f(\bx_r,\bx_s)\|u(\bx_r,\cdot;\bx_s\|^{-3}u(\bx_r,t;\bx_s)
$$
With these expressions, compose various components appearing on the
right hand side of equation \ref{eqn:compgrad}:
\[
  B_1\circ B_0[m] (\bx_r,\bx_s) =  \|\tilde{u}_{0,\epsilon}(\bx_r,\cdot; \bx_s)\|^{-1},
\]
\[
 DB_2(B_1\circ B_0[m],u)^Te_2 = \int dt\,te_2(\bx_r,t;\bx_s)u(\bx_r,t;\bx_s),
\]
(Note that the result is independent of the first argument.)
\[
DB_1(B_0[m])^Tf(\bx_r,t:\bx_s)=
-f(\bx_r,\bx_s)\|u_{0,\epsilon}(\bx_r,\cdot;\bx_s)\|^{-3}u_{0,\epsilon}(\bx_r,\cdot;\bx_s),
\]
and
\[
  (DB_1(B_0[m])^TDB_2(B_1\circ  B_0[m],u)^TT[m]u )(\bx_r,\cdot,\bx_s)
\]
\[
  = -\int dt\,t(T[m]u)(\bx_r,t;\bx_s)u(\bx_r,t;\bx_s)
  \|u_{0,\epsilon}(\bx_r,\cdot;\bx_s)\|^{-3}u_{0,\epsilon}(\bx_r,\cdot;\bx_s)
\]
In view of the definition of $T[m]$, this is
\begin{equation}
  \label{eqn:compgrad1}
  = -\||T[m]u(\bx_r,\cdot;\bx_s)\|^2
  \|u_{0,\epsilon}(\bx_r,\cdot;\bx_s)\|^{-2}u_{0,\epsilon}(\bx_r,\cdot;\bx_s)
\end{equation}
It remains to compute $ DB_0[m]\delta m,$:
$$
B_0[m] = u_{0,\epsilon}[m] = (S[m]^TS[m] + \epsilon^2 I)^{-1}S[m]^Td
$$
$$
= (C[U,D,D,F[m]w_*]^TC[U,D,D,F[m]w_*] + \epsilon^2 I)^{-1}C[U,D,D,F[m]w_*]^Td.
$$
So
$$
D_mB_0[m]\delta m = (C[U,D,D,F[m]w_*]^TC[U,D,D,F[m]w_*] + \epsilon^2 I)^{-1}
$$
$$
\times ((D_mC[U,D,D,F[m]w_*]\delta m)^Td -(D_m(C[U,D,D,F[m]w_*]^TC[U,D,D,F[m]w_*])\delta m)u_{0,\epsilon}[m])
$$
$$
=(C[U,D,D,F[m]w_*]^TC[U,D,D,F[m]w_*] + \epsilon^2 I)^{-1}
$$
$$
\times
(D_m(C[U,D,D,F[m]w_*]^Tr_0)\delta m  - C[U,D,D,F[m]w_*]^TD_m(C[U,D,D,F[m]w_*]u_0)\delta m)
$$ 
where the derivatives are to be evaluated before the insertion of the definitions 
$$
r_0 = d -C[U,D,D,F[m]w_*]u_{0,\epsilon}[m], \, u_0 = u_{0,\epsilon}[m].
$$
From the commutativity relation \ref{eqn:convcomm},
\begin{equation}
  \label{eqn:compgrad2}
D_m(C[U,D,D,F[m]w_*]u_0)\delta m = D_m(C[D,D,U,u_0]F[m]w_*)\delta m
=C[D,D,U,u_0]D_mF[m]\delta m.
\end{equation}
Similarly, use the identity \ref{eqn:convcommtransp} to write
$$
C[U,D,D,F[m]w_*]^Tr_0 = C[U,\check{D},\check{D},\check{r_0}]^T({F[m]w_*})\check, 
$$
so
\begin{equation} \label{eqn:compgrad3}
  D_m(C[U,D,D,F[m]w_*]^Tr_0)\delta m
  =C[U,\check{D},\check{D},\check{r_0}]^T(D_mF[m]w_* \delta m)\check= C[U,D,D,F[m]w_*\delta m]^Tr_0
\end{equation}


Putting \ref{eqn:compgrad}, \ref{eqn:compgrad1}, \ref{eqn:compgrad2},
and \ref{eqn:compgrad3} together,
\[
  \frac{\alpha^2}{2} D_m(\|T[m]u\|_U^2)\delta m =
=-\alpha^2 \langle C[U,D,D,F[m]w_*]^TC[D,D,U,u_0]D_mF[m]\delta m +C[U,D,D, D_mF[m]w_*
\delta m]^Tr_0, 
\]
\[
 \||T[m]u(\bx_r,\cdot;\bx_s)\|^2
  \|u_{0,\epsilon}(\bx_r,\cdot;\bx_s)\|^{-2}u_{0,\epsilon}(\bx_r,\cdot;\bx_s)\rangle_U
  DB_1(B_0[m])^TDB_2(B_1\circ
  B_0[m],u)^TT[m]u\rangle_U
\]

Like all penalty functions, this one is best viewed as a least-squares objective of a map with values in a product space: define
$$
A_{\alpha,\epsilon}[m] =  
\left[
\begin{array}{c}
S[m]\\
\epsilon I\\
\alpha T_{\epsilon}[m]
\end{array}
\right]
, \, b = \left[
\begin{array}{c}
d\\
0\\
0
\end{array}
\right]
$$
Then
$$
J_{\alpha,\epsilon}[m,w_*,u] = 0.5\|A_{\alpha,\epsilon}[m]u-b\|^2
$$

Both Newton's method with the Kaufman Hessian approximation (described in the VCL notebook) and LBFGS or similar first-order methods apply to minimization of $\tilde{J}_{\alpha,\epsilon}[m]$. As shown in the VCL notebook, application of these algorithms to a VPM reduction like $\tilde{J}_{\alpha,\epsilon}$ requires access to two linear operator valued functions of $m$, namely $A_{\alpha,\epsilon}[m]$ and the (partial) derivative $D_m(A_{\alpha,\epsilon}[m]u)$. The former is the main actor in computation of the objective via solution of the normal equations. The latter appears in the VPM gradient formula:
$$
\nabla \tilde{J}_{\alpha,\epsilon}[m] = (D_m A_{\alpha,\epsilon}[m]u)|_{u=\tilde{u}_{\alpha,\epsilon}[m]}^T(A_{\alpha,\epsilon}[m]\tilde{u}_{\alpha,\epsilon}[m] -b)
$$
Note that $D_m A_{\alpha,\epsilon}[m]u$ is a map from the model parameter space to the product range space of $A$, and the transpose is a map in the other direction. 
