\title{Solution of an Acoustic Transmission Inverse Problem by
  Extended Inversion: Theory}
\author{William W. Symes\footnotemark[1]}
\address{\footnotemark[1]PO Box 43, Orcas, WA 98280 USA, {\tt symes@rice.edu}}

\lefthead{Symes}

\righthead{1D Acoustic Inversion: Theory}

\maketitle
\begin{abstract}
  A single-trace transmission inverse problem for the wave equation
  seeks to determine both the wave velocity in a homogenous acoustic
  medium and the transient waveform of an isotropic point source. The
  duration (support) of the source waveform and the source-to-receiver
  distance are assumed known. A least squares formulation of this
  problem exhibits the ``cycle-skipping'' behaviour observed in field
  scale problems of this type, with many local minima differing
  greatly from the global minimizer. This behaviour is eliminated by
  dropping the hard support constraint on the source waveform,
  replacing it by a soft penalty implemented as a weighted mean-square
  of the source waveform. For properly chosen weight function,
  penalizing nonzero values away from $t=0$, the velocity component of
  any stationary point differs from the global minimizer of the
  constrained least-squares formulation by a linear combination of the
  source waveform support radius and data noise-to-signal ratio. Given
  an estimate of data noise, the penalty weight can be dynamically
  adjusteded during iterative optimization to maximize predicted data
  accuracy and closely approximate a support-constrained source waveform.

\end{abstract}

\section{Introduction}
Inverse problems based on wave equations (acoustic, elastic,
Maxwell's...) are commonplace in geophysics, nondestructive materials
testing, medical imaging, and other areas of science and engineering
in which wave motion plays an important role. Often these problems are
formulated as optimization of mean square (or other scalar measure)
functions of misfit between observed and predicted data. In many
contexts, such as exploration seismology, the computational size of
these problems is very large, and even the prediction (or modeling) of
the data requires use of high-performance computing
environments. Accordingly, rapidly convergent optimization algorithms
based on Newton's method and its relatives to update model parameters
are the numerical methods of choice. These however are local, in the
sense that they converge to stationary points of the objective
functions, usually near the initial model parameters. For
high-frequency (hence high resolution) data regimes, stagnation of
local optimization methods at physically irrelevant model vectors is
frequently observed. This ``cycle-skipping'' is one of the main
obstacles to widespread successful deployment of optimization-based
methods for inverse problems in wave propagation
\cite[]{GauTarVir:86,VirieuxOperto:09,Fichtner:10,Plessix:10,Schuster:17}.

This paper shows how a modification of least-squares data fitting via
{\em modeling operator extension} overcomes cycle-skipping in a simple
example. The problem studied here may be the simplest inverse wave
problem that exhibits cycle-skipping - in fact, essentially this
example is routinely used to illustrate cycle-skipping (see for
example \cite{VirieuxOperto:09}, Figure 7). Its data is a single trace
at a point in $\bR^3$ (``receiver location'') of the causal linear
acoustic pressure field, generated by an isotropic radiator acting at
another point (``source location'') at a known distance from the
receiver location. The acoustic material is supposed spatially
homogeneous, hence characterized by wave slowness (reciprocal
velocity) and density. The object of the inverse problem is to recover
the slowness and the time history of the isotropic radiator
(``wavelet'') from the recorded trace. In order that the data
constrain the slowness, the wavelet is presumed to have known compact
support. Thus the (``physical'') modeling operator has as its domain
the Cartesian product of an interval of positive slownesses, and the
subspace of $L^2(\bR)$ consisting of wavelets with the prescribed
support. Even for noise-free data (in the range of the modeling
operator), the obvious least squares problem formulated in terms of
this modeling operator exhibits many local minimizers having nothing
to do with the (``target'') slowness and wavelet used to create the
data - that is, cycle-skipping occurs (Theorem \ref{thm:fwi}).

Dropping the support constraint on the wavelet creates an extension of
the modeling map for which any data may be fit exactly, for any choice
of slowness. The link between data and slowness is restored by adding
a quadratic penalty on non-zero values of the wavelet outside of the
physical support interval to the extended mean-square data misfit. The
first main result of this paper is that for noise-free data, the
slowness component of {\em any} stationary point of this extended
penalty function differs from the target slowness by a multiple of the
target wavelet radius (Theorem \ref{thm:rampreallygood}). That is, cycle-skipping does not occur, and the
wavelet support radius determines the resolution of slowness. For
noisy data (that is, sum of a noise-free signal trace in the range of
the physical modeling operator and a square-integrable noise trace),
any stationary point differs from the target slowness by a combination
of the wavelet support radius and the noise-to-signal ratio (Theorem \ref{thm:mnoiseres}). Since the
wavelet component of the extended domain does not constrain support,
stationary points of the extended penalty function do not provide
solutions of the original, physical inverse problem, which includes
the support constraint. Such solutions can be obtained, with estimates
on support radius and data error, simply by truncating the wavelet
components of stationary points (Theorem \ref{thm:ipnoisesuf}).
  
The inverse problem studied here is far too simple to have any direct
use in applications. However it seems worthy of careful study for two
reasons. First, its simplicity allows an unusually complete account of
its properties. The present paper details its mathematical behaviour,
including explicit and sharp error estimates rarely available for nonlinear
inverse problems. A companion paper \cite[]{SymesChenMinkoff:21}
describes numerical examples that illustrate the mathematical
conclusions.

Second, the analysis reveals several important features shared with
similar approaches to other wave inverse problems, and so provides an
at least partial pattern for reaching similar conclusions with
immediate practical implications. For example, nested optimization
(variable projection method, \cite{GolubPereyra:03}) based on a model
decomposition into inner and outer variables seems to be very
important: in some cases, such as the simple problem presented here,
the decomposition is obvious (inner variable = wavelet, outer variable
= slowness), in others less so
\cite[]{geoprosp:2008,Terentyev:thesis}. The extended modeling
operator must be surjective, or at least have a dense range, {\em for
  each value of the outer variable}, so that data may be fit well even
for a poor initial guess of the outer variable. for the extended
inversion approach to be successful. The derivative of the extended
modeling operator with respect to the outer variable is
well-approximated by the composition of the extended modeling operator
itself and a pseudodifferential operator of order 1
\cite[]{Symes:IPTA14,tenKroode:IPTA14}. The choice of the penalty
operator, controlling the extended degrees of freedom, is critical: in
order to produce an objective immune from cycle-skipping, this penalty
operator must be (pseudo-)differential
\cite[]{StolkSymes:03}. Generally, the extended modeling operator is
not differentiable in the outer variable. However these last two facts
together imply the differentiability of the reduced objective function
from variable projection (Theorem \ref{thm:diffobj}, Appendix C). Because of the data-fitting
assumption, a straightforward algorithm for scaling the quadratic
penalty, based on the Discrepancy Principle, is available - see
\cite{FuSymes2017discrepancy,SymesChenMinkoff:21}, also Appendix
A. This scaling varies dynamically during iterative optimization, in
effect changing the objective function sporadically as the iteration
converges.

The approach explained here is an example of {\em extended source}
inversion, see \cite{HuangNammourSymesDollizal:SEG19} for a review,
and \cite{MetivierBrossier:SEG20} for a recent variation. Adding
degrees of freedom to the coefficients in the wave equation in various
ways produces so-called {\em extended model} inversion approaches,
which actually have a longer history in seismology - see
\cite{geoprosp:2008} for details. The need to overcome cycle-skipping
has spawned a great number of concepts, of which extension-based
inversion is only one. Least-squares inversion succeeds with
sufficiently good initial estimates of wave velocity, and the dominant
techniques in industrial and academic seismology exploit data other
than direct measurements of waves, for example measurement of wave
time-of-flight, to supply such initial estimates
\cite[]{VirieuxOperto:09,Fichtner:10,Schuster:17}. Use of a data
misfit measure other than least squares, such as versions of the
Wasserstein metric from optimal transport theory, also shows promise
\cite[]{Metivier:GEO18,EngquistYang:GEO18}.

The next section gives precise statements of the inverse problem
studied here, and its various components. The following sections
contain statements and proofs of the main results: the existence of
muliple stationary points of the least squares problem, properties of
the extended modeling operator and the reduced objective, choice of
the penalty operator, the main results mentioned above about
stationary points of the reduced objective, and the discrepancy-based
algorithm for adjusting the penalty scale. Three appendices justify
the discrepancy-based scale adjustment, review the link between
constrained support and spectrum provided by the Heisenberg inequality,
and expose the abstract structure underlying the differentiability of
the reduced objective.
  
I use the abbreviations ${\cal B}(X,Y)$ and ${\cal I}(X,Y)$ for 
the algebra of bounded linear operators from the Hilbert space $X$ to 
the Hilbert space $Y$, and its subalgebra of invertible
operators. All of the operators appearing in this discussion are
members of ${\cal B}(X,Y)$ for some choice of Hilbert spaces $X$ and
$Y$, and superscript $T$ denotes the transpose or adjoint of a such an
operator in the sense specified by its domain and range Hilbert structure.

\section{An Acoustic transmission Inverse Problem}
According to linear acoustics, the causal pressure field due to an
isotropic point radiator at $\bx_s \in \bR^3$ 
solves the the wave equation \cite[]{Frie:58}:
\begin{eqnarray}
  \label{eqn:awe}
  \left(m^2\frac{\partial^2 p}{\partial t^2} - \nabla^2\right) p(\bx,t) &=&
                                                                         w(t)\delta(\bx-\bx_s) \nonumber\\
  p(\bx,t)&=&0, t\ll 0.
\end{eqnarray}
The solution is well-known, see for instance
\cite{CourHil:62}, Chapter VI, section 12, equation 47:
\begin{equation}
  \label{eqn:homsol}
  p(\bx,t) = \frac{1}{4\pi |\bx-\bx_s|}w\left(t-m|\bx-\bx_s|\right),
\end{equation}
This trace of $p$ at $\bx_r \in \bR^3$ can be viewed as the result of
applying an m-dependent linear operator $F[m]$ to the wavelet $w$:
\begin{equation}
\label{eqn:mod}
F[m]w(t)  = p(\bx_r,t) = \frac{1}{4\pi r}w\left(t-mr\right) 
\end{equation}
The slowness $m$ must be positive, as follows from basic acoustics,
and in fact reside in a range characteristic of the
material model: for crustal rock, a reasonable choice would be
$m_{\rm min}=0.125, m_{\rm max}=0.6$ s/km.

Natural choices for domain and
range of $F$ are thus
\begin{itemize}
\item $M=(m_{\rm min}, m_{\rm max}),\,0 < m_{\rm min} \le m_{\rm
    max}$;
\item $W = L^2(\bR)$;
\item $D=L^2([t_{\rm min},t_{\rm max}]),\, t_{\rm min}<t_{\rm max}$;
\item $F: M \times W \rightarrow D$ as specified in \ref{eqn:mod}.
\end{itemize}
It is immediately evident from these choices and from the definition
\ref{eqn:mod} that
\begin{equation}
  \label{eqn:mapprop}
  \mbox{for }m \in M, F[m] \in {\cal B}(W,D),\mbox{ and }\|F[m]\| =
  \frac{1}{4\pi r}.
\end{equation}
Note also that $F[m]$ is surjective for every $m \in M$.

\noindent{\bf Remark:} In computational practice, $W$ will have to be replaced by a
finite-dimensional subspace of $L^2(\bR)$. Many such choices will
implicitly limit the support of $w \in W$ to a bounded interval, say
$[T_{\rm min},T_{\rm max}]$. To maintain the surjective property,
these bounds should be chosen so that $[t_{\rm min}, t_{\rm max}]
\subset [T_{\rm min}+mr,T_{\rm max}+mr]$ for all $m \in M$, that is,
\begin{eqnarray}
  \label{eqn:dombounds}
  t_{\rm min} &\ge & T_{\rm min}+m_{\rm max}r,\nonumber\\
  t_{\rm max} &\le & T_{\rm max}+m_{\rm min}r.
\end{eqnarray}
I will ignore these computational necessities in this work,
maintaining the definition $W=L^2(\bR)$.

As mentioned earlier, $F[m]$ is surjective for every $m \in M$. Since
all possible data lie in the range of $F[m]$ for any $m \in M$, some
restriction of the domain of $F$ is necessary in order that fitting
the data constrain $m$. The constraint employed is the specification
of a maxium support radius $\lambda_{\rm max} >0$. Then define for
$\lambda \in (0,\lambda_{\rm max}]$:
\begin{itemize}
\item $\lW = \{w \in W:
  \mbox{supp }w \subset [-\lambda,\lambda]\}$;
\item $\lF = F|_{M \times \lW}$.
\end{itemize}

In terms of this infrastructure, the inverse problem studied in 
this paper may be stated as

\begin{quote}
\noindent {\bf Inverse Problem:}
  given data $d \in D$, relative error level $\epsilon \in
  [0,1)$, and support radius $\lambda \in (0, \lambda_{\rm
    max}]$, find $(m,w) \in M \times \lW$ for which 
\begin{equation}
  \label{eqn:probstat0}  \|\lF[m]w-d\| \le \epsilon\|d\|,
\end{equation}
\end{quote}

Define the relative mean-square error $\lerr: M \times \lW \times D
\rightarrow \bR^+$ by
\begin{equation}
  \label{eqn:redms}
  \lerr[m,w;d]=\frac{1}{2}\|\lF[m]w-d\|^2/\|d\|^2,
\end{equation}
so that inequality \ref{eqn:probstat0} is equivalent to
\begin{equation}
  \label{eqn:probstat1}
  \lerr[m,w;d] \le \frac{1}{2}\epsilon^2,
\end{equation}
Minimization of $\lerr$ is the standard least-squares formulation of
the Inverse Problem defined above: if the global minimum value of
$\lerr$ is less than $\frac{1}{2}\epsilon^2$, then the global
minimizer is a solution of the Inverse Problem.

\noindent{\bf Remark:} The constraint $\epsilon < 1$ imposed on the
target noise level is eliminates the obvious choice $(m,0)$, which
satisfies the data misfit constraint for any $m \in M$. 

\noindent{\bf Remark:} I shall refer to the minimization of $\lerr$ as
``Full Waveform Inversion'' or ``FWI'', as this is the
terminology used in the seismology literature to identify this and
similar optimization problems.

\noindent{\bf Remark:} The support constraint is closely linked to the
folk theorem about FWI noted many times in the literature: convergence
of a descent method requires that the initial slowness must be known
to ``within a (fraction of a) wavelength''. The relation is a
consequence of Heisenberg's inequality, and will be reviewed in
Appendix B.

The best case for data fitting
is clearly the one in which the data can be fit precisely: that is,
there exists $(m_*,w_*) \in M\times \lW$ so that
\begin{equation}
  \label{eqn:defdatanonoise}
  d=\lF[m_*]w_*.
\end{equation}
Such data $d$ is {\em noise-free}, in the range of the map $\lF$. For
such data a solution of the Problem Statement \ref{eqn:probstat0}
exists with arbitrarily small $\epsilon>0$.


\section{Full Waveform Inversion}
While $F$ is surjective, as noted above, it is
very far from injective. On the other hand, under a constraint that
will be assumed throughout, $\lF[m]$ is injective for each $m \in M$ (in fact, $4 \pi r
\lF[m]$ is an isometry):
\begin{proposition}
  \label{thm:fullrec}
  Suppose that 
  \begin{equation}
    \label{eqn:fullrec}
    [ m_{\rm min}r-\lambda_{\rm max}, m_{\rm max}r+\lambda_{\rm max}]
    \subset [t_{\rm min},t_{\rm max}].
  \end{equation}
  Then $\lF[m]\in {\cal B}(\lW,D)$ is coercive for every $m \in M, \lambda \in
  (0,\lambda_{\rm max}]$.
\end{proposition}

\noindent{\bf Remark:} A useful consequence of the condition 
\ref{eqn:fullrec}: for every $m \in M$, 
\begin{equation}
  [-\lambda_{\rm max}, -\lambda_{\rm max}] \subset[ t_{\rm min}-mr , 
  t_{\rm max}-mr]. 
  \label{eqn:zeroinc}
\end{equation}

The first main result establishes the existence of large (100 \%)
residual local minimizers for the basic FWI objective $\lerr$, even
for noise-free data.
\begin{theorem}
  \label{thm:fwi}
  Suppose that $0 <\lambda\le \lambda_{\rm max}$,  $m_* \in M, w_*
  \in \lW, d=\lF[m_*]w_*$ is noise-free data per definition \ref{eqn:defdatanonoise},
  Under assumption \ref{eqn:fullrec}, for any $m \in M$ with $|m-m_*|r>2\lambda$,
\begin{equation}
  \label{eqn:isovpm}
 \min_w \lerr[m,w;d]=\lerr[m,0;d] = \frac{1}{2},
\end{equation}
and any such $(m,0)$ is a local minimizer of $\lerr$ with relative RMS
error = 1.0.
\end{theorem}

\begin{proof} From the definition \ref{eqn:mod},
\[
 \lerr[m,w;d] =  \frac{1}{32\pi^2
    r^2\|d\|^2}\int_0^T\,dt\,\left|w\left(t-mr\right)-w_*\left(t-m_*r\right)\right|^2
\]
Since $w_*, w$ vanish for $|t|>\lambda$,
$\lF[m_*]w_*(t)$ vanishes if $|t-m_*r|>\lambda$ and $\lF[m]w$ vanishes if $|t-mr|>\lambda$. So if $|mr-m_*r|
= |m-m_*|r > 2\lambda$, then $|t-mr|+|t-m_*r| \ge |mr-m_*r| >
2\lambda$ so either $|t-mr|>\lambda$ or $|t-m_*r|>\lambda$, that is,
either $\lF[m]w(t)=0$ or $\lF[m_*]w_*=0$. Therefore $\lF[m]w$ and
$\lF[m_*]w_*$ are orthogonal in the sense of the $L^2$ inner product
$\langle \cdot,\cdot \rangle_D$ on $D$:
\begin{equation}
  \label{eqn:ortho}
  |m- m_*|r > 2\lambda \,\, \Rightarrow \,\, \langle F[m]w,
  F[m_*]w_*\rangle_D = 0
\end{equation}
But $d = \lF[m_*]w_*$, so this is the same as saying that $d$ is
orthogonal to $F[m]w$. So conclude after a minor manipulation that
\[
  |m- m_*|r > 2\lambda \,\, \Rightarrow \,\, \lerr[m,w;d]=\frac{1}{32\pi^2 
    r^2\|d\|^2}(\|w\|^2 + \|w_*\|^2).
\]
\begin{equation}
  \label{eqn:iso}
  = \frac{1}{2}\left(\frac{\|w\|^2}{\|w_*\|^2} + 1 \right)
\end{equation}
That is, for slowness $m$ in error by more than $2\lambda/r$ from the 
target slowness $m_*$, the means square error (FWI objective) $\lerr$ is independent of
$m$, and its minimum over $w$ is attained for $w=0$
\end{proof}

Therefore local minimizers of $\lerr$ abound, as far as you like from the
global minimizer $(m_*,w_*)$. Local exploration of the FWI objective
$e$ gives no useful information whatever about constructive search
directions, and descent-based optimization tends to fail if the
initial estimate $m_0$ is in error by more than $2\lambda/r$
(``further than a multiple of a wavelength'', per discussion
in the second Appendix). In fact the actual behaviour of FWI itererations is worse
(failure if $m_0$ is in error by ``half a wavelength''), as follows
from a more refined analysis of the cycle-skipping local behaviour of $\lerr$ near its
global minimizer.

\section{Extended Source Inversion}
The phenomenon explained in the last section can be avoided by
reformulating the inverse problem via an extended modeling operator
and a soft (penalty) constraint to replace the support
requirement. The extension simply amounts to dropping the support
constraint, and replacing $\lW$ and $\lF$ by $W$ and $F$.

The hard
support constraint implicit in the choice of $\lW$ as domain for the
modeling operator is replaced by a soft constraint in the form of a
quadratic penalty, with weight operator $A:W \rightarrow W$.
The choices for the penalty operator $A$ considered here are scalar 
multiplication operators on $W$ defined by a choice of multiplier $a \in L^{\infty}(\bR)$:
\begin{equation}
  \label{eqn:annmult}
  A w(t)= a(t)w(t), \, t\in \bR.
\end{equation}
Explicit choices
for $a$ are discuss below.

With these choices, define
\begin{eqnarray}
  \label{eqn:edef}
  e[m,w;d] & = & \frac{1}{2}\|F[m]w-d\|^2/\|d\|^2;\\
  \label{eqn:gdef}
  g[w;d] & = & \frac{1}{2}\|Aw\|^2/\|d\|^2;\\
  \label{eqn:jdef}
  \Ja[m,w;d] & = & e[m,w;d] + \alpha^2g[w;d].
\end{eqnarray}

\section{Variable Projection}
The main theoretical device used in the proofs of our main results on 
extended inversion is Variable Projection reduction of the penalty objective $\Ja$
(equation \ref{eqn:jdef}) to a function $\tJa$ of 
$m$ alone, by minimization over $w$:
\begin{equation}
  \label{eqn:redexp}
  \tJa[m;d] = \inf_w \Ja[m,w;d]
\end{equation}

A minimizer $w$ on the right-hand side of definition
\ref{eqn:redexp} must solve the {\em normal equation}
\begin{equation}
  \label{eqn:norm}
  (F[m]^TF[m]+\alpha^2A^TA)w= F[m]^Td, 
\end{equation}


With $A$ of the form \ref{eqn:annmult}, $\tilde{J}_{\alpha}$ is explicitly
computable. First observe that apart from amplitude, $F[m]$ is
unitary: for $g \in D$,
\begin{equation}
\label{eqn:tran}
F[m]^T g (t) =
\left\{
  \begin{array}{c}
    \frac{1}{4\pi r}g\left(t+mr\right), \, t \in [t_{\rm min}-mr,
    t_{\rm max}-mr],\\
    0, \mbox{ else.}
  \end{array}
\right.
\end{equation}
so
\begin{equation}
  \label{eqn:unit}
  F[m]^TF[m] = \frac{1}{(4\pi r)^2}{\bf 1}_{[t_{\rm min}-mr,  
    t_{\rm max}-mr]}
\end{equation}
in which ${\bf 1}_{S}$ denotes
multiplication by the characteristic function of a measurable 
$S \subset \bR$.

Therefore the normal equation for the minimizer on the RHS of equation \ref{eqn:redexp} is
\begin{equation}
  \label{eqn:norm1}
  \left(\frac{1}{(4\pi r)^2} {\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]} + \alpha^2 A^TA\right)w= F[m]^Td.
\end{equation}

With these choices, the normal equation \ref{eqn:norm1} becomes
\begin{equation}
\label{eqn:norm2}
\left(\frac{1}{(4\pi r)^2}  {\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]} + \alpha^2a^2\right)w= F[m]^Td.
\end{equation}

\begin{proposition}
  \label{thm:norminvexp}
  Assume the conditions \ref{eqn:mod}, \ref{eqn:fullrec},
  \ref{eqn:annmult}. Also assume that $\lambda \in (0,\lambda_{\rm
    max}], \alpha > 0,$ and that  $C>0$ exists so that $a \in L^{\infty}(\bR)$
  mentioned in condition \ref{eqn:annmult} satisfies condition
  \ref{eqn:abnd}.
%  \begin{equation}
%   \label{eqn:abnd}
%    |t| > \lambda \Rightarrow a(t) \ge C,
%  \end{equation}
%  in which $C>0$ may depend on $\lambda$.
  Then
  \begin{itemize}
  \item[1. ]the normal operator $F[m]^TF[m] + \alpha^2A^TA$ is
    invertible for any $m \in M$, $\alpha > 0$;
  \item[2. ]the solution $\aw[m;d]\in W$ of the normal equation
    \ref{eqn:norm} is given by
    \begin{equation}
      \label{eqn:normsol}
      \aw[m;d](t) = \left\{
        \begin{array}{c}
          \left(\frac{1}{(4\pi r)^2} + \alpha^2
          a^2(t)\right)^{-1}\frac{1}{4 \pi r}d(t+mr), t \in [t_{\rm
          min}-mr, t_{\rm max}-mr];\\
          0, \mbox{ else;}
        \end{array}
      \right.
    \end{equation}
  \item[3. ]if in addition $d=F[m_*]w_*, w_* \in \lW$ is noise-free, as in equation
    \ref{eqn:defdatanonoise},
    \begin{equation}
      \label{eqn:solnonoise}
      \aw[m,d](t)= \left(1+ (4\pi r)^2\alpha^2 a(t)^2\right)^{-1}w_*\left(t+(m-m_*)r\right).
    \end{equation}
  \end{itemize}
\end{proposition}

\begin{proof} 
  \begin{itemize}
  \item[1. ]Note that thanks to \ref{eqn:zeroinc}, if $|t|\le
    \lambda \le \lambda_{\rm max}$, then ${\bf 1}_{[t_{\rm min}-mr,  
      t_{\rm max}-mr]}(t) = 1$, whereas if $|t|>\lambda$,
    then $a(t) \ge C$, whence
    \[
      \frac{1}{(4\pi r)^2}  {\bf 1}_{[t_{\rm min}-mr,  
        t_{\rm max}-mr]} + \alpha^2a^2  \ge \min\{(4\pi r)^2,
      \alpha^2\min\{1/(4\pi r)^2,C^2\} \}> 0.
    \]
    Therefore the normal operator is invertible under the stated
    conditions.

  \item[2. ]From the identity \ref{eqn:tran}.
    \[
      \mbox{supp }F[m]^Td \subset [t_{\rm min}-mr,t_{\rm max}-mr].
    \]
    Define $w_{\rm tmp}$ to be the right-hand side of equation \ref{eqn:normsol}. Then
    from the previous observation and identity \ref{eqn:tran},
    \[
      \mbox{supp }w_{\rm tmp} \subset [t_{\rm min}-mr,t_{\rm max}-mr].
    \]
    From the identity \ref{eqn:unit}, for any $w \in W$,
    \[
      t \in [t_{\rm min}-mr,t_{\rm max}-mr] \Rightarrow F[m]^TF[m]w(t)
      = \frac{1}{(4 \pi r)^2}w(t).
    \]
    It follows from this and the previous two observations that
    $w_{\rm tmp}$ solves the normal equation \ref{eqn:norm}, and
    therefore that $\aw[m;d]=w_{\rm tmp}$.

  \item[3. ]Follows by inserting the definition
    \ref{eqn:defdatanonoise} of $d$ in \ref{eqn:normsol} and
    rearranging.
  \end{itemize}
\end{proof}

\begin{theorem}
  \label{thm:norminv}
  Assume the condition \ref{eqn:fullrec}, $C>0$, and suppose that $A$ is
  given by equation \ref{eqn:annmult} for $a \in L^{\infty}(\bR)$
  satisfying
  \begin{equation}
    \label{eqn:abnd} 
    a \ge 0; \, a(t) \ge C\mbox{ for }|t| \ge \lambda_{\rm max}.
  \end{equation}
  Then \begin{itemize}
  \item[1. ]the reduced objective $\tJa$ is given by
    \begin{equation}
      \label{eqn:redexp1}
      \tJa[m;d] = \Ja[m,\aw[m;d];d],
    \end{equation}
    in which $\aw[m;d] \in W$ is the unique solution of the normal
    equation \ref{eqn:norm}.
  \item[2. ]The following are equivalent:
    \begin{itemize}
    \item[i. ]$(m,w) \in M \times W$ is a local minimizer of
      $\Ja[\cdot,\cdot;d]$, and
    \item[ii. ]$m$ is a local minimizer of $\tJa[\cdot;d]$ and
      $w=\aw[m;d]$.
    \end{itemize}
  \end{itemize}
\end{theorem}

\begin{proof} These conclusions follow immediately from Proposition
  \ref{thm:norminvexp}.
\end{proof}

\noindent{\bf Remark:} If $\Ja[\cdot,\cdot;d]$ and $\tJa[\cdot;d]$
were differentiable, then ``local minimizer'' in the conclusion of the
preceding theorem could be replaced by ``stationary point''. However,
for the problem addressed in this paper, $\Ja[\cdot,\cdot;d]$ {\em is
  not} differentiable without added smoothness constraints on $w$,
whereas $\tJa[\cdot;d]$ {\em is} differentiable for proper choice of
penalty operator $A$. 

This conclusion follows from properties of the modeling operator $F$
shared with many other inverse problems in wave propagation, as
explained in the third appendix. Here, I derive it from explicit
expressions for $\tJa$ and its components.

\begin{proposition}
  \label{thm:epjgen}
  Assume the hypotheses of Proposition \ref{thm:norminvexp}. Then
  \begin{equation}
  \label{eqn:residnormgen}
  e[m,\aw[m,d];d] = \frac{1}{2\|d\|^2}\int_{t_{\rm min}}^{t_{\rm max}} \,dt\,(4\pi r \alpha a(t-mr))^4(1 +
  (4\pi r \alpha a(t-mr))^2)^{-2}d(t)^2
\end{equation}
\begin{equation}
  \label{eqn:anninormgen}
  p[m,\aw[m,d];d] = \frac{1}{2\|d\|^2}\int_{t_{\rm min}}^{t_{\rm max}} \,dt\,(4\pi r a(t-mr))^2(1 +
  (4\pi r \alpha a(t-mr))^2)^{-2}d(t)^2
\end{equation}

\begin{equation}
  \label{eqn:expjgen}
\tJa[m;d] = \frac{1}{2\|d\|^2}\int_{t_{\rm min}}^{t_{\rm max}}\,dt\,(4\pi r \alpha a(t-mr))^2(1+(4\pi r \alpha 
a(t-mr))^2)^{-1}d(t)^2. 
\end{equation}
\end{proposition}

\begin{proof}
  From equation \ref{eqn:normsol},
  \[
    F[m]\aw[m;d](t) = 
    \frac{1}{4 \pi r}\left(\frac{1}{(4\pi r)^2} + \alpha^2
      a^2(t-mr)\right)^{-1}\frac{1}{4 \pi r}d(t),
  \]
  so
  \[
    (F[m]\aw[m;d]-d)(t) = (1 + (4 \pi r\alpha
    a(t-mr))^2)^{-1}-1)d(t)
  \]
  \[
    = -(4 \pi r\alpha a(t-mr))^2(1 + (4 \pi r\alpha
    a(t-mr))^2)^{-1}d(t).
  \]
  Half the integral of the square of this data residual is
  $e[m,\aw[m;d],d]$, which proves identity \ref{eqn:residnormgen}.

  To compute $p[m,\aw[m;d],d]$, note that
  \[
    A\aw[m;d](t)=a(t) \left(\frac{1}{(4\pi r)^2} + \alpha^2
      a^2(t)\right)^{-1}\frac{1}{4 \pi r}d(t+mr)
  \]
  \[
    = 4\pi r a(t) (1 + (4\pi r \alpha a(t))^2)^{-1}d(t+mr)
  \]
  for $ t \in [t_{\rm min}-mr, t_{\rm max}-mr]$, so squaring,
  integrating, and changing integration variables $t \mapsto t-mr$
  gives the result \ref{eqn:anninormgen}

  That the VPM objective $\tJa$ is given by \ref{eqn:expjgen} follows from equations \ref{eqn:pen},
  \ref{eqn:redexp}, \ref{eqn:residnormgen}, and
  \ref{eqn:anninormgen}.
\end{proof}

\begin{theorem}
  \label{thm:diffobj}
  Suppose that in addition to the hypotheses of Theorem
  \ref{thm:norminv}, $a \in W^{1,\infty}_{\rm loc}(\bR)$, then $\tJa[\cdot;d]
  \in C^1(M)$.
\end{theorem}

\begin{proof}
Suppose first that $a \in C^1(\bR)$. Differentiation under the integral sign  
  yields the expression for its derivative:
\begin{equation}
  \label{eqn:dexpjgen}
  \frac{d}{dm}\tJa[m;d] = -\frac{(4 \pi r \alpha)^2}{\|d\|^2} \int_{t_{\rm min}}^{t_{\rm max}} \,dt \, 
  \left(a\frac{da}{dt}\right)(t-mr)(1+(4\pi r \alpha 
  a(t-mr))^2)^{-2}d(t)^2. 
\end{equation}
For $a \in W^{1,\infty}_{\rm loc}(\bR)$ a limiting argument shows that the
same expression gives the derivative of $\tJa$.
\end{proof}

It will be useful to record expressions for the various componenets of
$\tJa$ when the data is noise-free, that is, the context of
Proposition \ref{thm:norminvexp}, item 3.

\begin{corollary}
  \label{thm:epjnonoise}
  Assume the hypotheses of Proposition \ref{thm:norminvexp}, item
  3. Then noting that $\|d\| = \|w_*\|/(4 \pi r)$
\begin{equation}
  \label{eqn:residnorm}
  e[m,\aw[m,d];d] 
= \frac{\alpha^4}{2\|w_*\|^2}\int\,dt\,a(t-(m-m_*)r)^4(1+(4\pi r)^2 \alpha^2 
    a(t-(m-m^*)r)^2)^{-2}w_*(t)^2.
\end{equation}
\begin{equation}
  \label{eqn:anninorm}
  p[m,\aw[m,d];d] = \frac{(4\pi r)^2}{2\|w_*\|^2}\int \,dt\,  
  \frac{a(t-(m-m_*)r)^2}{(1+ (4\pi r)^2\alpha^2
    a(t-(m-m_*)r)^2)^{2}}w_*(t)^2.
\end{equation}
so 
\begin{equation}
\label{eqn:expjnonoise}
\tJa[m;d] = \frac{(4\pi r \alpha)^2}{2\|w_*\|^2}\int\,dt\,a(t-(m-m_*)r)^2(1+(4\pi r)^2 \alpha^2 
  a(t-(m-m_*)r)^2)^{-1}w_*(t)^2. 
\end{equation}
Finally, if $a \in W^{1,1}_{\rm loc}(\bR)$, then $\tJa[\cdot;d]$ is differentiable, and 
\begin{equation}
  \label{eqn:dexpjnonoise}
  \frac{d}{dm}\tJa[m;d] = -\frac{r (4\pi r \alpha)^2}{\|w_*\|^2} \int \,dt \, 
  \frac{\left(a\frac{da}{dt}\right)(t-(m-m_*)r)}{(1+(4\pi r)^2 \alpha^2 
  a(t-(m-m_*)r)^2)^{2}}w_*(t)^2. 
\end{equation}
\end{corollary}

\section{Choice of Penalty Operator}

I examine two choices for $A$. For each choice, I ask first whether local minimizers 
of the resulting VPM objective $\tJa[\cdot;d]$ occur far from a slowness $m_*$. 

A penalty operator $A$ of which $\lW$ is the null space would be a
natural choice. Such operators have come to be called
``annihilators'', since they map all members of the constraint
subspace $\lW$ to zero.  A simple example is
\begin{eqnarray}
  A = E^c_{\lambda}&=&I - E_{\lambda},\mbox{ where } \nonumber \\
  E_{\lambda}w(t) &=&{\bf 1}_{[-\lambda,\lambda]}(t)w(t). 
                      \label{eqn:ann0}
\end{eqnarray}
That is, $E_{\lambda}$ is the orthogonal projector onto $\lW$,
and $E_{\lambda}^c$ is the orthogonal projector onto its
orthocomplement, an operator of the form \ref{eqn:annmult} with $a
= 1 - {\bf 1}_{[-\lambda,\lambda]}$. %Note that this choice of $a$
%satisfies condition \ref{eqn:abnd} with $C \equiv 1$.

\begin{theorem}
  \label{thm:boxcarbad}
  Suppose that
  \begin{itemize}
  \item[1. ] $\lambda \in (0,\lambda_{\rm max}]$;
  \item[2. ] $m_*\in M, w_*\in \lW$, and $d=F[m_*]w_*$ (noise-free
    data);
  \item[3. ] $A=E^c_{\lambda}$, that is, $a=1-{\bf 1}_{-\lambda,\lambda]}$ in
    the definition \ref{eqn:annmult}.
  \end{itemize}
  Then if $|m-m_*| >  2\lambda/r$,
  \[
    \tJa[m;d] = \frac{(4\pi r \alpha)^2}{2(1+(4 \pi r \alpha)^2)}.
  \]
\end{theorem}

\begin{proof} of Theorem \ref{thm:boxcarbad}
  
 This is clear from the definition $a = 1-{\bf 1}_{-\lambda,\lambda]}$ and equation \ref{eqn:expjnonoise}, as the supports of $w_*$ and ${\bf
    1}_{[-\lambda,\lambda]}(t-(m-m_*))$ are disjoint for the range of
  $m$ identified in the theorem.
\end{proof}

\noindent {\bf Remark.}
One might have thought that $A=E^c_{\lambda}$ would be a better choice
of annihilator, as for noise-free
data, the solution set defined by the problem statement
\ref{eqn:probstat0} is the same as the set of global minimizers of
$\Ja$ in this case. However, for this choice of annihilator,
$t\Ja$ exhibits the same feature as the mean square error $e$, namely
a continuum of local minimizers at any distance from the global
minimizer $m_*$ greater than a multiple of $\lambda$. Therefore the
extended inversion with this choice of annihilator is no more amenable
to local optimization than is FWI.

A second possible 
penalty operator penalizes energy away from 
$t=0$: choose $\tau > 0$ and set 
\begin{equation}
  \label{eqn:ann}
  a(t) = \min(|t|, \tau). 
\end{equation}
Note that $a \ge 0$ and $a \in L^{\infty}(\bR) \cap W^{1,1}_{\rm
  loc}(\bR)$. The cutoff $\tau$ will be chosen large enough to be effectively inactive: 
specifically, hindsight suggests 
\begin{equation}
  \label{eqn:taudef}
  \tau = \max\{|t_{\rm min}-m_{\rm min}r|,|t_{\rm min}-m_{\rm max}r|, |t_{\rm max}-m_{\rm min}r|, |t_{\rm max}-m_{\rm max}r|\}. 
\end{equation}
This 
particular annihilator has been employed in earlier papers on extended 
source inversion 
\cite[]{Plessix:00a,LuoSava:11,Warner:14,HuangSymes:SEG15a,Warner:16,HuangSymes:Geo17}.

\section{Stationary Points}

\begin{proposition}
  \label{thm:rampgood}
  Suppose that
  \begin{enumerate}
  \item $m_* \in M$;
  \item $0 < \mu \le \lambda$, and $w_* \in W_{\mu}$;
  \item $d_* = F[m_*]w_*$;
  \item $a(t)=\min\{|t|,\tau\}$ in the definition \ref{eqn:annmult},
    with $\tau$ given by equation \ref{eqn:taudef}; and
  \item $\alpha > 0$.
  \end{enumerate}
  Then for any $m \in M$, 
  \begin{equation}
    | (m - m_*)r| > \lambda  \Rightarrow  \left|\frac{d}{dm}\tJa[m;d_*]\right| >  
    \frac{r(4 \pi r \alpha)^2(\lambda-\mu)}{(1+(4\pi r\alpha)^2 
      (\lambda+\mu)^2)^{2}}  
    \label{eqn:gradbndnonoise}
  \end{equation}
\end{proposition}
\begin{proof}
  As observed before, $\mbox{supp }\aw[m;d_*] \subset [t_{\rm
    min}-mr,t_{\rm max}-mr]\subset [-\tau,\tau]$, with $\tau$ defined
  in \ref{eqn:taudef}. Therefore, $a(t) = |t|$, $a a'(t) = t$ in the
  support of the integrand on the RHS of equation
  \ref{eqn:dexpjnonoise}, which therefore 
  becomes (after change of integration variable)
  %%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{equation}
    \label{eqn:gradfinal}
    \frac{d}{dm}\tJa[m;d_*] = -\frac{r (4\pi r\alpha)^2}{\|w_*\|^2} \int \,dt \, 
  t(1+(4\pi r)^2 \alpha^2 
  t^2)^{-2}w_*(t+(m-m_*))^2.
  \end{equation}
  Recall that $w_*(t+(m-m_*)r)$
  vanishes if $|t+(m-m_*)r| > \lambda$. Therefore the integral on the
  RHS of equation \ref{eqn:gradfinal} can be re-written
  \[
    = -\frac{r(4 \pi r \alpha)^2}{\|w_*\|^2}\int_{-(m-m_*)r-\lambda}^{-(m-m_*)r+\lambda}
    \,dt\, t(1+(4\pi r)^2\alpha^2 t^2)^{-2}w_*\left(t+(m-m_*)r\right)^2
  \]
  Suppose that $\mu \le \lambda$ and $w_* \in W_{\mu}$. 
  If $m > m_*+\lambda/r$, then $t+(m-m_*)r \in \mbox{supp }w_*$
  implies $-\mu - \lambda < t < \mu-\lambda<0$, so 
  \[
    t(1+(4\pi r)^2\alpha^2 t^2)^{-2} < (\mu-\lambda)(1+(4\pi r)^2\alpha^2 (\mu+\lambda)^2)^{-2}<0
  \]
  in the support of the integrand in equation
  \ref{eqn:gradfinal}. Arguing similarly for $m<m_*-\lambda/r$, obtain
  a similar inequality, implying the conclusion \ref{eqn:gradbndnonoise}.
\end{proof}

\begin{theorem}
  \label{thm:rampreallygood}
  Suppose that
  \begin{enumerate}
  \item $m_* \in M$;
  \item $0 <  \lambda$, and $w_* \in \lW$;
  \item $d_* = F[m_*]w_*$;
  \item $a(t)=\min\{|t|,\tau\}$ in the definition \ref{eqn:annmult},
    with $\tau$ given by equation \ref{eqn:taudef}; 
  \item $\alpha > 0$; and
  \item$m \in M$ is a stationary point of $\tJa[\cdot;d_*]$.
  \end{enumerate}
  Then $|m-m_*| < \lambda /r$.
\end{theorem}

\begin{proof} Follows directly from Proposition \ref{thm:rampgood} by
  taking $\mu=\lambda$.
\end{proof}

The preceding theorem established that a proper choice of annihilator
leads to a reduced penalty objective all of whose stationary points
are within $O(\lambda)$ of the target slowness $m_*$, provided that
the data are noise-free in the sense of equation
\ref{eqn:defdatanonoise}. This result leaves open two questions:
\begin{itemize}
\item how does one use this reduced penalty minimization to produce
  a solution of the inverse problem as in problem statement
  \ref{eqn:probstat0}? 
\item how does one answer the same question for noisy data?
\end{itemize}

The next result answers the first question, in the case of noise-free data:
\begin{proposition}
  \label{thm:ipnonoisesuf}
  Suppose that $a$ is given by definition \ref{eqn:ann}, $\alpha$,
  $\mu \in (0,\lambda_{\rm max}]$,
  $d$ is given by
  \ref{eqn:defdatanonoise} with $w_* \in W_{\mu}$, and  $m_{\infty}$ is a stationary
  point of $\tJa[\cdot;d]$. Then $(m_{\infty},\aw[m_{\infty};d])$ is a
  solution of the inverse problem \ref{eqn:probstat0} for any $\lambda
  \ge 2\mu$ and
  \begin{equation}
    \label{eqn:estresidnorm}
    \epsilon \ge \frac{(8\pi r \mu \alpha)^2}{1 + (8\pi r \mu\alpha)^2}.
  \end{equation}
\end{proposition}

\begin{proof}
  From the assumption $w_* \in W_{\mu}$ and Theorem
  \ref{thm:rampreallygood}, $|(m_{\infty}-m_*)r|\le \mu$. From the
  identity \ref{eqn:solnonoise},
  $\mbox{supp }\aw[m_{\infty};d] \subset
  [(m_{\infty}-m_*)r-\mu,(m_{\infty}-m_*)r+\mu] \subset
  [-2\mu,2\mu]$. Because of the support limitation, $a(t)=|t|$ in the
  interval of integration appearing in \ref{eqn:residnorm}, so
\[
  e[m_{\infty},\aw[m_{\infty},d];d] 
= 8 \pi^2 r^2 \alpha^4\int^{\mu}_{-\mu}\,dt\,\frac{|t-(m_{\infty}-m_*)r|^4}{(1+(4\pi r)^2 \alpha^2 
|t-(m_{\infty}-m^*)r|^2)^{2}}w_*(t)^2
\]
\[
  \frac{1}{2} (4\pi r \alpha)^4\int^{\mu}_{-\mu}\,dt\,\frac{|t-(m_{\infty}-m_*)r|^4}{(1+(4\pi r)^2 \alpha^2 
|t-(m_{\infty}-m^*)r|^2)^{2}}d(t+m_*r)^2
\]
\[
  \le \frac{1}{2} \|d\|^2  \left(\frac{(8\pi r \alpha \mu)^4}{(1+(8\pi
      r \alpha \mu)^2)^2}\right).
  \]
\end{proof}

The inequality \ref{eqn:estresidnorm} can be interpreted as a bound 
on $\alpha$, given $\epsilon$ and $\lambda$, for a
stationary point of $\tJa$ to yield a solution of the inverse
problem: one obtains a solution, provided that $\alpha$ is
sufficiently small. On the other hand, it is clear that $\alpha$
cannot be too large if stationary points of $\tJa$ are to yield
solutions: the integrand in \ref{eqn:residnorm} is increasing in
$\alpha$ for every $t$ and $m$, and the multiplier
\[
t \mapsto (4\pi r \alpha(t-(m_{\infty}-m_*)r))^4(1+(4\pi r)^2 \alpha^2 
|t-(m_{\infty}-m^*)r|^2)^{-2}
\]
tends monotonically to $1$ as $\alpha \rightarrow \infty$, uniformly
on the complement of any open interval containing
$t=(m_{\infty}-m_*)r$. Therefore
\begin{equation}
  \label{eqn:elimit}
  \lim_{\alpha \rightarrow \infty}e[m_{\infty},\aw[m_{\infty};d];d] =
  \frac{1}{2}\frac{1}{(4 \pi r)^2}\|w_*\|^2 = \frac{1}{2}\|d\|^2.
\end{equation}
Consequently, there exists $\alpha_{\rm max}(\epsilon,\lambda,d)$ so
that
\[
  e[m_{\infty},\aw[m_{\infty};d];d]  \le \frac{1}{2}\epsilon^2\|d\|^2
  \Rightarrow \alpha \le \alpha_{\rm max}(\epsilon,\lambda,d).
\]
The existence of this limiting penalty weight has been inferred
indirectly; Appendix A describes a constructive algorithm
for its approximation.

I turn now to the second issue identified above, the effect of
noise. Suppose that the data trace $d$ takes the form
\begin{equation}
  \label{eqn:defdatanoisy}
  d = F[m_*]w_* + n = d_*+n,
\end{equation}
with $m_* \in M, w_* \in W_{\mu}$, $0<\mu<\lambda$, and noise trace $n \in
D$. Since no support assumptions can be made about $n$, equation
\ref{eqn:normsol} implies that $\aw[m;d] \notin \lW$ for any values of
$\alpha$ and $\lambda$.  Therefore minimization of $\tJa$ cannot by itself yield a
solution of the inverse problem as defined in the problem statement
\ref{eqn:probstat0}. In this section, I explain how a solution may
nontheless be constructed from a stationary point of $\tJa$.

First, examine the effect of additive noise on the estimation of the
slowness $m$. In expressing the result, use the dimensionless
relative data error
\begin{equation}
  \label{eqn:defeta}
  \eta = \frac{\|n\|}{\|d_*\|}. 
\end{equation}

\begin{proposition}
  \label{thm:mnoise}
  Assume the hypotheses of Proposition \ref{thm:rampgood}, and that $d$ is
  given by definition \ref{eqn:defdatanoisy}. Suppose that $m \in M$
  is a stationary point of $\tJa[\cdot;d]$, and that
  \begin{equation}
    \label{eqn:mnoisebnd}
    \eta(1+\eta) \le \frac{16}{3\sqrt{3}}\frac{4\pi r \alpha
      (\lambda-\mu)}{(1+(4\pi r\alpha(\lambda+\mu))^2)^2}
  \end{equation}
  Then
  \begin{equation}
    \label{eqn:mnoisebndfin}
    |m-m_*| \le \frac{\lambda}{r}
  \end{equation}.
\end{proposition}

\begin{proof}
  From equation \ref{eqn:dexpjgen}, $d \tJa / dm$ is the
value of a quadratic form in $d$ with (indefinite) symmetric operator
$B = $ multiplication by
\[
  b(t;m,\alpha)  = -\frac{(4 \pi r \alpha)^2 (t-mr)}{(1+(4\pi r \alpha (t-mr))^2)^{2}}
\]
Therefore
\begin{equation}
  \label{eqn:gradlip}
  \left|\frac{d}{dm}\tJa[m;d]-\frac{d}{dm}\tJa[m;d_*]\right| =
  |\langle (d+d_*),B(d-d_*)\rangle| \le \max_{t \in
    \bR}|b(t;m,\alpha)|\eta(1+\eta)\|d_*\|^2
\end{equation}
A straightforward calculation shows that
\[
  \max_{t \in \bR} b(t;m,\alpha) = \frac{3\sqrt{3}}{16} 4\pi r\alpha.
\]
For a stationary point $m$ of
$\tJa[\cdot;d]$, the inequality \ref{eqn:gradlip} implies
\[
  \left|\frac{d}{dm}\tJa[m;d_*]\right| \le \frac{3\sqrt{3}}{16} 4\pi
  r\alpha \eta(1+\eta)\|d_*\|^2
\]
On the other hand, the conclusion \ref{eqn:gradbndnonoise} of Proposition
\ref{thm:rampgood} implies that if also
\[
  \frac{3\sqrt{3}}{16} 4\pi r\alpha \eta(1+\eta)\|d_*\|^2 \le (4 \pi r
  \alpha)^2 \frac{\lambda-\mu}{(1+(4\pi r)^2\alpha^2
    (\lambda+\mu)^2)^{2}} \|d_*\|^2
\]
then $|m-m_*|\le \lambda/r$. Rearranging, obtain the conclusion.
\end{proof}

\begin{corollary}
  \label{thm:mnoisecor}
  Asumme the hypotheses of Theorem \ref{thm:mnoise}, in particular
  that $m$ is a stationary point of $\tJa[\cdot;d]$, $d=d_*+n$. Then
  \begin{equation}
    \label{eqn:mnoisecor}
    |m-m_*| \le \frac{\mu}{r} + \frac{\eta}{\alpha} \left(\frac{3\sqrt{3}(1+\eta)}{64\pi r^2}(1+(8\pi r \alpha
      \lambda_{\rm max})^2)^2\right)
  \end{equation}
\end{corollary}

\begin{proof} Assume that $\lambda$ is chosen to obtain equality in
  the condition \ref{eqn:mnoisebnd}, substitute the bound $2
  \lambda_{\rm max}$ for $\lambda + \mu$ in the denominator, solve for
  $\lambda$ and substitute in inequality \ref{eqn:mnoisebndfin}.
\end{proof}

\noindent {\bf Remark:} This bound 
suggests that error in the estimate of $m$ due to data noise is a
decreasing function of $\alpha$, at least for small $\alpha$. This result is intuitively
appealing, and is supported by numerical evidence. A similar but stronger bound will be demonstrated in the next section.

\begin{theorem}
  \label{thm:mnoiseres}
  Asumme that
  \begin{itemize}
  \item[1. ] $\alpha, \mu> 0$,
  \item[2. ] $m_* \in M, w_* \in W_{\mu}$,
  \item[3. ] $d_* = F[m_*]w_*$,
  \item[4. ] $n \in D$ and $d = d_* + n$.
  \end{itemize}
  Set $\eta = \|n\|/\|d_*\|$. Assume that $\eta$ satisfies inequality \begin{equation}
  \label{eqn:mnoisecond}    
  \eta < \frac{\sqrt{5}-1}{2},
  \end{equation}
  and that $m$ is a stationary point of $\tJa[\cdot;d]$.
  Then
  \begin{equation}
    \label{eqn:mnoisesuff}
    |m-m_*| \le \left(1+\frac{2\eta(1+\eta)}{1-\eta(1+\eta)}\right)\frac{\mu}{r}.
  \end{equation}
\end{theorem}

\begin{proof} of Theorem \ref{thm:mnoiseres}:
  Write $\lambda = (1+\delta)\mu$, and $x=4 \pi r \alpha \mu$. Then
  the right-hand side of equation \ref{eqn:mnoisebnd} may be written as
  \begin{equation}
    \label{eqn:mnoisebndrev}
    \frac{16}{3\sqrt{3}}\frac{4\pi r \alpha
      (\lambda-\mu)}{(1+(4\pi r\alpha(\lambda+\mu))^2)^2} = D
    \frac{x}{(1+C^2 x^2)^2},
  \end{equation}
  where
  \[
    D=\frac{16}{3\sqrt{3}}\delta,\,C=2+\delta.
  \]
  The positive stationary point of the quantity on the right-hand side
  of \ref{eqn:mnoisebndrev} is a maximum, and occurs at
  $x=1/(\sqrt{3}C)$, that is
  \[
    4 \pi r \alpha \mu = \frac{1}{\sqrt{3}(2+\delta)}.
  \]
  Thus
  \[
    1+C^2x^2 = \frac{4}{3}
  \]
  hence the maximum value is
  \[
    \frac{D3\sqrt{3}}{16C} = \frac{\delta}{2+\delta}.
  \]
  This maximum value must be larger than the left hand side of inequality
  \ref{eqn:mnoisebnd}, that is,
  \[
    \eta(1+\eta) \le \frac{\delta}{2+\delta},
  \]
  in order that there be any solutions at all, but the right hand side
  is less than $1$. This observation establishes the necessity of
  hypothesis \ref{eqn:mnoisecond} of the
  theorem. Solving this above inequality for $\delta$ and unwinding
  the definitions, one finds that the right-hand side of inequality
  \ref{eqn:mnoisebnd} is bounded by \ref{eqn:mnoisesuff}, so appeal to
  Proposition \ref{thm:mnoise} finishes the proof.
\end{proof}

\noindent {\bf Remark.} That is, {\em with the choice of penalty
  multiplier $a$ given in equation \ref{eqn:ann}, and support radius
  $\mu$ of the ``noise-free'' wavelet $w_*$, $\Ja$ has no local
  minima with slownesses further than $(1+O(\eta))\mu/r)$ from the slowness used to
  generate the data}.

\noindent {\bf Remark.} The estimate $|m-m_*|r<\mu(1+O(\eta))$ for
local minima of $\Ja$ is sharp: it is possible to choose $w_* \in W_{\mu}$
so that $\mu - |m-m_*|r$ is as small as you like. In particular,
the ``exact'' or ``true'' slowness $m_*$ is not necessarily the only
slowness component of a local minimizer, or even the slowness
component of any local minimizer, and in particular
is not (necessarily) the slowness component of a global minimizer of $\Ja$.

\noindent {\bf Remark.} No similar bound could hold for much larger
noise levels than specified in condition \ref{eqn:mnoisecond}, the
right-hand side of which is a bit larger than 0.6. For example, if the noise is
the predicted data for the same wavelet $w_*$ with a substantially different
slowness $m_{\flat}$, that is, $n=F[m_{\flat}]w_*$, then a simple
symmetry argument shows that if there is a local minimizer of $\Ja[\cdot,\cdot;d_*+n]$. with
slowness near $m_*$,
there must also be a minimizer with slowness near $m_{\flat}$.
so that the difference with $m_*$ is not constrained at
all by the assumed support radius of $w_*$. So for this example with 100\% noise, no
bound of the type given by conclusion 2 could possibly hold. The
companion paper \cite[]{SymesChenMinkoff:21}
illustrates this phenomenon numerically.

\noindent{\bf Remark.} Note that $\alpha$ plays no role in the
conclusions of this theorem. It is only required that $\alpha >0$.

\noindent {\bf Remark:} I emphasize that Theorem \ref{thm:mnoiseres} states {\em sufficient} conditions for a bound on
the slowness error $|m-m_*|$ in terms of the relative data noise level $\eta$,
giving an additional ``fudge factor'' beyond the support size $\mu$
of the noise-free wavelet $w_*$ for an interval within which the slowness error is
guaranteed to lie.

Conclusion 1 in Theorem \ref{thm:mnoiseres} constrains the range of
noise level to which these results apply to a bit more than 60\%. That
is, the bound given by conclusion 2 is useful only for small noise. In
the limit as $\eta \rightarrow 0$, conclusion 2 becomes
$\lambda/\mu \gtrsim 1 + 2\eta$, that is, the ``fudge factor'' beyond
the noise-free bound is approximatly twice the noise level.

On the other hand, stronger bounds than given by Theorem
\ref{thm:mnoiseres} are possible, given additional constraints on the
noise $n$. A natural example is uniformly distributed random noise,
filtered to have the same spectrum as the source. The expression
\ref{eqn:dexpjgen} implies that the interaction of noise $n$ and
signal $d_*$ in the derivative of $\tJa$ is local, so that the
coefficient of $\eta$ on the left-hand side of inequality
\ref{eqn:mnoisebnd} is effectively much less that 1, resulting in a
larger range of allowable $\eta$. While I will not formulate such a
result, one of the numerical examples in the companion paper
\cite[]{SymesChenMinkoff:21} suggests its feasibility.

Unless the data is noise-free, there is no reason to suppose that the
estimated wavelet $\aw[m;d]$ (Theorem \ref{thm:norminv}) will lie in
$\lW$, unless the support of the noise $n$ is not restricted. In order
to construct a solution of the inverse problem \ref{eqn:probstat0}, 
project $\aw[m;d]$ onto $\lW$. For sufficiently large $\lambda,
\epsilon$, the result is a solution of the inverse problem:

\begin{theorem}
  \label{thm:ipnoisesuf}
  Assume the hypotheses of Theorem \ref{thm:mnoiseres}, and that
  inequality \ref{eqn:mnoisecond} holds, and $\mu \in
  (0,\lambda_{\rm max}]$. Then the pair
  \[
    (m,{\bf 1}_{[-\lambda,\lambda]}\aw[m,d])
  \]
  solves the inverse problem as stated in \ref{eqn:probstat0} if
  \begin{equation}
    \label{eqn:ipnoiselam}
    \left( 2+\frac{2\eta(1+\eta)}{1-\eta(1+\eta)}\right)\mu \le \lambda
    \le \lambda_{\rm max}, 
  \end{equation}
  and
  \begin{equation}
    \label{eqn:ipnoiseeps}
    \epsilon \ge \frac{(8 \pi r \alpha\lambda)^2}{1 + (8 \pi r \alpha\lambda)^2}+\eta. 
  \end{equation}    
\end{theorem}

\begin{proof} of Theorem \ref{thm:ipnoisesuf}:
From Theorem \ref{thm:norminv}, 
\[
  \aw[m;d](t) = (1+ (4 \pi r \alpha t)^2)^{-1}(w_*(t+(m-m_*)) + 4\pi r 
  n(t+mr)) 
\]
\[
  = \aw[m;d_*] + (1+ (4 \pi r \alpha t)^2)^{-1}4\pi r n(t+mr)
\]

From Theorem \ref{thm:mnoiseres},
\[
  |m-m_*| \le \left(1+\frac{2\eta(1+\eta)}{1-\eta(1+\eta)}\right)\frac{\mu}{r} 
\]
which from assumption \ref{eqn:ipnoiselam} is
\[
  =\left(2+\frac{2\eta(1+\eta)}{1-\eta(1+\eta)}\right)\frac{\mu}{r}
  -\frac{\mu}{r} \le \left(\frac{\lambda}{\mu}-1\right)\frac{\mu}{r}
\]
That is,
\[
  |m-m_*|r \le \lambda-\mu.
\]
From Theorem \ref{thm:norminv},
\[
  \mbox{supp }\aw[m;d_*] \subset [-\mu-(m-m_*)r, \mu-(m-m_*)r]
  \subset [-\lambda,\lambda]
\]
so
\[
  E_{\lambda}\aw[m;d](t) = \aw[m;d_*](t) + E_{\lambda}(1+ (4 \pi r \alpha
  t)^2)^{-1}4\pi r n(t+mr)
\]
From the definition of $F[m]$, for any $t_1<t_2$, $w \in W$,  
\[
  F[m]{\bf 1}_{[t_1.t_2]}w = {\bf 1}_{[t_1+mr,t_2+mr]}F[m]w  
\]
Thus the data residual after projection is
\[
  F[m]E_{\lambda}\aw[m,d](t) -d(t) = F[m]\aw[m,d_*](t) -d_*(t)  
\]
\[
  + {\bf 1}_{[-\lambda+mr,\lambda+mr]}(4 \pi r \alpha (t-mr))^2 (1+ (4 \pi
  r \alpha (t-mr))^2)^{-1} n(t)
\]
\[
  -(1- {\bf 1}_{[-\lambda+mr,\lambda+mr]})n(t)
\]
From \ref{eqn:residnorm} and the bound on $m-m_*$,
\[
  \| F[m]E_{\lambda}\aw[m,d_*] -d_*\|^2 = (4 \pi r \alpha)^4
  \int_{-\lambda+(m-m_*)r}^{\lambda+(m-m_*)r}\,dt\, (t-(m-m_*)r)^4
\]
\[
  \times (1+(4\pi r \alpha)^2 
  (t-(m-m^*)r)^2)^{-2}d_*(t)^2
\]
\[
  \le (4 \pi r \alpha \lambda)^4\|d_*\|^2
\]
Similarly, the norm squared of the sum of the last two terms is 
\[
  \le (4 \pi r \alpha \lambda)^2 \|{\bf 1}_{[-\lambda+mr,\lambda+mr]}
  n\|^2 + \|(1 - {\bf 1}_{[-\lambda+mr,\lambda+mr]}n\|^2
\]
Without additional hypotheses to outlaw the accumulation of $n$ near
$t=mr$, all that can be said is that this is
\[
  \le \max \{(4 \pi r \alpha \lambda)^2, 1\} \|n\|^2
\]
Putting this all together,
\[
  \|F[m]E_{\lambda}\aw[m,d]-d\| \le (4 \pi r \alpha
  \lambda)^2\|d_*\| + \max \{4\pi r \alpha \lambda, 1\}\|n\|
\]
\[
  \le ((4 \pi r \alpha \lambda)^2 +\max \{4\pi r \alpha \lambda, 1\}
  \eta)\|d_*\|.
\]
If the right-hand side is to be less than $\|d_*\|$ as required by the
definition \ref{eqn:probstat0} of the inverse problem, then
necessarily $4\pi r\alpha \lambda < 1$, so the right hand side in the
preceding inequality is bounded by the right hand side of assumption
\ref{eqn:ipnoiseeps} of the theorem. Therefore this assumption implies
that the relative residual is $\le \epsilon$.
\end{proof}

\noindent {\bf Remark:} Note that the sufficient condition \ref{eqn:ipnoiselam} for
$\lambda$ is independent of $\alpha$. It follows that for any choice
of $\lambda$ consistent with this bound, $(m,{\bf
  1}_{[-\lambda,\lambda]}\aw[m,d])$ is a solution of the inverse
problem for any $\epsilon > 0$ provided that $\alpha$ is chosen
sufficiently small ($O(\sqrt{\epsilon})$).

\section{Selection of $\alpha$}

So far in this story, the selection of the penalty weight $\alpha$ has played a relatively minor role. In practical calculation, on the other hand, selection of $\alpha$ strongly influences the convergence of iterative methods and the quality of the results. 

Concentration of the wavelet near $t=0$ clearly favors larger $\alpha$: as will be established in Appendix A, the reduced penalty term $g[m,\aw[m;d];d]$ is a decreasing function of $\alpha$ for any $m$. As $g$ measures the dispersion of the wavelet away from $t=0$, larger $\alpha$ is to be preferred, all else being equal. 

However all else is not equal: the choice of $\alpha$ affects to VPM reduced objective $\tJa[m;d]$ and therefore the estimation of the nonlinear variable $m$ as well. To express this connection, observe that for any $d \in D$, $\lambda \in (0,\lambda_{\rm max}]$ and $m \in M$, 
the orthogonal projection of $d$ onto the range $F[m]W_{\lambda}$ is
$({\bf 1}_{[mr-\lambda,mr+\lambda]}d)(t+mr)$, whence the distance from $d$ to the range of $F$ restricted to $W_{\lambda}$ is
\[
\mbox{inf}_{w \in W_{\lambda}} \|F[m]w-d\| = \|(1-{\bf 1}_{[mr-\lambda,mr+\lambda]})d\|.
\]
Denote by $r_{\lambda}[m;d]$ this distance normalized by $\|d\|$:
\[
r_{\lambda}[m;d] = \frac{ \|(1-{\bf 1}_{[mr-\lambda,mr+\lambda]})d\|}{\|d\|}.
\]
Denote by $v_{\lambda}[m;d]$ the pre-image under
$F$ of the projection of $d$ onto $F[m]W_{\lambda}$:
\[
v_{\lambda}[m;d](t) = 4\pi r {\bf 1}_{[-\lambda,\lambda]}(t)d(t+mr).
\]
Then using notation introduced earlier in this section for the residual norm,
\[
r_{\lambda}[m;d] = r[m,v_{\lambda}[m;d];d]
\]

\begin{theorem}
  \label{thm:malpha}
  Suppose $d \in D$, $\alpha>0$, $0<\lambda \le \lambda_{\rm max}$, $m,m_* \in M$, and condition \ref{eqn:fullrec} holds. Also assume that
  \begin{equation}
      \label{eqn:rcomp}
  \gamma = \frac{r[m,\aw[m;d];d]}{\sqrt{1-r_{\lambda}[m_*;d]^2}} < 1.
  \end{equation}
  Then
  \begin{equation}
      \label{eqn:mbnde}
      |m-m_*| \le \lambda/r + \frac{1}{4\pi r^2 \alpha}\left(\frac{\gamma}{1-\gamma}\right)^{1/2}
   \end{equation}
\end{theorem} 

\begin{proof}
From the definition, $0 \le r_{\lambda}[m;d] < 1 $ for all $m \in M$, so $\gamma$ is well-defined.

Since condition \ref{eqn:fullrec} is assumed to hold, the conclusions of Proposition \ref{thm:epjgen} are available, in particular, equation \ref{eqn:residnormgen}:
  \[
  e[m,\aw[m;d];d] = \frac{1}{2}r[m,\aw[m;d];d]^2 = \frac{1}{2\|d\|^2} \int\, dt\, H(4\pi r\alpha|t-mr|)d(t)^2
  \]
  in which 
  \[
  H(x) = \left( \frac{x^2}{1+x^2}\right)^2.
  \]
  Note that $0 \le H \le 1$ and $H$ is strictly increasing on $\bR^+$. 
  
  Either $|m-m_*|r > \lambda$, or not. In the latter case, the conclusion \ref{eqn:mbnde} follows trivially. In the former case,
  \[
  r[m,\aw[m;d];d]^2 = \frac{1}{\|d\|^2} \int\, dt\, H(4\pi r\alpha|t-mr|)d(t)^2
  \]
  \[
  \ge \frac{1}{\|d\|^2} \int\, dt\, H(4\pi r\alpha|t-mr|)({\bf 1}_{[m_*r-\lambda,m_*r+\lambda]}d(t))^2
  \]
  \[
  = \frac{1}{\|d\|^2} \int\, dt\, H(4\pi r\alpha|t-(m-m_*)r|)({\bf 1}_{[-\lambda,\lambda]}d(t+m_*r))^2
  \]
  \[
  \ge \frac{H(4\pi r\alpha(|m-m_*|r-\lambda))}{\|d\|^2} \int\, dt\, ({\bf 1}_{[-\lambda,\lambda]}d(t+m_*r))^2
  \]
  \[
   = \frac{H(4\pi r\alpha(|m-m_*|r-\lambda))}{\|d\|^2} \int\, dt\, ({\bf 1}_{[m_*r-\lambda,m_*r+\lambda]}d(t))^2
  \]
  \[
  = H(4\pi r\alpha(|m-m_*|r-\lambda))(1 - r_{\lambda}[m_*;d]^2)
  \]
  whence from the definition \ref{eqn:rcomp} of $\gamma$
  \begin{equation}
      \label{eqn:hineq}
    H(4\pi r\alpha(|m-m_*|r-\lambda)) \le \gamma.
  \end{equation}
$H$ is bijective: $\bR^+ \rightarrow [0,1)$. Solving inequality \ref{eqn:hineq} for $4 \pi r \alpha(|m-m_*|r-\lambda)$ and rearranging, obtain the conclusion \ref{eqn:mbnde}. 
\end{proof}

\noindent {\bf Remark:} The parameters $\alpha$ and $\lambda$ play completely independent roles in this result. Given $d \in D$, $m_* \in M$, and $\lambda \in (,\lambda_{\rm max}]$, $r_{\lambda}[m_*;d]$ is the lower bound on relative RMS data error with slowness $m_*$, attained at $w = v_{\lambda}[m;d]$. Select $r_*>0$ so that $r_*^2 < 1 - r_{\lambda}[m_*;d]^2$, and suppose the for each $\alpha>0$, it is possible to find $m_{\alpha} \in M$ so that $r[m_{\alpha},w_{\alpha}[m_{\alpha};d];d] = r_*$. Then from \ref{eqn:mbnde},
\[
|m_{\alpha}-m_*| \le \lambda/r + O(1/\alpha).
\]
That is, this bound suggests that amongst $\alpha>0$ that permit attainment (via proper choice of m) of a prescribed value for the RMS data error at the minimizer of $J_{\alpha}[m,\cdot;d]$, then the bound \ref{eqn:mbnde} on the error between this $m$ and the ``target'' slowness $m_*$ is smaller if $\alpha$ is larger. This observation motivates the Discrepancy Algorithm described in Appendix A. 

\bibliographystyle{seg}
\bibliography{../../bib/masterref}

\append{Penalty weight selection via the Discrepancy Principle}
This appendix describes an algorithm for controlling the penalty
weight $\alpha$ based on a version the Discrepancy Principle
\cite[]{EnglHankeNeubauer,Hanke:17,FuSymes2017discrepancy}. 
The algorithm solves the problem
\begin{quote}
  \label{eqn:probmod}
  given $d \in D$ and $0 < e_- < e_+$,  find  $m \in M, \alpha \in \bR^+$ so that
  \begin{itemize}
  \item[(i) ]$m$ is a stationary point of $\tJa[\cdot;d]$, and
  \item[(ii) ]$e[m,\aw[m;d];d] \in (e_-,e_+)$.
  \end{itemize}
\end{quote} 
I describe an alternating, or coordinate search, algorithm for
solution of this problem, combining a local optimization
algorithm for updating $m$, and a second algorithm for updating
$\alpha$. A first version of this algorithm appeared in \cite{FuSymes2017discrepancy}.Note that the mean square error $e$ lies in an open interval $(e_-,e_+)$ at a solution of this problem. Use of an interval, rather than a single target error level, accomplishes two objectives:
\begin{itemize}
    \item it is consistent with the general lack of precise knowledge of data error in most applications;
    \item it permits a local optimization algorithm to make several small updates of $m$ before an update of $\alpha$ is required, as is required for good performance of algorithms such as BFGS.
\end{itemize}

Given an
objective function $\Phi: M \rightarrow \bR$, a local optimization algorithm 
generates a map $G[\Phi,...]: M \rightarrow M$, mapping a current estimate
of $m$ to an updated estimate. The update rule may depend only on the
current estimate, as is the case for steepest descent, the
Gauss-Newton method, or Newton's method, or may also depend on
information generated during earlier updates, as for secant-type
methods such as BFGS. The notation is intended to allow for this
latter possibility. Under ``standard conditions'' on $\Phi$ and the initial estimate for
$m$, and equipped with so-called globalization safeguards to ensure
satisfaction of sufficient decrease conditions
(see \cite{NocedalWright}), iteration of $G$ produces a sequence in $M$
converging to a local minimizer of $\Phi$. 

A normal stopping criterion for such an algorithm would be a tolerance
test for the norm of $\nabla \Phi$ (and a limit on iteration count, of
course). In application to $\Ja = e + \alpha^2 g$, add another
stopping criterion:
\begin{quote}
  stop if $e \notin [e_-,e_+]$.
\end{quote}
Since $e$ is a summand in $\tJa$, $e$ will typically decrease along a
minimizing sequence, so the expected condition invoking this stopping
rule is $e<e_-$. Denote a update rule with this enhancement,
applied to $\tJa$, by $G[\alpha,e_-,e_+,...]$, still allowing the
possibility of information in addition to the current iterate.

The algorithm also requires a rule for updating $\alpha$, defining a map $H: M \times E
\rightarrow \bR^+$, in which $E=\{(e_-,e_+) \in \bR^2: 0 < 
e_-<e_+\}$. For any $m \in M$ the rule is required to produce an $\alpha$ for which
the error bounds are satisfied: If $(m, e_-, e_+) \in M \times E$ and $\alpha = H(m, e_-,
e_+)$, then $e[m,\aw[m,d];d] \in [e_-,e_+]$. A usable rule is
described below (equation \ref{eqn:alphasecant}).

In outline, the algorithm is as follows:
\begin{algorithm}[H]
\caption{Scheme for updating $m, \alpha$}
\begin{algorithmic}[1]
  \State Choose $m\in M$
  \Repeat
  \State $\alpha \gets H[m,e_-,e_+]$
  \Repeat
  \State $m \gets G[\alpha,e_-,e_+,...](m)$
  \Until{$e[m,\aw[m,d];d] \notin [e_-,e_+]$ or $\|\nabla \tJa[m;d]\|$
    sufficiently small, or...}
  \Until{$e[m,\aw[m,d];d] \in [e_-,e_+]$} 
\end{algorithmic}
\end{algorithm}
Note that after step 3, the error bounds are satisfied, that is,
$e[m,\aw[m;d];d] \in [e_-,e_+]$, but after step 4, that is likely not
to be the case: the alteration of $m$ is likely to reduce $e$, as it is a
summand in the definition of $\tJa$. If the $e$ is reduced below
$e_-$, or if an approximate local minimizer is detected, then the condition in step 6 is satisfied, so control returns
to step 7. If it is satisfied, the algorithm terminates, else
control loops back to step 2, $\alpha$ is updated, and the inner $m$
update loop is entered again. Termination requires that the bounds on
$e$ are satisfied (step 7), hence that the $m$ update loop terminates
by finding a local min which satisfies these bounds. 

The penalty parameter update strategies are based on the following fact about linear combinations of quadratic forms, similar to well-known results from the theory of Tihonov regularization \cite[]{Hanke:17}:

\begin{theorem}
  \label{thm:tich}
  Suppose that $W, D$ are Hilbert spaces, $F \in \cal{B}(W,D)$, $A \in \cal{B}(W,W)$, $F^TF + \alpha^2 A^TA > 0$ for any $\alpha \ge 0$ (in particular, $F^TF>0$), and $A$ is injective. For $d \in D$, define 
  $w: D \times \bR^+ \rightarrow W$,
  $e: D \times \bR^+ \rightarrow \bR^+$, and $g: D \times \bR^+ \rightarrow \bR^+$ by
  \begin{eqnarray}
    w[d,\alpha] &=& (F^TF + \alpha^2 A^TA)^{-1}F^Td \nonumber \\
    e[d,\alpha] &=& \frac{1}{2\|d\|^2}\|Fw[d,\alpha]-d\|^2 \nonumber \\
    g[d,\alpha] &=& \frac{1}{2\|d\|^2}\|Aw[d,\alpha]\|^2
    \label{eqn:tichdefs}
  \end{eqnarray}
  Then for any $d\in D$, $w[d,\cdot]$, $e[d,\cdot]$, and $g[d,\cdot]$ are of class $ C^{\infty}(\bR^+)$, and 
  \begin{itemize}
      \item[0. ] EITHER $d$ is perpindicular to the range of $F$, $e \equiv \frac{1}{2}, g \equiv 0$ for all $\alpha \ge 0$, 
      \item[1. ] OR $e$ is positive and strictly increasing, $g$ is positive and strictly decreasing for  $\alpha\ge 0$, and 
      \begin{equation}
        \label{eqn:lep}
        \frac{de}{d\alpha^2}  \le 2g.
      \end{equation}
  \end{itemize}
\end{theorem}

\noindent {\bf Remark:} The conditions on $F$ and $A$ in the statement of this theorem might seem unduly restrictive. Both hold for the cartoon problem studied in the body of this paper. In fact, in most cases studied in the literature, the extended modeling operator represented here by $F$ is not coercive, and must be modified by regularization to meet the conditions of the theorem. Instances of the penalty operator $A$ studied in the literature are in fact usually injective, though this fact is commonly overlooked. See \cite{Symes:09} for some discussion.

\begin{proof}
Since the normal operator $F^T F + \alpha^2 A^T A$ is boundedly invertible and smooth in $\alpha \ge 0$, $w$ is smooth in $\alpha$, as are $e$ and $g$. 
Differentiate the definition of $w$ 
with respect to  $\alpha^2$ to obtain
\begin{equation}
(F^T F + \alpha^2 A^T A ) \frac{dw}{d\alpha^2} = -A^T A w
\label{eqn:dnorm}
\end{equation}
whence
\begin{align}
\frac{de}{d\alpha^2} 
&=\left\langle\frac{dw}{d\alpha^2},F^T(Fw-d) \right\rangle \nonumber \\
&=-\alpha^2\left\langle\frac{dw}{d\alpha^2},A^TAw\right\rangle \nonumber \\ 
&=\alpha^2 \langle A^TAw,(F^TF + \alpha^2 A^TA)^{-1}A^TAw\rangle \nonumber \\
&\ge 0
\label{eqn:de}
\end{align}
Note that the inequality in equation \ref{eqn:de} is {\em strict}  if $g[d,\alpha] > 0$ hence $A^TAw[d,\alpha] \ne 0$, since the normal operator is assumed to be positive definite.
Also,
\begin{align}
\frac{dg}{d\alpha^2} &=  -\langle A^T Aw,(F^TF + \alpha^2 A^TA)^{-1}A^TAw \rangle \nonumber \\
&\leq 0
\label{eqn:dp}
\end{align}
similarly a strict inequality if $g \ne 0$.

It follows that either $g[d,\alpha]>0$ for all $\alpha\ge 0$, or there exists $\alpha_0\ge 0$ for which $g[d,\alpha]>0$ for $0
\le \alpha<\alpha_0$ and $g[d,\alpha]=0$ for $\alpha \ge \alpha_0$, hence $Aw[d,\alpha]=0$ for $\alpha \ge \alpha_0$. Since $A$ is assumed injective, $w[d,\alpha]=0$ for $\alpha > \alpha_0$. From the definition of $w$, it follows that $F^Td=0$, that is, $d$ is orthogonal to the range of $F$, and in fact $\alpha_0=0$, $g \equiv 0$ and $e \equiv \frac{1}{2}$ for all $\alpha\ge 0$. 

In the first case, that is, $F^Td \ne 0$, equations \ref{eqn:de} and \ref{eqn:dp} show that increasing $\alpha^2$ implies increasing $e$ while decreasing $g$, and
\begin{align}
&\langle A^TA w,(F^TF + \alpha^2 A^TA)^{-1} A^TA w \rangle \nonumber \\ 
=& \lim_{\epsilon \rightarrow 0}\langle (A^TA+\epsilon^2I)^{1/2}w,[(A^TA+\epsilon^2I)^{-1/2}F^TF(A^TA+\epsilon^2I)^{-1/2} + \alpha^2 I]^{-1}(A^TA+\epsilon^2I)^{1/2}w \rangle \nonumber \\ 
\le& \lim_{\epsilon \rightarrow 0}\frac{1}{\alpha^2} \langle (A^TA+\epsilon^2I) w, w\rangle = \frac{2}{\alpha^2}g.
\end{align}
which establishes inequality \ref{eqn:lep}.
\end{proof}

\begin{proposition}
  \label{thm:alphainf}
  Under the hypotheses of Theorem \ref{thm:tich}, 
  \begin{eqnarray}
  \label{eqn:walphainf}
  \lim_{\alpha \rightarrow \infty} w(d,\alpha) & = & 0,\\
  \label{eqn:ealphainf} 
  \lim_{\alpha \rightarrow \infty} e(d,\alpha) & = & 1/2.
  \end{eqnarray}
\end{proposition}
\begin{proof}
  Since $F^TF$ is positive-definite, it has a positive-definite square root, and the definition \ref{eqn:tichdefs} of $w(d,\alpha)$ is equivalent to
  \begin{equation}\label{eqn:ftfw}
  (F^TF)^{1/2}w(d,\alpha) = (I+\alpha^2 (F^TF)^{-1/2}A^TA(F^TF)^{-1/2})^{-1}(F^TF)^{-1/2}F^Td.
  \end{equation}
  Denote by $E:\bR \rightarrow {\cal B}(W,W)$ the resolution of the identity for the self-adjoint bounded operator $(F^TF)^{-1/2}A^TA(F^TF)^{-1/2}$. According to the theorem on spectral representation of functions of self-adjoint operators (\cite{Yosida}, section XI.5, Theorem 1),
  \begin{equation}\label{eqn:specw}
  \|(F^TF)^{1/2}w(d,\alpha)\|^2 = \int_0^{\|F^TF\|}\frac{1}{1+\alpha^2 \lambda}d \|E(\lambda)(F^TF)^{-1/2}F^Td\|^2.
  \end{equation}
  Since $A^TA$ is assumed injective, so is $F^TF)^{-1/2}A^TA(F^TF)^{-1/2}$, whence $0$ is either a member of the resolvent set of the latter operator, or of its continuous spectrum. In either case, $\{0\}$ is a set of measure zero with respect to $\|E(\lambda)(F^TF)^{-1/2}F^Td\|^2$, so the integrand $(1+\alpha^2 \lambda)$ converges to zero $\|E(\lambda)(F^TF)^{-1/2}F^Td\|^2$-almost everywhere as $\alpha \rightarrow \infty$, and is bounded above by $1$ on the spectrum. The Dominated Convergence Theorem implies that $\|(F^TF)^{1/2}w(d,\alpha)\|^2 \rightarrow 0$ as $\alpha \rightarrow 0$, which establishes the first conclusion. The second follows from the definition of $e$ and the continuity of the operators involved in it.
\end{proof}

\begin{proposition}
  \label{thm:psidef}
  In addition to the hypotheses of Theorem \ref{thm:tich}, assume that $F^Td \ne 0$, and that $e[d,0]<e_+ \le 1/2$. Then there exists a unique $\alpha_+ > 0$ satisfying $e_+ = e[d,\alpha_+]$, and $e[d,\alpha]<e_+$ for $\alpha \in [0,\alpha_+)$. Define $\Psi: [0,\alpha_+] \rightarrow \bR$ by
 \begin{equation}
\label{eqn:alphasecant}
\Psi(\alpha)= \left(\alpha^2 + \frac{e_{+}-e[d,\alpha]}{2g[d,\alpha]} \right)^{1/2}
\end{equation} 
Then $\Psi([0,\alpha_+]) \subset (0,\alpha_+]$ and $\alpha<\Psi(\alpha)<\alpha_+$ for $\alpha \in [0,\alpha_+)$. 
\end{proposition}

\noindent {\bf Remark:} Note that $\Psi(0)$ is well-defined and $>0$.

\begin{proof} Existence and uniqueness of $\alpha_+$ follows from Theorem \ref{thm:tich}, Proposition \ref{thm:alphainf}, and the assumption that $e[d,0]<e_+\le 1/2$. Since $e$ is strictly increasing under the assumption that $d$ is not perpindicular to the range of $F$, $e[d,\alpha]<e_+$ for $\alpha \in [0,\alpha_+)$. 

Suppose that $0 \le \alpha \le \psi \le \alpha_+$. From inequality \ref{eqn:lep}, 
\[
e[d,\psi]-e[d,\alpha] \le \int^{\psi^2}_{\alpha^2} d\tau^2 g[d,\tau] 
\]
\begin{equation}
\label{eqn:basic}
< 2 g[d,\alpha] (\psi^2-\alpha^2)  
\end{equation}
since $g$ is positive and strictly decreasing. From the definition \ref{eqn:alphasecant}, $\Psi(\alpha) \ge \alpha$ for $\alpha \in [0,\alpha_+]$, and $\Psi(\alpha)>\alpha$ if $\alpha \in [0,\alpha_+)$. Setting $\psi=\Psi(\alpha)$, 
inequality \ref{eqn:basic} and definition \ref{eqn:alphasecant} imply
\[
e[d,\Psi(\alpha)]-e[d,\alpha] < e_{+}-e[d,\alpha]
\]
so
\begin{equation}
\label{eqn:assert}
\alpha \in [0,\alpha_+) \Rightarrow e[d,\alpha] < e[d,\Psi(\alpha)] < e_{+}.
\end{equation}
The last inequality ans the strict increase of $e$ imply that $\Psi(\alpha)<\alpha_+$ if $\alpha \in [0,\alpha_+)$.
\end{proof}

These results suggest an algorithm to determine $\alpha$: select an initial $\alpha_0 \ge 0$ for which $e[d,\alpha_0]<e_-$, then set $\alpha_n=\Psi(\alpha_{n-1}), n=1,2,...$, until $e[d,\alpha_n] > e_-$. Since $e_-<e_+$, the preceding result implies that $e[d,\alpha_n]<e_+$ also. 

That is, iteration of $\Psi$ produces an increasing sequence in $[0,\alpha_+]$. Two further ingredients are required to view it as an algorithm for production of an $\alpha$ satisfying the discrepancy principle:  a method for selection of an initial $\alpha_0$, and assurance that the error $e$ will eventually exceed the lower bound $e_-$. 

\begin{theorem}
  \label{thm:discrepalg} In addition to the hypotheses of Proposition\ref{thm:psidef}, assume that $e[d,0] <e_-<e_+\le 1/2$. Define the sequence $\{\alpha_n: n \in {\bf Z}_+\}$ by
  \begin{itemize}
      \item $\alpha_0=0$, and
      \item $\alpha_{n+1}=\Psi(\alpha_n), n \in {\bf Z}_+$.
  \end{itemize}
  Then there exists $N=N(d,e_-,e_+) \in {\bf Z}_+$ for which 
  \begin{itemize}
      \item $e(d,\alpha_n) \le e_-$ for $0 \le n <N$, and
      \item $e[d,\alpha_N] \in (e_-,e_+)$
  \end{itemize}
\end{theorem}

\begin{proof}
  By hypothesis, the first conclusion is satisfied for $N=0$. From the definition \ref{eqn:alphasecant} of $\Psi$ and the conclusions of Proposition \ref{thm:psidef}, $\{\alpha_n\}$ is strictly increasing and $\alpha_n \le \alpha_+$ for all $n \ge 0$, so has a limit point $\alpha_{\infty} \in (0,\alpha_+]$. If $\alpha_{\infty}<\alpha_+$, $\epsilon = \Psi(\alpha_{\infty})-\alpha_{\infty} >0$. Since $\alpha \mapsto \Psi(\alpha)-\alpha$ is continuous, there is $\delta>0$ so that if $|\alpha-\alpha_{\infty}|<\delta$, then $\Psi(\alpha)-\alpha >\epsilon/2$. Since $\alpha_{\infty}=\lim_{n\rightarrow \infty}\alpha_n$, there is $m \in {\bf Z}_+$ for which $\alpha_m \in (\alpha_{\infty}-\min(\epsilon/2,\delta),\alpha_{\infty})$, so that $\alpha_{m+1} = \Psi(\alpha_m) >\alpha_m + \epsilon/2>\alpha_{\infty}$, contradicting the definition of $\alpha_{\infty}$ as the limit of the increasing sequence $\{\alpha_n\}$. Therefore, conclude that $\alpha_{\infty}=\alpha_+$.
  
  According to Proposition \ref{thm:psidef}, $\alpha_n \rightarrow \alpha_+$ and $\alpha_n$ increases with $n$,  whence $e[d,\alpha_n] \rightarrow e_+$ and $e[d,\alpha_n]$ increases with $n$. Therefore there must exist a least $n = N(d,e_-,e_+)$ for which $e[d,\alpha_n] \in (e_-,e_+)$.
\end{proof}

\noindent {\bf Remark:} The condition $e(d,0)\le e_-$ means that $\sqrt{2e_-}$ is an upper bound for the distance from $d$ to the range of $F$. Recall that $e_-$ represents an underestimate for data noise level. That is, it is assumed that the data is within data noise level of the range of $F$. Such an assumption is unlikely to be tenable for the physical (non-extended) modeling operator, unless the nonlinear model parameters ($m$ in this discussion) are chosen near-optimally. The ability to fit data, even when the parameters represented by $m$ are greatly in error, is an essential characteristic of successful extended inversion strategies. 

\noindent {\bf Remark:} The update rule \ref{eqn:alphasecant} is convergent to a satisfactory value, as just shown, but in practice is rather slow. Accelerated updates using variants of the secant rule are possible, but an analysis of those possibilities is beyond the scope of this paper. 

A satisfactory penalty parameter selection rule can now be defined: in the notation introduced at the beginning of this appendix, define 
\begin{equation}
    H[m,e_-,e_+] = \alpha_{N(d,e_-,e_+)}.
\end{equation}
Choosing $F=F[m]$ in Theorem \ref{thm:discrepalg}, conclude that $H$, so defined, has the required properties.

This is as far as one can go without more concrete assumptions on $F$
and $A$. If the reduced
objective $\tJa$ possesses a large (in some sense) domain of convexity for a range of $\alpha$, then the discrepancy algorithm sketched
here can efficiently drive $\alpha$ to near its largest feasible
value, thus giving the best resolution near the global minimizer. That
program has been carried out for the simple cartoon problem of this
paper, and partly for some other more protypical inverse wave problems \cite[]{FuSymes2017discrepancy}.

\append{Heisenberg, support, and wavelength}
Without further contraints on data or solution, nothing more can be
said about bounds on $\alpha$. If the data $d$ (and the target wavelet
$w_*$) are assumed to have a square integrable derivative, then a
necessary condition follows from the Heisenberg
inequality (see for example \cite{Folland:07}, p. 255). To formulate
this result in its most general form, introduce the Hilbert subspaces
$V^0 \subset L^2(\bR), V^1 \subset H^1(\bR)$:
\begin{eqnarray}
  V^0 & = & \{ f \in L^2(\bR): Af \in L^2(\bR)\}, \nonumber\\
  V^1 & = & V^0 \cap H^1(\bR).
            \label{eqn:vdef}
\end{eqnarray}
$V^0$ is the domain of $A$, and equipped with the graph norm of $A$. A
natural norm in $V^1$ is
\[
  \|f\|^2_{V^1} = \|f\|_{V^0}^2 + \|f\|_{H^1}^2.
\]
$V^j$ is the completion of $C_0^{\infty}(\bR)$ in the corresponding
norm, j=0,1.

\begin{proposition}
  \label{thm:heis}
For $w \in V^1$,
  \begin{equation}
    \label{eqn:heis}
    \|Aw\|\|w'\| \ge \frac{1}{2}\|w\|^2
  \end{equation}
\end{proposition}

\begin{proof}
  For $w \in C_0^{\infty}(\bR)$,
  \[
    \int w^2 = \left|\int\,dt\, t (w(t)^2)' \right|= \left|2\int\,dt\,tw(t)w'(t)\right| \le
    2\|Aw\| \|w'\|
  \]
  by the Cauchy-Schwarz inequality. Since $C_0^{\infty}(\bR)$ is dense
  in $V^1$, the conclusion follows by continuity.
\end{proof}

In the conventional formulation of the Heisenberg inequality, the $L^2$ norm of
$w'$ is replaced by the its equivalent in terms of the Fourier
transform $\hat{w}$. Adopting temporarily the use of dummy variables
in the expression of functions, the identity \ref{eqn:heis} turns into
the usual form of the Heisenberg inequality: for $w \in V^1$,
\begin{equation}
\label{eqn:fheis}
\|tw(t)\|\|k\hat{w}(k)\| \ge \frac{1}{4\pi}\|w\|^2.
\end{equation}

Define $\krms[w]$, the root mean square estimator of frequency of $w
\in V^1$, by
\begin{equation}
  \label{eqn:krms}
  \krms[w]=\frac{1}{2\pi}\frac{\|w'\|}{\|w\|} = \left(\int
    \,dk\,\frac{|\hat{w}(k)|^2}{\|\hat{w}\|^2} k^2\right)^{1/2}.
\end{equation}
Then the inequalities \ref{eqn:heis}, \ref{eqn:fheis} can be rewritten as
\begin{equation}
  \label{eqn:frmsheis}
  \|Aw\| \ge \frac{\|w\|}{4\pi \krms[w]}.
\end{equation}

For $\lambda >0$, define
\begin{equation}
  \label{eqn:wlam1}
  \lW^1 = \lW \cap H^1(\bR).
\end{equation}
Note that $\lW^1 \subset V^1$ is a closed subspace for any
$\lambda>0$.

\begin{proposition}
  \label{thm:klam}
  For $w \in \lW^1$,
  \[
    \krms[w] \ge \frac{1}{4 \pi \lambda}
  \]
\end{proposition}

\begin{proof}
  Follows directly from inequality \ref{eqn:frmsheis} and the obvious bound
  $\|A|_{\lW}\|\le \lambda$.
\end{proof}

\noindent{\bf Remark:} This result is the link mentioned earlier between the support
constraint and the well-known frequency-based criteria for success of
FWI. The result of Theorem \ref{thm:fwi} can be rephrased as showing the
existence of many stationary points of the mean-square error function
for which the travel time is in error by more than
$\lambda \ge 1/{4 \pi \krms[w]|}$. In fact, the usual error criterion mentioned in the
literature is that the initial estimate of travel time must be in error by at most ``half a
wavelength'' if FWI is to converge reliably. This is correct in some
circumstances, depending on features of the target wavelet $w_*$ of
which the arguments in this paper do not take account.

\begin{proposition}
  \label{thm:heis2}
  For $\lambda>0, w \in \lW^1$,
  \begin{equation}
    \label{eqn:heis2}
    \|A^2w\| \ge \frac{\|w\|}{24 \lambda (2\pi\krms[w])^3}.
  \end{equation}
\end{proposition}

\begin{proof}
  Since $A$  preserves $\lW^1$, inequality \ref{eqn:heis} implies
  \begin{equation}
    \label{eqn:heissq}
    \|A^2w\| \ge \frac{\|Aw\|^2}{2\|(Aw)'\|}.
  \end{equation}
  From the definition of $A$, $(Aw)'=w +Aw'$, so for $w \in \lW^1$,
  \[
    \|(Aw)'\| \le \|w\|+ \lambda\|w'\|.
  \]
  Applying \ref{eqn:heis} again,
  \[
    \|w\|^2 \le 2\|w'\|\|Aw\|\le 2\lambda \|w'\|\|w\|,
  \]
  so obtain the 1D Poincar\'{e} inequality: for $w \in \lW^1$,
  \[
    \|w\| \le 2\lambda\|w'\|.
  \]
  Thus
  \[
    \|(Aw)'\| \le 3\lambda\|w'\|.
  \]
  Apply this estimate together with the basic Heisenberg estimate
  \ref{eqn:heis} to the inequality \ref{eqn:heissq} to obtain
  \[
    \|A^2w\| \ge \frac{\|w\|^4}{24\lambda \|w'\|^3}
  \]
  Rearranging and using the definition \ref{eqn:krms} of $\krms[w]$,
  arrive at inequality \ref{eqn:heis2}.
\end{proof}

\begin{theorem}
  \label{thm:ipnonoisenec}
  Suppose that $a$ is given by definition \ref{eqn:ann}, $\alpha$,
  $\mu \in (0,\lambda_{\rm max}/2]$, $d$ is given by
  \ref{eqn:defdatanonoise} with $w_* \in W_{\mu}^1$, and that a
  stationary point $m_{\infty}$ of $\tJa[\cdot;d]$ yields a solution
  of the inverse problem \ref{eqn:probstat0} for $\lambda \ge 2\mu$,
  $\epsilon>0$. Then
\begin{equation}
  \label{eqn:epsalpha}
  3 (4\pi \lambda \krms[w_*])^3 \epsilon \ge \frac{(4\pi  r\alpha\lambda)^2}
  {(1+(4\pi r \alpha \lambda)^2)}
\end{equation}
\end{theorem}

\begin{proof}
  Rearranging the identity \ref{eqn:residnorm}, and observing as in
  the proof of Theorem \ref{thm:ipnonoisesuf} that the support of the
  integrand is contained in $[-2\mu, 2\mu] \subset [\lambda,\lambda]$,
  \[
  e[m_{\infty},\aw[m_{\infty},d];d] 
= 8 \pi^2 r^2 \alpha^4\int\,dt\,t^4(1+(4\pi r)^2 \alpha^2 t^2)^{-2}w_*(t+(m_{\infty}-m_*))^2
\]
\[
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\int\,dt\,t^4w_*(t+(m_{\infty}-m_*))^2
\]
\begin{equation}
  \label{eqn:residnormbis}
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\|A^2w_*(\cdot + (m_{\infty}-m_*)r)\|^2.
 \end{equation}
Since $\krms[w]$ is invariant under translation of $w \in V^1$,
Proposition \ref{thm:heis2} implies that this is
\[
  \ge \frac{8\pi^2r^2\alpha^4}{(1+(4\pi r)^2 \alpha^2
    \lambda^2)^{2}}\frac{\|w_*\|^2}{(24)^2 \lambda^2 (2\pi\krms[w_*])^6}
\]
\[
  = \frac{1}{2}\frac{(4\pi r\alpha\lambda)^4}{(1+(4\pi r \alpha
    \lambda)^2)^{2}}\frac{\|d\|^2}{(24)^2 (2\pi
    \lambda \krms[w_*])^6}
\]
The pair $(m_{\infty}, \aw[m_{\infty},d])$ is presumed to solve the inverse
problem as stated in \ref{eqn:probstat0}, in particular
\[
  \epsilon \ge (2 e[m_{\infty}, \aw[m_{\infty},d],d])^{1/2}/\|d\|
\]
the inequality \ref{eqn:epsalpha} follows.

\end{proof}

Inequality \ref{eqn:epsalpha} couples the dimensionless quantites
$\epsilon$,  $4\pi r \lambda \alpha$, and $4 \pi \lambda
\krms[w_*]$. Proposition \ref{thm:klam}, implies that the left
hand side is $\ge 3\epsilon$. Since the right hand side is $\le 1$,
the inequality implies no limitation on $\alpha$ if the left hand side
is $\ge 1$. For small $\epsilon$, the the largest permissible $\alpha$
is $O(\sqrt{\epsilon})$. The permissible range of $\alpha$ increases with
nondimensionalized RMS frequency $4\pi \lambda \krms[w_*]$. Since
$\krms$ is invariant under translation and scaling of its argument,
$\krms[w_*]=\krms[d]$, that is, the nondimensionalized RMS frequency
is an observable property of the data, in the noise-free case at
least.

\append{Abstract Structure of the Gradient}
The expression \ref{eqn:dexpjgen} for the derivative of the reduced
objective $\tJa$ is the result of elementary manipulations, based on the
explicit expression \ref{eqn:mod} for the modeling operator $F$. In
this appendix I give an alternative derivation that generalizes to
extended inversion formulations for much less constrained physics. I
will point out the additional reasoning necessary to reach similar
conclusions in these more complex instances of extended inversion, as
presented for example in \cite{StolkSymes:03,StolkDeHoopSymes:09,Symes:IPTA14,tenKroode:IPTA14,HuangSymes:Geo17,HuangSymes:Geo18a,HuangSymes:Geo18b,HuangNammourSymesDollizal:SEG19}.

Recall that $\aw[m;d]$ is the solution of the normal equation
\ref{eqn:norm}. The reduced objective $\tJa$ is given by
\[
  \tJa[m;d] = \Ja[m,\aw[m;d];d]
\]
\[
  = \frac{1}{2}(\|F[m]\aw[m;d]-d\|^2 + \alpha^2\|Aw[m;d]\|^2)
\]
(equation \ref{eqn:redexp1})
\begin{equation}
  \label{eqn:redsimple}
  =\frac{1}{2}(\|d\|^2 - \langle d,F[m]\aw[m;d]\rangle),
\end{equation}
after a little algebra.

As mentioned above, $F:M \times W \rightarrow D$ is {\em not}
differentiable. Neither is $\aw: M \times D \rightarrow W$, as follows
immediately from the identity \ref{eqn:normsol}. However $F\aw: M
\times D \rightarrow D$ is differentiable, hence so is $\tJa: M \times
D \rightarrow \bR$ thanks
to the identity \ref{eqn:redsimple},
under the conditions on the multiplier $a$ identified in Theorem
\ref{thm:diffobj}.

For the remainder of this appendix, assume that
$a \in C^{\infty}(\bR)\cap L^{\infty}(\bR)$ and $a(t)=t$ for $|t|<\tau$ and $|a(t)| \ge
\tau$ for $|t| \ge \tau$, with $\tau$
defined in equation \ref{eqn:taudef}.

Proposition \ref{thm:norminvexp}, item [1], implies that
\begin{equation}
  \label{eqn:faw}
  F[m]\aw[m;d] = F[m](F[m]^TF[m] + \alpha^2 A^TA)^{-1}F[m]^T d.
\end{equation}
The normal operator and its inverse are multiplication
operators, whereas $F$ and $F^T$ are scaled shift operators, inverse
to each other except for scale. From this it is easy to see that the
operator on the RHS of equation \ref{eqn:faw} is multiplication by a
smooth function, with its arguments shifted by $mr$. Such an operator
is smooth in $m$, hence so is $F\aw$.

In the other extended inversion settings mentioned at the beginning of
this appendix, $F$ is a microlocally elliptic Fourier Integral
Operator (FIO) \cite[]{Dui:95}. The canonical relation takes on the role
of the shift operator $t \rightarrow t-mr$, and has the properties
necessary to conclude that the composition of $F$ and $F^T$ in both
ordersis pseudodifferential, at least in an open conic subset of the
cotangent bundle. If $A^TA$ is pseudodifferential, then so is the
normal operator. The inverse in the RHS of equation \ref{eqn:faw} must
be replaced by a microlocal parametrix, and a smooth error added to
the RHS. Thanks to Egorov's Theorem \cite[]{Tay:81}, a special case of
the rules for composing FIOs, the operator on the RHS of equation
\ref{eqn:faw} is a pseudodifferential operator whose symbol is
algebraic in geometric optics quantities, hence depends smoothly on
the coefficients in the wave equation, as does $F\aw$. 

Since $\tJa$ and its gradient depend smoothly on their arguments, it
is possible to derive an alternate expression assuming that $d \in
H^1(\bR)$, then extend it by continuity to $d \in L^2(\bR)$. Note that
if $w \in H^1(\bR)$, then
  differentiable, and 
\begin{equation}
\label{eqn:deriv}
(D(F[m]w)\delta m)(t) = F[m](Q[m]\delta m)w (t), 
\end{equation}
where 
\begin{equation}
\label{eqn:defq}
(Q[m]\delta m)w = -r\delta m \frac{dw}{dt}. 
\end{equation}

That is, $DF[m]\delta m$ factors into $F[m]$ following $Q[m]\delta m$,
where the latter a skew-adjoint differential operator of order 1,
depending linearly on $\delta m$.

Assume that $d \in H^1(\bR)$. From equation \ref{eqn:normsol}, it
follows that $w[m;d] \in H^1(\bR)$ and moreover that $m \mapsto
\aw[m;d]$ is differentiable as a map from $\bR^+$ to
$L^2(\bR)$. From equation \ref{eqn:redexp1},
\[
  \tJa[m;d] = \frac{1}{2}(\|F[m]w[m;d]-d\|^2 + \alpha^2 \|Aw[m;d]\|.
\]
whence $m \mapsto \tJa[m;d]$ is differentiable. A standard calculation
invoking the normal equation \ref{eqn:norm} shows that
\[
  d\tJa[m;d]\delta m = \langle D(F[m]\aw[m;d])\delta m, F[m]\aw[m;d]-d\rangle
\]
\[
  = \langle F[m](Q[m]\delta m)\aw[m;d], F[m]\aw[m;d]-d\rangle
\]
\[
  = \langle (Q[m]\delta m)\aw[m;d], F[m]^T(F[m]\aw[m;d]-d)\rangle
\]
\[
  = -\alpha^2\langle (Q[m]\delta m)\aw[m;d],A^TA\aw[m;d] \rangle
\]
\[
  = \alpha^2 \langle \aw[m;d], (Q[m]\delta m)A^TA \aw[m;d]\rangle
\]
(using antisymmetry of $Q$)
\[
  = \alpha^2 \langle \aw[m;d],[ (Q[m]\delta m), A^TA]\aw[m;d] \rangle
  + \alpha^2 \langle (Q[m]\delta m)\aw[m;d],A^TA\aw[m;d] \rangle
\]
Rearranging,
\begin{equation}
  \label{eqn:djq}
  d\tJa[m;d]\delta m = \frac{1}{2}\alpha^2 \langle \aw[m;d],[
  (Q[m]\delta m), A^TA]\aw[m;d] \rangle.
\end{equation}
Since $Q$ is a differential operator of order 1, and $A^TA$ an
operator of order 0, the commutator has order 0. That is, the RHS of
equation \ref{eqn:djq} defines a continuous quadratic form in $d$ with
respect to the $L^2$ norm. As shown above, the same is true of the
left hand side. Therefore their extensions by continuity to $d \in
L^2(\bR)$ are the same, and the identity \ref{eqn:djq} holds for $d
\in L^2(\bR)$.

Making the identity \ref{eqn:djq} explicit by means of equation
\ref{eqn:defq} and $A^TA = t^2$, substituting the expression
\ref{eqn:normsol} for $\aw[m;d]$, and rearranging: one obtains
precisely the expression \ref{eqn:dexpjgen}.

For the more general contexts mentioned earlier, a similar derivation
is possible. The analog of $Q$ is a pseudodifferential operator of
order 1, which is essentially (to leading order) skew-symmetric, and
the more general version of the identity \ref{eqn:djq} is approximate,
with lower-order (smoother) error terms.

These ideas were used in \cite{tenKroode:IPTA14,Symes:IPTA14} to show
that the analog of $\tJa$ is tangent to second order to a convex
quadratic form, related to the Hessian of an objective formulated in
terms of traveltime. This relation explains the ability of the variant
of extended inversion studied in the cited references to recover
kinematically accurate velocity fields. Of course, that is what is
shown in detail in this paper for the very simple model problem
studied here.
