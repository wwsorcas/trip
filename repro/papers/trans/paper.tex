\title{Waveform Inversion via Source Extension: Major Questions and Research Plan}
\author{William. W. Symes \thanks{The Rice Inversion Project,
Department of Computational and Applied Mathematics, Rice University,
Houston TX 77251-1892 USA, email {\tt symes@caam.rice.edu}.}}

\extrafloats{100}

\lefthead{Symes}

\righthead{Source Extension}

\maketitle
\begin{abstract}
FWI via source extension is a workaround for cycle skipping. For pure transmission (including diving wave) data, a close link to traveltime tomography explains the good results observed in experiments. No such link is known to exist in the case of pure reflection data, and numerical results are mixed. This proposal outlines several numerical experiments that should complement theoretical investigations and decide the effectiveness of these algorithms.
\end{abstract}

\section{Overview}
FWI can be described in terms of a wave operator $L[c]$, depending on an array of space-dependent coefficients $c$, a trace sampling operator $P$, a wavefield $u$, and a source function (of space and time) $f$. The basic FWI problem is: given $d$, find $c$ so that 
\begin{equation}
\label{eqn:fwi}
Pu \approx d \mbox{ and } L[c]u = f.
\end{equation}
Sometimes $f$ is regarded as given, sometimes only some aspects are given (for example, localization at a source point) and others are to be determined as part of the solution. A typical formulation is via nonlinear least squares:
\[
\mbox{choose } c \mbox{ to minimize } \|PL[c]^{-1}f -d \|^2 
\]
usually with some sort of regularization thrown in.

Note that $f$ and $d$ are typically vectors of trace gathers, one for each source position in the survey (or possibly some other parameter, produced by processing - will stick with source position $\bx_s$).

As is well-known, local optimization methods are the only feasible approach given the dimensions of a typical instance of \ref{eqn:fwi}, and those have a tendency to stall due to ``cycle-skipping''. Source extension is one approach to avoiding this problem. It consists in imposing the wave equation as a soft as opposed to hard constraint, by allowing thw source to have more degrees of freedom than is permitted by the basic model. For sake of argument (and because almost all work so far has assumed it), adopt the isotropic point source as the model: for the data gather at source positon $\bx_s$, the source takes the form
\begin{equation}
\label{eqn:ptsrc}
f(\bx,t;\bx_s) = w(t)\delta(\bx-\bx_s).
\end{equation}
Every method explained below can be altered to accommodate more complex source models.

Source extension replaces $f(\bx,t;\bx_s)$ with an artificial source $g(\bx,t; \bx_s)$ with more degrees of freedom than merely a single common source wavelet $w(t)$ as in definition \ref{eqn:ptsrc}. Assuming that \ref{eqn:ptsrc} is actually correct physics, these extra degrees of freedom have to be suppressed in the optimal solution, via an {\em annihilator} $A$, an operator acting on extended sources $g$, whose null space is precisely the physical sources (of the form \ref{eqn:ptsrc}). Extended source inversion replaces the FWI objective with a penalty function
\begin{equation}
\label{eqn:esi}
\mbox{choose } c, g \mbox{ to minimize } \|PL[c]^{-1}g -d \|^2 + \alpha \|A[g]\|^2 
\end{equation}

The way in which degrees of freedom are introduced, and the choice of annihilator, distinguish the various source extension methods.

\subsection{Wavefield Reconstruction Inversion}
Introduced by \cite{LeeuwenHerrmannWRI:13} and significantly modified by \cite{WangYingst:SEG16}. Extended source space: arbitrary functions of space-time $g(\bx,t;\bx_s)$, one for each source position. Annihilator: 
\begin{equation}
\label{eqn:wri}
A[g] = g-f
\end{equation}
With these choices, WRI takes the form introduced by \cite{WangYingst:SEG16}. The original approach is equivalent: \cite{LeeuwenHerrmannWRI:13} use the dyamic wavefield $u$ as the additional degrees of freedom, and minimize
\[
\|Pu-d\|^2 + \alpha \|L[c]u-f\|^2
\]
Since the wave equation (any reasonable version!) has a unique solution for each right-hand side, replacing $L[c]u$ turns this original form of WRI into the form \ref{eqn:esi}.

Note that in princple at least, the actual source $f$ (positions and wavelet) must be known {\em a priori}.

The Wang-Yingst variant leads to a huge reduction in the number of degrees of freedom: either the original or the modified versions, as originally presented, appear to require storage of an entire space-time source, which is prohibitive for 3D and/or time domain formulation. I will explain in an appendix how the Wang-Yingst approach drastically shrinks the storage requirements of this method, and makes it feasible for time-domain 3D application, which Wang and Yingst and their co-authors have demonstrated.

\subsection{Space-Time Extension}
\cite{HuangSymes:SEG16b}. Differs from WRI by choice of annihilator:
\begin{equation}
\label{eqn:msste}
A[g](\bx,t;\bx_s) = |\bx-\bx_s|g(\bx,t;\bx_s).
\end{equation}
This annihilator merely enforces the position constraint on the source, without relying on {\em a priori} knowledge of the source wavelet. In fact, the source wavelet is a by-product of the minimization \ref{eqn:esi}.

Appears to require storage of full space-time source volume - the trick explained in the appendix for WRI does not work, due the form of the annihilator. 

\subsection{Volume Extension}
\cite{HuangSymes:SEG16a}. Assumes that source signature deconvolution has been performed, so that the actual wavelet is (a bandlimited version of) $\delta(t)$. The extended source is an {\em exploding reflector}:
\begin{equation}
\label{eqn:msvol}
g(\bx,t;\bx_s) = h(\bx;\bx_s)\delta(t).
\end{equation}
Uses the same annihilator as the Space-Time Extension, but loses the advantage of solving for the source as by-product - the source must be known, so that decon can be carried out prior to inversion. 

However, only requires a spatial storage volume per source position, essentially equivalent to a second data volume, therefore practical for 3D (though I don't believe it has been tried except in 2D).

The variable projection method applied to this extension has essentially exploding reflector inversion (as opposed to exploding reflector migration) as its inner problem, i.e. estimation of $h$ in \ref{eqn:msvol}. The same problem arises in so-called photoacoustic tomography, and has received a lot of attention in the mathematical inverse problems and biomedical imaging literature. In fact there is a quite accurate approximate inverse available for the inner problem, consisting of converting pressure traces to normal velocity traces and back-projecting (this is known as ``time reversal'') in the IP literature).

A disadvantage of this approach, if you want to handle reflections, is that unlike post-stack migration the actual physical wave velocity is used in modeling, so that the the region in which $h$ must be defined may considerably larger than the ordinary modeling domain - reflection two-way time is effectively modeled by one-way time.

The volume extension concept is similar to an idea discussed by \cite{ZhangGao:08}.

\subsection{Surface Extension}
No publication yet - subject of my workshop talk at the 2016 SEG, and GH has described it in talks at Total. Similar to the volume extension, but the source is spread over the source space-time surface, rather than over space at $t=0$. In the simplest case, where the sources all lie on $z=z_s$, the extended source space consists of functions (distributions) of the form ($\bx=(x,y,z)$)
\begin{equation}
\label{eqn:mssur}
g(\bx,t;\bx_s) = h(x,y,t)\delta(z-z_s).
\end{equation}
As for the previous two cases, the annihilator is given by the localization penalty \ref{eqn:msste}. Therefore the source wavelet is a by-product of the inversion, and does not need to be specified {\em a priori}. Like the volume extension, the additional data volume required is on the same order of size as the data.

\subsection{Adaptive Waveform Inversion = Source-Receiver Extension}
 Popularized by \cite{Warner:14}, \cite{Warner:16}, has many antecedents, listed in \cite[]{HuangSymes:17}. Each trace regarded as separate gather, with separate point source waveform. In effect,
\begin{equation}
\label{eqn:awi}
g(\bx,t;\bx_s) = w(t;\bx_s)\delta(\bx-\bx_s) 
\end{equation}
with repeated occurances of $\bx_s$ regarded as distinct. Usually written by making the wavelet a function of the receiver position also, to distinguish redundant source positions:
\begin{equation}
\label{eqn:awi-alt}
g(\bx,t;\bx_r,\bx_s) = w(t;\bx_r,\bx_s)\delta(\bx-\bx_s) 
\end{equation}
hence the alternate name.

Various annihilators have been suggested - see references. Warner proposes a variant (``AWI'')  that (in principle) requires the source wavelet to be known, and deconvolved from the data traces. Then uses multiplication by $t$, which annihilates $\delta(t)$:
\begin{equation}
\label{eqn:awi-ann}
A[g](\bx,t;\bx_r,\bx_s) = t g (\bx,t;\bx_r,\bx_s)
\end{equation}
AWI distinguished from earlier variants by normalizing the objective with the estimated wavelet $L^2$ norm:
\begin{equation}
\label{eqn:awi-obj}
\mbox{choose } c, g \mbox{ to minimize } \sum_{\bx_r,\bx_s}\frac{\int dt\,|t|^2 |w(t;\bx_r,\bx_s)|^2}{\int dt\,|w(t;\bx_r,\bx_s)|^2}
\end{equation}
Note the order of summation and normalization: each trace is normalized independently (this is not clear from the notation in \cite[]{Warner:16}).

The justification for normalization given by \cite{Warner:16} is that it avoids a familar source/reflector ambiguity. Actually it is more important than that, and plays an important role when no reflectors are present. It is possible to recast the objective \ref{eqn:awi-obj} as equivalent to \ref{eqn:esi} with a weighted norm used in the latter. [I am writing this up - take my word for it!] The weighted norm makes the linear map from source to data {\em unitary} (at least to leading order in frequency), and that is an essential property in relating AWI to traveltime tomography.

\section{What is known}
For several of the source extension algorithms, the objective is known to be closely linked to traveltime tomography in one form or another, and to inherit its convergence properties. These connections are known {\em only} for pure transmission, and not always then. Nothing theoretical is known about any of these methdods for reflection data.

The connection to traveltime tomography was first (so far as I know) established for the source-receiver extension in several TRIP technical reports related to Hua Song's PhD thesis \cite[]{Song:94c,SoSy:92,SongSymes:94a,SongSymes:94b,Symes:94c}. This work established a form of differential semblance velocity estimation for crosswell tomography, and the related tomographic objective was a form of slope, or stereo-, tomography. The time moment objective \ref{eqn:awi-obj} (without normalization) first appears in \cite[]{Plessix:00a,Plessix:00}. A similar idea appears in \cite[]{LuoSava:11}. Neither of these refferences made an explicit connection to tomography - that was done in \cite[]{HuangSymes2015SEG,HuangSymes:17}.

The defect, in the relation between AWI/source-receiver extention and traveltime tomography, is that it collapses when the ray field (must be able to speak of rays, to invoke traveltime tomography) develops caustics. This was already demonstrated both theoretically and by numerical example in \cite[]{Symes:94c}, though the fact does not seem to have percolated very far. \cite{HuangSymes:17} give an explicit illustration of this failure in a diving wave setting. As soon as the diving wave field is triplicated, AWI is just as bad as FWI.

On the other end of the list, almost nothing is known about why WRI converges, or when it does. It may be possible to repeat a version of the analysis mentioned next for the volume and surface extensions, based on geometric optics and the Wang-Yingst variant as described in the appendix, though that has not been tried yet.

The volume and surface extensions have a well-defined relation to tomographic principles (slope or traveltime, depending on type of annihilator). This relation holds regardless of ray geometry (with or without triplication) in transmission configuration.

Again, nothing is known theoretically about the behaviour of any of these methods for reflection data, and numerical evidence is ambiguous.

\section{Proposed Project}.

Amongst the source extension options described in the previous section, only the surface extension has all of these properties:
\begin{itemize}
\item requires only additional storage roughly equivalent to the data volume
\item does not require prior knowledge of the source wavelet, in fact recovers an estimate as a by-product
\item inner problem (in variable projection - estimation of the extended source) has a cheap approximate solution, via back-propagation
\item relation with traveltime inversion persists, regardless of ray geometry
\end{itemize}

Therefoire I propose that we begin with a time-domain implementation of the surface source extension.

\section{Detailed Description}
The simplest version of the surface source extension assumes acoustic wave physics, and that the physical source locations lie on the horizontal surface $z=z_s$. The coefficient vector $c$ is the pair $(\kappa,\beta)$ of bulk modulus and buoyancy, and the wave operator $L[c]$ is given by the linear acoustics system:
\begin{eqnarray}
\label{eqn:awe}
\frac{\partial p}{\partial t} & = & - \kappa (\nabla \cdot \bv +
w(t)\delta(\bx-\bx_s)), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t<0\nonumber\\ 
\bv & = & 0 \mbox{ for } t<0 
\end{eqnarray}
The physical source is modeled as an isotropic point constitutive law defect. As mentioned earlier, it is simple to accommodate more complex source models.

The dynamical field is the 4-tuple $u=(p,\bv)$. Either the pressure $p$ or the particle velocity $\bv$ could be sampled (or both) - presume that the pressure is sampled, that is $Pu = \{p(\bx_r,t;\bx_s)\}$, where $\{\bx_r\}$ are the receiver positions. 

Extended sources are spread over $z=z_s$ (\ref{eqn:mssur}). The annihilator is the source localization penalty, equation \ref{eqn:msste}.

Denote by $J_{\alpha}[g,c;d]$ the objective function defined in \ref{eqn:esi}:
\begin{equation}
\label{eqn:jdef}
J_{\alpha}[g,c;d] = \frac{1}{2}( \|PL[c]^{-1}g -d \|^2 + \alpha \|A[g]\|^2) 
\end{equation}
To compute the objective 
\begin{itemize}
\item solve the acoustic wave system \ref{eqn:awe} with $g$ as in \ref{eqn:mssur}
\item sample at receiver locations and compute mean-square difference with pressure data traces, take mean square
\item add scaled mean square of annihilator applied to the source
\end{itemize}

The variable projection method \cite[]{GolubPereyra:03} applied to the objective \ref{eqn:esi} approaches its minimization via a nested algorithm: first minimize \ref{eqn:esi} over the source $g$ for given coefficients $c$, thus making $g = g[c]$ a function of $c$ (inner minimization); then substitute $g[c]$ in $J$ and minimize the resulting function of $c$ (outer minimization). The optimal $g[c]$ solves the first order necessary condition for minimization of $J$ over $g$: $\nabla_g J_{\alpha}[g,c;d] = 0$. For linear annihilators such as that defined by \ref{eqn:mssur}, $J$ is quadratic in $g$, so the first order necessary condition is also sufficient, and is equivalent to the linear {\em normal equation}:
\begin{equation}
\label{eqn:normal}
(S[c]^TS[c] + \alpha A^TA) g = S[c]^T d
\end{equation}
where I have written $S[c]=PL[c]^{-1}$ for the operator that maps the (extended) source to the data traces. Substitution of $g[c]$ in $J$ results in the {\em reduced objective}
\begin{equation}
\label{eqn:jred}
\tilde{J}_{\alpha}[c;d] = J_{\alpha}[g[c],c;d].
\end{equation}
The gradient would appear to involve the derivative of the inner solution $g[c]$ with respect to $c$, computation of which implies solution of additional linear systems. However by fortunate accident, the chain rule and the normal equation \ref{eqn:normal}
conspire to eliminate all terms involving derivatives of $g[c]$. Consequently the direction derivative of $\tilde{J}_{\alpha}$ in the direction $\delta c$ is
\begin{equation}
\label{eqn:tildejderiv}
D_c\tilde{J}_{\alpha}[c;d]\delta c = \langle D(S[c]g)\delta c,S[c]g-d \rangle|_{g=g[c]}
\end{equation}
whence
\begin{equation}
\label{eqn:l2grad}
\nabla \tilde{J}_{\alpha}[c;d] = D_c(S[c]g)^T|_{g=g[c]}(S[c]g[c]-d)
\end{equation}
Let us field-strip the expression on the right. The rightmost factor is the data residual, which is a by-product of many methods for the solution of the normal equation \ref{eqn:normal}. $D_cS$ is the linearization or Born approximation of $S$, or perturbation with respect to $c$, with $g$ fixed:
\[
S[c+\delta c]g \approx S[c]g + D_c(S[c]g)\delta c
\]
That is, $D_c(S[c]g)$ is the object commonly called the Born modeling operator, with source wavefield $g$. Its adjoint or transpose is $D_c(S[c]g)^T$ - as is well known, this is the RTM operator (or rather a version of it), computed via the adjoint state method in the time domain. Note that the Born and RTM operators need to match, i.e. both be derived from the same underlying full waveform modeling operator (in this case acoustodynamics, but the same would apply for any other wave theory).

The upshot is that the computation of the gradient goes like this:
\begin{itemize}
\item solve the normal equation \ref{eqn:normal} for $g[c]$
\item compute (or retain) the data residual $S[c]g[c]-d$
\item reverse-time migrate the data residual, using $g[c]$ to generate the source wavefield in the adjoint state method - the output is the gradient of $\tilde{J}_{\alpha}$.
\end{itemize}

To make this algorithm practical and efficient, effective preconditioning needs to be applied to both inner and outer problems.

\subsection{Preconditioning the Outer Problem: Gradient Smoothing}
Conditioning the outer (velocity etc.) update has different meaning for transmission versus reflection problems. One interpretation of ``transmission'' is that no reflection occurs on any ray path connecting source and receiver, whereever these may be located. Is is practically implicit in the use of ``ray path'' in this statement that the material model is at least mostly smooth, or essentially localized in low spatial frequencies. Smoothness is also one way to interpret ``lack of reflections''. The classic work explaining the need for model smoothness in tomography is \cite{DelpratLailly:92}. Since the surface source extension method gets its ability to identify kinematically correct velocity from its relation to tomography, it also requires model smoothness, at least for transmission.

A natural method for enforcing smoothness is use of a norm in model space that penalizes lack of smoothness. This method is also properly called preconditioning, which consists in altering the choice of norm to enhance convergence. For pure transmission, the choice recommended by \cite[]{DelpratLailly:92} is a norm from the Sobolev family. These are weighted $L^2$ (RMS) norms with weight operators indexed by a real parameter $s$:
\begin{equation}
\label{eqn:sob}
\|u\|^2_s = \langle u, W_s u \rangle_{L^2},\,\,W_s = (I - \nabla^2)^s
\end{equation}
Choosing $s>0$ penalizes lack of smoothness. According to the Sobolev Embedding Theorem and the basic well-posedness theorem for ordinary differential equations (which governs the behaviour of the ray system), it is neccessary to choose $s > 2 + d/2$ in dimension $d$, in order that metrizing the velocity (amongst other things) with the $s$-norm will guarantee sufficient smoothness that ray theory is well-behaved, and the link with tomography honored. For both $d=2$ and $d=3$, $s=4$ would work.

The gradient of a function is defined with respect to the domain norm. Using the $s$-norm, the gradient is
\begin{equation}
\label{eqn:grads}
\nabla_s \tilde{J}_{\alpha}[c,d] = W_s^{-1}\nabla \tilde{J}_{\alpha}[c,d]
\end{equation}
If $s>0$ then $W_s^{-1}$ is a smoothing operator. It is best implemented in the Fourier domain.

The most natural way view the reflection problem is to allow non-smoothness in $c$. It is not clear at this point what would be the optimal choice of weight operator in this case, or even whether there is a choice of Hilbert space structure that would offer an easily computable gradient and development of short-scale structures to generate reflections. GH used very low frequency data (in frequency domain) and Tihonov regularization. His results seemed to depend heavily on the regularization weight. Of course we also do not know how to connect the reflection case to a tomographic principle, so have no guidance for how a model space norm should be selected. These are the main target questions of this project.

\subsection{Preconditioning the Inner Problem: Time Reversal}
%In contrast to the outer problem, a preconditioner for the inner problem is available that works regardless of medium smoothness, so for both transmission and reflection (the latter with some caveats). 
In contrast to the outer problem, the inner problem admits a conventional preconditioner, that is, a change of variables that in the spaces of source traces and data traces respectively that moves the singular values of the modeling operator closer to 1. This preconditioner also creates an approximate solution operator for the inner problem, of the form
\begin{equation}
\label{eqn:appinv}
S[c]^{\dagger} = W_m[c]^{-1}S[c]^TW_d[c],
\end{equation}
in which $W_m$ and $W_c$ are $c$-dependent positive definite symmetric operators that define weighted inner products in domain and range of $S[c]$, and their associated weighted norms:
\begin{eqnarray}
\label{eqn:norms}
\langle f,g \rangle_m & = & \langle f, W_mg\rangle; \|f\|_m^2 = \langle f,f\rangle_m \nonumber \\
\langle f,g \rangle_d & = & \langle f, W_d g\rangle; \|f\|_d^2 = \langle f,f \rangle_d
\end{eqnarray}
We have implicitly used the notations $\langle \cdot,\cdot \rangle, \|\cdot \|$ (without subscripts) to denote the (unweighted) $L^2$ norm, in whatever form is appropriate for the arguments. That is what these symbols mean in display \ref{eqn:norms}, and will continue to mean.

The approximate inverse $S[c]^{\dagger}$ defined by the relation \ref{eqn:appinv} is the transpose of $S[c]$ with respect to the inner products $\langle \cdot,\cdot \rangle_m,, \langle \cdot,\cdot \rangle_d$, so $S[c]$ is approximately unitary in the same sense. Consequently the singular spectrum (which is also dependent on choice of inner product) is close to that of the identity - or most of it is, as we shall explain - and the convergence of  a Krylov subspace method like Conjugate Gradient (CG) Iteration converge relatively rapidly. 

The system that needs solving, of course, is the normal equation \ref{eqn:normal},  so it is the convergence of CG or the like applied to that system that is the important issue. The normal equation is the first order necessary condition for minimization of the objective
\begin{equation}
\label{eqn:jdefs}
J_{\alpha}[g,c;d] = \frac{1}{2}( \|S[c]g -d \|^2 + \alpha \|A[g]\|^2).
\end{equation}
over the choice of surface source $g$. The choice of norms appearing in \ref{eqn:jdefs} is actually arbitrary, and the norms appearing in the first and second terms on the right-hand side do not even have to be the same. 

%First the transmission case - a sort of rotated crosswell configuration. Idealize the receiver set as a continuum plane $z=z_r$, with $z_r > z_s$ - not essential but simpler. 

%Two key facts and a geometric assumption underly the construction of the time-reversal preconditioner: (i) a local relation between the surface source and the normal particle velocity, and (ii) the uniqueness of solutions of the Dirichlet boundary value problem for the wave equation. Together these facts allow us to reconstruct the pressure field, or the significant part of it, from the transmission data, and to approximate a distributed source responsible for a given set of data traces.

%The geometric assumption is that the pressure field is {\em outgoing} at the recording datum: that is, that it is the restriction to $z>z_s$ of a pressure field that is negligible in $z<z_r$ for large enough time. ``Negligible'', rather than vanishing, as the latter is strictly speaking impossible: this property is asymptotic in frequency, and assured by an assumption about the ray field associated to the acoustic system. 

%NB: the definition of the forward map (and the data) should include a taper $\phi(\bx_r,\bx_s)$ masking out the shot gather $d_r(\cdot,\cdot;\bx_s)$ for each source position $\bx_s$. However I will not explicitly mention this operation. I will also ignore it in the numerical examples, but there will be a price to pay.

To begin with, change notation slightly, and write $S[c]h_s$ rather than $S[c]g$ with $g = h_s \delta(z-z_s)$, as $z_s$ will be presumed ot be the same for all shots. 
%The addition of the subscript indicates that the sources lie on the plane $z=z_s$ and the receivers are on the plane $z=z_r$. As will be apparent shortly, a similar operator with source and receiver planes swapped will also be important, hence the need for the additional information in the notation.

The pressure and velocity fields occuring in the computation of $d_r=p|_{z=z_r}=S[c]_{s,r}h_s$ form the solution of
\begin{eqnarray}
\label{eqn:awedata}
\frac{\partial p}{\partial t} & = & - \kappa( \nabla \cdot \bv +
h \delta(z-z_s)), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t \ll 0 \nonumber\\ 
\bv & = & 0 \mbox{ for } t \ll 0 \nonumber\\
\end{eqnarray}
Actually $p,\bv$ form a {\em weak solution} of the system \ref{eqn:awedata}, defined by integration by parts. A more general setting for this type of system provides an advantageous framework within which to explain the properties of these weak solutions.

Let $\omega \subset \bR^d$ be a smooth orientable surface. Then the distribution $\delta_{\omega}$ is well-defined: for $u \in C_0^{\infty}(\bR^d)$, 
\[
\langle \delta_{\omega}, u \rangle = \int_{\omega} u dS
\]
The system \ref{eqn:awedata} is a special case of
\begin{eqnarray}
\label{eqn:aweomega}
\frac{\partial p}{\partial t} & = & - \kappa( \nabla \cdot \bv +
h \delta_{\omega}, \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t \ll 0 \nonumber\\ 
\bv & = & 0 \mbox{ for } t \ll 0 \nonumber\\
\end{eqnarray}
in which $h \in C_0^{\infty}(\omega)$.

\begin{thm}
\label{thm:bvs}
Suppose that $\kappa,\beta$ are smooth. Then the system \ref{eqn:aweomega} has a unique weak solution $(p,\bv)$. Denote by $[v_n] \in C^{\infty}(\omega)$ the jump of the normal component of $\bv$ across $\omega$:
\[
[v_n](\bx) = \lim_{\epsilon \rightarrow 0} (\bv(\bx + \epsilon {\bf n}(\bx))-\bv(\bx - \epsilon {\bf n}(\bx)))\cdot {\bf n}(\bx),\,\bx \in \omega.
\]
Then $h(\bx) = [v_n](\bx), \bx \in \omega$.

%Let $p,\bv$ be the weak solution of the system \ref{eqn:awedata}, and assume that $\kappa, \beta$ are smooth. Denote by $[v_z]|_{z=z_s}$ the jump of the z-component of particle velocity in the z-direction:
%\[
%[v_z](x,z) = v_z^+(x,z)-v_z^-(x,z),  v_z^{\pm}(x,z)=\lim_{z'\rightarrow z^{\pm}} v_z(x,z')
%\]
%Then $[v_z](x,z)=0$ unless $z=z_s$, and
%\[
%h(x) = [v_z](x,z_s).
%\]
\end{thm}
For the proof, see  Appendix B.

For the rest of this section, $\omega$ will be a coordinate hyperplane, defined by $z=$ constant, with its usual orientation, and I will write $[v_z] = [v_b]$.

To begin with, change notation slightly, and write $S[c]h_s$ rather than $S[c]g$ with $g = h_s \delta(z-z_s)$, as $z_s$ will be presumed ot be the same for all shots. 
%The addition of the subscript indicates that the sources lie on the plane $z=z_s$ and the receivers are on the plane $z=z_r$. As will be apparent shortly, a similar operator with source and receiver planes swapped will also be important, hence the need for the additional information in the notation.

The pressure and velocity fields occuring in the computation of $d_r=p|_{z=z_r}=S[c]_{s,r}h_s$ form the solution of
\begin{eqnarray}
\label{eqn:awedata1}
\frac{\partial p}{\partial t} & = & - \kappa( \nabla \cdot \bv +
h \delta(z-z_s)), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t \ll 0 \nonumber\\ 
\bv & = & 0 \mbox{ for } t \ll 0 \nonumber\\
\end{eqnarray}
Actually $p,\bv$ form a {\em weak solution} of the system \ref{eqn:awedata1}, defined by integration by parts.
\begin{thm}
\label{thm:vz2}
If $\kappa,\beta$ are constant near the $z=z_s$, then $v_z^-=-v_z^+$ so $[v_z] = 2 v_n^+$. 
\end{thm}
(for proof see Appendix 2). The formulation is for 2D propagation, but a precisely similar fact is true for 3D - in that case, $x$ is replaced everywhere by the horizontal coordinates.

The import of Theorem \ref{thm:bvs} is that we can read off the source if we can infer the (particle) velocity field near the source surface, from knowledge of the data traces (values of the pressure at $z=z_r$). The key observation that enables this feat is a well-posedness theorem for the wave equation:

\begin{thm} \label{thm:bvp}
Suppose that $\omega \subset \bR^d$ is a smooth orientable surface, $\omega = \partial  \Omega^- = \partial \Omega^+$, where $\Omega^{\pm}$ are the open exterior/interior regions defined by the outward unit normal field of $\omega$, and let $d \in C^{\infty}_0(\omega \times \bR)$. Then there exist unique smooth solutions $p^{\pm},\bv^{\pm}$ of the initial/boundary value problems 
\begin{eqnarray}
\label{eqn:awebdry}
\frac{\partial p^{\pm}}{\partial t} & = & - \kappa \nabla \cdot \bv^{\pm} \mbox{ in } \Omega^{\pm}, \nonumber \\
\frac{\partial \bv^{\pm}}{\partial t} & = & - \beta \nabla p^{\pm} \mbox{ in } \Omega^{\pm},\nonumber \\
p^{\pm} & =& 0 \mbox{ for } t \ll 0\nonumber\\ 
\bv^{\pm} & = & 0 \mbox{ for } t \ll 0 \nonumber\\
\lim_{\bx \rightarrow \omega^{\pm}}p & = & d 
\end{eqnarray}
\end{thm}


Putting the two theorems together, obtain
\begin{cor}\label{thm:bvps}
Suppose that $d_s$ is a sufficiently regular function, = 0 for $t<0$, and denote by $p^{\pm},\bv^{\pm}$ the half-space solutions provided by Theorem \ref{thm:bvp}. Define the vector fields $p,\bv$ in $\bR^4$ by
\begin{eqnarray}
\label{eqn:putemtogether} 
p(x,z) &=& p^{\pm}(x,z) \mbox{ for } \pm(z-z_s) > 0\\
p(x,z_s)&=&d_s(x)\\
\bv(x,z) &=& \bv^{\pm}(x,z) \mbox{ for } \pm(z-z_s) > 0\\
\end{eqnarray}
Then $p,\bv$ is a weak solution of the system \ref{eqn:awedata1} with $h=[v]_n$.
\end{cor}

Now invoke the {\em outgoing assumption}: $p,\bv$ is the restriction to $z>z_s$ of a solution vanishing in $z<z_r$ for large $t$. This condition needs to be interpreted asymptotically, that is, $p,\bv$ is relatively smooth (lower order in frequency) for $t \gg 0, z<z_r$. It holds if the rays carrying significant energy in $p,\bv$ intersect $z=z_r$ transversally, and do not return to $z<z_r$ in forward time. The examples below will satisfy this condition, because the wave velocity is constant in $z > z_r$.

Note that isotropic point sources do not produce fields outgoing in a half space, in the sense just described - simply because they radiate in all directions. However, for any specified finite time of propagation, it is possible to replace a point source by an anisotropic source that fits the point source data to within an arbitrarily small error, and does produce outgoing fields. In fact, the numerical examples below will construct such sources. 

This assumption implies that for $t$ sufficiently large, $d_r = p|_{z=z_r}$ is negligible for $t \gg 0$, so $p,\bv$ differs in $z_s<z < z_r$ by a negligible error from the solution of $p^-,\bv^-$, part of the solution of the  backwards-in-time  initial value problem, similar to the system \ref{eqn:awebdry}:
\begin{eqnarray}
\label{eqn:awetr}
\frac{\partial \tilde{p}^{\pm}}{\partial t} & = & - \kappa \nabla \cdot \tilde{\bv}^{\pm}, \pm(z-z_r)>0 \nonumber \\
\frac{\partial \tilde{\bv}^{\pm}}{\partial t} & = & - \beta \nabla \tilde{p}^{\pm},\nonumber \\
\tilde{p}^{\pm} & =& 0 \mbox{ for } t \gg 0,\nonumber\\ 
\tilde{\bv}{\pm} & = & 0 \mbox{ for } t \gg 0,\nonumber\\
\lim_{z \rightarrow z_r^{\pm}}\tilde{p}^{\pm} &=&  d_r, \nonumber\\,
\tilde{p} &=& \tilde{p}^{\pm}, \pm(z-z_r)>0, \nonumber\\, 
\tilde{\bv} &=& \tilde{\bv}^{\pm}, \pm(z-z_r)>0.
\end{eqnarray} 

This must be so, as $p,\bv$ satisfies the wave equation system (first two conditions), and the rest as well except for a negligible error. The last condition must be interpreted consistently, that is $\tilde{p}=0$ for $t$ large enough that $d_r$ is negligible. (In practice consistency will be obtained by replacing $t \gg 0$ respectively $t \ll \infty$ by $t > t_{\rm max}$, $t < t_{\rm max}$ for a suitably large $t_{\rm max}$). Then the standard energy estimate shows that $p, \tilde{p}$ differ also by a negligible error: in particular, $p|_{z=z_s} \approx \tilde{p}_{z=z_s}$.

According to Corollary \ref{thm:bvps}, recovering $p_{z=z_s}$ entails recovering $h_s=[v_z]|_{z=z_s}$. So a procedure for computing an approximate inverse $S[c]_{s,r}^{\dagger}$ for $S[c]_{s,r}$ is 
\begin{equation}
\label{eqn:psinv}
S[c]_{s,r}^{\dagger}d_r\equiv [v_z]_{z=z_s}.
\end{equation}
where the jump in $v_z$ at $z=z_s$ is computed from knowledge of $d_r=p|_{z=z_r}$ by
\begin{itemize}
\item solving \ref{eqn:awetr} backwards in time,
\item reading off $d_s=\tilde{p}$ at $z=z_s$,
\item solving \ref{eqn:awebdry} forwards in time, and computing $[v_z]|_{z=z_s}$, or if appropriate ($\kappa$ and $\beta$ constant near $z=z_s$) simply using $\bar{v}_{z=z_s^+} = v_z^{+}=-v_z^{-}$ at $z=z_s$ (Theorem \ref{thm:bvs}).
\end{itemize}

The relations developed in the last few paragraphs may be summarized as follows:
\begin{equation}
\label{eqn:appinv}
d_r = S[c]_{s,r}h_s \,\,\Leftrightarrow \,\, h_s \approx S[c]_{s,r}^{\dagger}d_r
\end{equation}
that is, $S[c]_{s,r}^{\dagger}$ is an approximate inverse of $S[c]_{s,r}$.

Inspection of systems \ref{eqn:awebdry} and \ref{eqn:awetr} reveals that they are actually the same, except for the direction of time, and that observation leads to another useful relation. Denote by $R$ the time-reversal operator:
\[
Ru(\bx,t)=u(\bx,-t)
\]
and by {\cal R} the time reversal operator for acoustic fields:
\[
{\cal R}\left(\begin{array}{c}
p\\
\bv
\end{array}
\right) = 
\left(\begin{array}{c}
Rp\\
-R\bv
\end{array}
\right) 
\]
The sign in the definition of ${\cal R}$ is chosen so that if $(\tilde{p},\tilde{\bv})^T$ solves the system \ref{eqn:awetr}, then ${\cal R}(\tilde{p},\tilde{\bv})^T=(\hat{p},\hat{\bv})^T$ is the solution of 
\begin{eqnarray}
\label{eqn:awetrs}
\frac{\partial \hat{p}^{\pm}}{\partial t} & = & - \kappa \nabla \cdot \hat{\bv}^{\pm}, \pm(z-z_r) > 0 \nonumber \\
\frac{\partial \hat{\bv}^{\pm}}{\partial t} & = & - \beta \nabla \hat{p}^{\pm},\nonumber \\
\hat{p}^{\pm} & =& 0 \mbox{ for } t \ll 0,\nonumber\\ 
\hat{\bv}^{\pm} & = & 0 \mbox{ for } t \ll 0,\nonumber\\
\lim _{z=z_r^{\pm}}\hat{p}^{\pm} &=&  Rd_r,\nonumber\\
\hat{p} &=& \hat{p}^{\pm}, \pm(z-z_r)>0, \nonumber\\, 
\hat{\bv} &=& \hat{\bv}^{\pm}, \pm(z-z_r)>0.
\end{eqnarray} 

%For a smooth orientable surface $\omega$, denote by $\Lambda[c,\omega]$ the operator that maps Dirichlet pressure data $d$ on $\omega \times \bR$ to the jump in normal velocity $[v_n]$ across $\omega \times \bR$ via solution of the system \ref{eqn:awebdry}.

 From Corollary \ref{thm:bvps} applied to the system \ref{eqn:awetrs}, $\hat{p}|_{z=z_s} = S[c]_{r,s}\Lambda[c]_rRd_r$. Denote by $\Lambda[c]_r$ the same operator with $z_s$ replaced by $z_r$. Then $\hat{p}|_{z=z_s} = S[c]_{r,s}\Lambda[c]_rRd_r=R\tilde{p}|_{z=z_s}$. Thus the definition \ref{eqn:psinv} implies
\begin{equation}
\label{eqn:psinvrev}
S[c]_{s,r}^{\dagger}= -\Lambda[c]_sRS[c]_{r,s}R\Lambda[c]_r
\end{equation}
since $\Lambda[c]_rR=-R\Lambda[c]_r$ from the definition of ${\cal R}$.
Define $T[c]_{s,r}d_s = p|_{z=z_r}$ for the solution of \ref{eqn:awebdry}. Since ${\cal R}^2=I$, Then $\hat{p}|_{z=z_s} = T[c]_{r,s}Rd_r$. Since ${\cal R}^2=I$, ${\cal R}(\hat{p},\hat{\bv})^T=(\tilde{p},\tilde{\bv})^T$, whence $\tilde{p}|_{z=z_s}=RT[c]_{r,s}Rd_r$.

Denote by $\Lambda[c]_s$ the operator that produces the jump in normal velocity $[v_z]=\lim_{z \rightarrow z_s^+}v_z^+ - \lim_{z\rightarrow z_s^-} v_z^-$ from Dirichlet pressure data $d_s$ and causal solution of the system \ref{eqn:awebdry}. From Corollary \ref{thm:bvps}, $T[c]_{s,r} = S[c]_{s,r}\Lambda[c]_s$. Denote by $\Lambda[c]_r$ the same operator with $z_s$ replaced by $z_r$. Then $\hat{p}|_{z=z_s} = S[c]_{r,s}\Lambda[c]_rRd_r=R\tilde{p}|_{z=z_s}$. Thus the definition \ref{eqn:psinv} implies
\begin{equation}
\label{eqn:psinvrev}
S[c]_{s,r}^{\dagger}= -\Lambda[c]_sRS[c]_{r,s}R\Lambda[c]_r
\end{equation}
since $\Lambda[c]_rR=-R\Lambda[c]_r$ from the definition of ${\cal R}$.

As I will show in the example section, the approximation may be accurate enough without any further improvement. However, a closer examination shows that in fact it is possible to express the relation between $S[c]_{s,r}$ and $S[c]_{s,r}^{\dagger}$ in terms of a change of norm, that is, in terms of a preconditioner, and this relation can be used to considerably accelerate an iterative solution of the normal equation \ref{eqn:normal}. 

To see this, compute the formal or $L^2$ adjoint $S[c]_{s,r}^T$, by a variant of the adjoint state method, in this case a by-product of the conservation of energy. Suppose that $\bar{h}_r$ is a function on $z=z_r$ (data traces), and 
$\bar{p},\bar{\bv}$ solve the backwards-in-time boundary value problem:
\begin{eqnarray}
\label{eqn:aweadj}
\frac{\partial \bar{p}}{\partial t} & = & \kappa (-\nabla \cdot \bar{\bv} + 
 \bar{h}_r \delta(z-z_r))\nonumber \\
\frac{\partial \bar{\bv}}{\partial t} & = & - \beta \nabla \bar{p},\nonumber \\
\bar{p} & =& 0 \mbox{ for } t \gg 0\nonumber\\ 
\bar{\bv} & = & 0 \mbox{ for } t \gg 0 
\end{eqnarray} 
Then
\[
0 = 
\left(\int\, dx\,dy\,dz\, \frac{p \bar{p}}{\kappa} +  
\frac{\bv \cdot \bar{\bv}}{\beta} \right)|_{t=T}
-
\left(\int\, dx\,dy\,dz\, \frac{p \bar{p}}{\kappa} +  \frac{\bv \cdot \bar{\bv}}{\beta} \right)|_{t=0}
\]
\[
= 
\int_{0}^{T} \,dt\, \frac{d}{dt}\left(\int\, dx\,dy\,dz\, \frac{p \bar{p}}{\kappa} +  \frac{\bv \cdot \bar{\bv}}{\beta} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \frac{1}{\kappa} \frac{\partial p}{\partial t} \bar {p} +  p \frac{1}{\kappa}\frac{\partial \bar{p}}{\partial t} \right.
\]
\[
+
\left. \frac{1}{\beta} \frac{\partial \bv}{\partial t} \cdot \bar{\bv} + \frac{1}{\beta} \bv \cdot \frac{\partial \bar{\bv}}{\partial t} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 h_s \delta(z-z_s)\right) \bar{p} + p \left(- \nabla \cdot \bar{\bv} + 
 d_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  (- \nabla p) \cdot \bar{\bv} + \bv \cdot (-\nabla \bar{p}) \right)
\]
\[
= 
\int_{0}^{T}\,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 h_s \delta(z-z_s)\right) \bar{p} + p \left(- \nabla \cdot \bar{\bv} + 
 d_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  p (\nabla \cdot \bar{\bv}) + (\nabla \cdot \bv) \bar{p} \right)
\]
after integration by parts in the last two terms. Most of what is left cancels, leaving 
\[
0 = \int\,dt\,dx\,dy\, (h_s \bar{p})_{z=z_s} + (p  \bar{h}_r)_{z=z_r}
\]
whence
\begin{equation}
\label{eqn:sadj}
 S[c]_{s,r}^T \bar{h}_r = -\bar{p}|_{z=z_s}
\end{equation}
Compare the systems \ref{eqn:awetr} and \ref{eqn:aweadj}: they differ in that 
\begin{itemize}
\item in \ref{eqn:awetr}, $d_r$ appears as a Dirichlet condition on $z=z_r$,
\item in \ref{eqn:aweadj}, $\bar{h}_r$ appears as a distributed source on $z=z_r$.
\end{itemize}
According to Corollary \ref{thm:bvps}, $\bar{p}=\bar{p}, \bar{\bv}=\bar{\bv}$ if $\bar{h}_r = [\bar{v}_z]|_{z=z_r}$, i.e. the source is that which generates the same solution as the Dirichlet condition..

Simlarly denote by $\tilde{\Lambda}[c]_r$ the operator that produces the jump in normal velocity at $z=z_r$ from pressure data $d_r$ via {\em anticausal} solution of the system 
\begin{eqnarray}
\label{eqn:awebdrytr}
\frac{\partial {\tilde p}^{\pm}}{\partial t} & = & - \kappa \nabla \cdot {\tilde \bv}^{\pm} \mbox{ in } \pm(z-z_r) > 0, \nonumber \\
\frac{\partial {\tilde \bv}^{\pm}}{\partial t} & = & - \beta \nabla {\tilde p}^{\pm} \mbox{ in } \pm(z-z_r) > 0,\nonumber \\
{\tilde p}^{\pm} & =& 0 \mbox{ for } t \gg 0\nonumber\\ 
{\tilde \bv}^{\pm} & = & 0 \mbox{ for } t \gg 0 \nonumber\\
\lim_{z\rightarrow z_r^{\pm}}{\tilde p} & = & {\tilde d}_r 
\end{eqnarray}
%\end{thm}
 for the receiver surface $z=z_r$ (in the math literature, this is the ``Dirichlet-to-Neumann operator'', more or less). The discussion so far can be summarized as follows:
\begin{equation}
\label{eqn:wadj}
\bar{p}|_{z=z_s} = S[c]_{s,r}^T\Lambda[c]_r \bar{p}|_{z=z_r} 
\end{equation}
According to the discussion above, $\bar{p}|_{z=z_s} \approx p|_{z=z_s}$, and $h_s = \Lambda[c]_s p|_{z=z_s} = S[c]_{s,r}^{\dagger}d_r$. That is,
\begin{equation}
\label{eqn:appinv}
S[c]_{s,r}^{\dagger} = \Lambda[c]_s S[c]_{s,r}^T \Lambda[c]_r
\end{equation}
This relation can be re-written in the useful form
\begin{equation}
\label{eqn:adj}
S[c]_{s,r}^{\dagger} = W_m[c]^{-1}S[c]_{s,r}^TW_d[c]
\end{equation}
in which 
\begin{equation}
\label{eqn:weights}
W_m[c] = \Lambda[c]_s^{-1}\,\, W[c]_d = \Lambda[c]_r
\end{equation}
Since $\Lambda[c]$ is positive (semi-)definite, equation \ref{eqn:adj} exhibits $S[c]_{s,r}^{\dagger}$ as the adjoint of $S[c]_{s,r}$ with respect to weighted norms 
\begin{itemize}
\item on the source space ($h_s$): weight $W[c]_m$
\item on the data space ($d_r$): weight $W[c]_d$
\end{itemize}
Since $S[c]_{s,r}^{\dagger}S[c]_{s,r} \approx I$ as shown above, $S[c]_{s,r}$ is approximately unitary in the Hilbert spaces of source and data traces with norms defined by the weighting operators $W_m$ and $W_d$ respectively. Therefore a Krylov space method employing these norms will converge rapidly.

\cite{HouSymes:EAGE16} demonstrated a very similar preconditioner for Least Squares Migration, also for its subsurface offset extension \cite[]{HouSymes:16}, motivated by \cite{tenKroode:12}. These constructions all involve the Dirichlet-to-Neumann operator. This concept also turns up in hidden form in the work of Yu Zhang and collaborators on true amplitude migration \cite[]{YuZhang:14,TangXuZhang:13,XuWang:2012,XuZhangTang:11,Zhang:SEG09,ZhangYuSun:08,ZhangSunGray:07,ZhangBleistein:05,Bleisteinetal:05}.
 
\subsection{Implementation Considerations}
The Dirichlet-to-Neumann operator $\Lambda$ is an essential part of the inner problem preconditioner just presented. Implementation can be accomplished in several ways:
\begin{itemize}
\item \cite{tenKroode:12} suggests using a one-way operator;
\item if both pressure and normal partical velocity are measured (or simulated), then the two are related by $\Lambda$ and the velocity component can simply be used as the output;
\item presence of a free surface implies all of the usual problems, such as the need for removal of receiver-side ghosts. On the other hand, if the free surface is within a quarter-wavelength throughout the useful bandwidth of the data, then the ghosted data differs from $\Lambda p$ by a time integration and a scale factor, a fact used to good effect by \cite{HouSymes:15}.
\end{itemize}

In the examples, I have used the second observation. With a finite difference implementation of the pressure-velocity system \ref{eqn:awe}, velocity components are available ``for free'', short-circuiting explicit computation of $\Lambda$.

The variable projection gradient formula \ref{eqn:l2grad} has an Achilles heel, not mentioned so far: it is correct only when the normal equation \ref{eqn:normal} is satisfied {\em exactly}. Any iterative method (or the asymptotic inverse developed in the last subsection) solves \ref{eqn:normal} only approximately, so a nonzero error term is neglected in using \ref{eqn:l2grad}. This error term can be large, because it amplifies the high frequencies in the solution error, in fact depends on derivatives of the solution. For pure transmission through transparent (smooth) material models, the approximate inverse $S[c]_{s,r}^{\dagger}$ is asymptotically exact (at least in principle), and that fact can be used to devise a modification of \ref{eqn:l2grad} that converges with a convergent iteration for \ref{eqn:normal}. [LAST YEAR TRIP REPORT, MAY PUT SOMETHING IN APPENDIX]

Choice of the penalty weight $\alpha$, and of region in which the extended source $g$ is permitted to be nonzero, must be managed. These two choices are linked. Lei Fu's thesis \cite[]{Fu:Geo17,Fu:Geo17b} explains and tests algorithms for similar choices that occur in subsurface offset WEMVA. All of these ideas apply {\em ipso facto} to the surface source extension.

\subsection{Practical Preconditioning}

The formula \ref{eqn:adj} allows direct use of the conjugate gradient (or other Krylov space iterative) algorithm with the inner products described above. This is however computationally inconvenient, as it requires explicit computation of the weight operators, in this case the Dirichlet-to-Neumann operator. There are several ways to carry out this computation, as mentioned somewhere, but it is in any case a  pain. On the other hand, it is possible to rearrange the calculation with weighted norms into the form of the {\em preconditioned conjugate gradient (PCG) algorithm}. For the current problem, identify $A=S[c]_{s,r}$, $A^* = S[c]_{s,r}^T W_d$. PCG takes the form
\begin{algorithm}[H]
\caption{Preconditioned Conjugate Gradient Algorithm}
\begin{algorithmic}[1]
\State Choose $x_0$ somehow
  \State $r_0 \gets A^*(b-Ax_0)$
  \State $p_0 \gets W_m^{-1}r_0$
  \State $g_0 \gets p_0$
  \State $q_0 \gets A^*Ap_0$
  \State $k \gets 0$
  \Repeat
  \State $\alpha_k \gets \frac{\langle g_k,r_k \rangle}{\langle p_k,q_k\rangle}$
  \State $x_{k+1} \gets x_k + \alpha_k p_k$
  \State $r_{k+1} \gets r_k - \alpha_kq_k$
  \State $g_{k+1} \gets W_m^{-1}r_{k+1}$
  \State $\beta_{k+1} \gets \frac{\langle g_{k+1},r_{k+1}\rangle}{\langle g_k,r_k\rangle}$
  \State $p_{k+1}\gets g_{k+1}+\beta_{k+1}p_k$
  \State $q_{k+1} \gets A^*Ap_{k+1}$
  \State $k \gets k+1$
  \Until{Error is sufficiently small, or max iteration count exceeded} 
\end{algorithmic}
\end{algorithm}
Of course, this variant of CG merely hides the data sides weight operator, and requires application on the model side. This is still inconvenient. 

The simplest solution is to use the coexistence of the $d_r=p$ and $\Lambda[c]_rd_r =[v_z]=2v_z$ traces (assuming a horizontal datum), and similarly for the source datum. That is, the computation of $\Lambda[c]_r$ is intrinsic: when $S[c]_{s,r}h_s$ has been computed, so has $\Lambda[c]_rS[c]h_s$ - it is simply the collection of corresponding vertical velocity traces. Similarly,  $\Lambda[c]_sS[c]^T$ is the collection of vertical velocity traces at $z=z_s$ arising from the time-reversal computation input equal to sources traces on $z=z_r$. If we can rewrite the PCG loop to require only these computations, then the acoustic simulations in forward and reverse time will suffice to compute the step.

In the loop body above, introduce the vector sequences $t_k=Ap_k,v_k=W_d t_k, s_k=W_m^{-1}q_k$, and re-write the iteration by unfolding $W_m^{-1}S[c]^T W_d S[c]$ and noting that $\langle p_k, q_k \rangle =\langle p_k,A^TW_dAp_k \rangle =\langle Ap_k,W_dAp_k \rangle =\langle t_k,v_k\rangle$: 
\begin{algorithm}[H]
\caption{Modified Preconditioned Conjugate Gradient Algorithm}
\begin{algorithmic}[1]
\State Choose $x_0$ somehow
  \State $r_0 \gets A^*(b-Ax_0)$
  \State $p_0 \gets W_m^{-1}r_0$
  \State $g_0 \gets p_0$
  \State $k \gets 0$
  \Repeat
  \State $t_k \gets Ap_k$
  \State $v_k \gets W_dt_k$
  \State $q_k \gets A^Tv_k$
  \State $s_k \gets W_m^{-1}q_k$
  \State $\alpha_k \gets \frac{\langle g_k,r_k \rangle}{\langle t_k,v_k\rangle}$
  \State $x_{k+1} \gets x_k + \alpha_k p_k$
  \State $r_{k+1} \gets r_k - \alpha_kq_k$
  \State $g_{k+1} \gets g_k - \alpha_k s_k$
  \State $\beta_{k+1} \gets \frac{\langle g_{k+1},r_{k+1}\rangle}{\langle g_k,r_k\rangle}$
  \State $p_{k+1}\gets g_{k+1}+\beta_{k+1}p_k$
  \State $k \gets k+1$
  \Until{Error is sufficiently small, or max iteration count exceeded} 
\end{algorithmic}
\end{algorithm}
This rewrite does not look like an improvement - four operator applications appear to be required:
\begin{itemize}
\item $p_k \rightarrow S[c] p_k = t_k$
\item $p_k \rightarrow  \Lambda[c]_r S[c] p_k = v_k$
\item $v_k \rightarrow S[c]^Tv_k = q_k$
\item $v_k \rightarrow \Lambda[c]_s S[c]^T v_k = s_k$
\end{itemize}
However this rearrangement reveals that actually only two wave equation solves are needed. For $q_k$ and $s_k$, both by-products of the adjoint (time-reversed) simulation: $q_k$ stores the pressure traces on $z=z_s$, $s_k$ the velocity traces. The same is true of $t_k$ and $v_k$, with the latter being the input for the $q_k,s_k$ computation.

Denote by $F_{pp}$ the acoustic simulation operator with a $p$ source (constitutive law defect, as in the system \ref{eqn:awedata}), and $p$ output traces. Similarly, denote by $F_{pv}$ the simulation operator with $p$ source and $v_z$ output traces, likewise $F_{vp}$ and $F_{vv}$, and by ${\bf F}$ the matrix operator
\begin{equation}
\label{eqn:iwop}
{\bf F} = \left(
\begin{array}{cc}
F_{pp} & F_{vp} \\
F_{pv} & F_{vv}
\end{array}
\right)
\end{equation}
The {\tt IWaveLOVOp} built in the asg package implements this matrix operator if the keywords {\tt source\_p, source\_v0, data\_p, data\_v0} are defined in its parameter list: these keywords point to SEGY trace files that define the spaces in which the $p$ and $v_z$ sources respectively data traces are treated as vectors. That is, IWAVE provides an easy access to ${\bf F}$. In terms of the operators used in the statement of the inverse problem, $F_{pp}=S[c]$, $F_{pv}=\Lambda[c]_rS[c]$, $F_{vp} = S[c]\Lambda[c]_s$, $F_{pp}^T = S[c]^T$, and $F_{vp}^T = \Lambda[c]_sS[c]^T$. So if we define 
\begin{equation}
\label{eqn:vecvec}
{\bf p}_k = \left[
\begin{array}{c}
p_k\\
0
\end{array}
\right],\,\,
{\bf r}_k = \left[
\begin{array}{c}
r_k\\
g_k
\end{array}
\right],\,\,
{\bf t}_k = \left[
\begin{array}{c}
t_k\\
v_k
\end{array}
\right],\,\,
{\bf v}_k = \left[
\begin{array}{c}
v_k\\
0
\end{array}
\right],\,\,
{\bf q}_k = \left[
\begin{array}{c}
q_k\\
s_k
\end{array}
\right],
\end{equation}
and
\begin{equation}
\label{eqn:proj}
{\bf P} = \left(
\begin{array}{cc}
0 & I \\
0 & 0
\end{array}
\right),
\end{equation}
then the algorithm listing above can be rewritten as
\begin{algorithm}[H]
\caption{Further modified Preconditioned Conjugate Gradient Algorithm}
\begin{algorithmic}[1]
\State Choose $x_0$ somehow
  \State $r_0 \gets A^TW_d(b-Ax_0)$
  \State $p_0 \gets W_mr_0$
  \State $g_0 \gets p_0$
  \State $k \gets 0$
  \Repeat
  \State ${\bf t}_k \gets {\bf F}{\bf p}_k$
  \State ${\bf v}_k \gets {\bf P}{\bf t}_k$
  \State ${\bf q}_k \gets {\bf F}^T{\bf v}_k$
  \State $\alpha_k \gets \frac{\langle g_k,r_k \rangle}{\langle t_k,v_k\rangle}$
  \State $x_{k+1} \gets x_k + \alpha_k p_k$
  \State ${\bf r}_{k+1} \gets {\bf r}_k - \alpha_k{\bf q}_k$
  \State $\beta_{k+1} \gets \frac{\langle g_{k+1},r_{k+1}\rangle}{\langle g_k,r_k\rangle}$
  \State ${\bf p}_{k+1}\gets {\bf P}{\bf r}_{k+1}+\beta_{k+1}{\bf p}_k$
  \State $k \gets k+1$
  \Until{Error is sufficiently small, or max iteration count exceeded} 
\end{algorithmic}
\end{algorithm}

Note that
\begin{equation}
\label{eqn:note}
{\bf r}_0 = \left[
\begin{array}{c}
r_0\\
g_0 
\end{array}
\right] = 
\left[
\begin{array}{c}
r_0\\
p_0 
\end{array}
\right] = 
\left[
\begin{array}{c}
A^TW_d(b-Ax_0)\\
W_m^{-1}A^TW_d(b-Ax_0)
\end{array}
\right]
\end{equation}
Define 
\begin{equation}
\label{eqn:rhsdef}
{\bf b}
= 
\left[
\begin{array}{c}
W_d(b-Ax_0)\\
0
\end{array}
\right].
\end{equation}
Then identity \ref{eqn:note} implies that
\begin{equation}
\label{eqn:init}
{\bf r}_0= {\bf F}^T{\bf b},\,\, {\bf p}_0 = {\bf P}{\bf r}_0.
\end{equation}
This observation allows a convenient ``hoist'' of the $k=0$ operator applications out of the iteration loop. Also, it is convenient to introduce the indefinite quadratic form that returns the inner product of the two components of a vector in the product space introduced above:
\begin{equation}
\label{eqn:dotfunc}
Q({\bf x})=\langle x_0,x_1\rangle=\left\langle \bx,
\left(
\begin{array}{cc}
0 & I \\
I & 0 
\end{array}
\right) \bx \right\rangle
\end{equation}
and the scalar sequences
\begin{eqnarray}
\label{eqn:scalars}
\gamma_k &=& \langle r_k,g_k \rangle = Q({\bf r})\\
\delta_k &=& \langle t_k,v_k \rangle = Q({\bf t})
\end{eqnarray}

With these modifications, we obtain 
\begin{algorithm}[H]
\caption{Intrinsically Preconditioned Conjugate Gradient Algorithm}
\begin{algorithmic}[1]
\State Choose $x_0$ somehow
\State ${\bf b} \gets \left[
\begin{array}{c}
W_d(b-Ax_0)\\
0
\end{array}
\right]$
  \State ${\bf r}_0 \gets {\bf F}^T{\bf b}$
  \State ${\bf p}_0 \gets {\bf P}{\bf r}_0$
  \State $\gamma_0 \gets Q({\bf r}_0)$
  \State $k \gets 0$
  \Repeat
  \State ${\bf t}_k \gets {\bf F}{\bf p}_k$
  \State $\delta_k \gets Q({\bf t})$
  \State ${\bf v}_k \gets {\bf P}{\bf t}_k$
  \State ${\bf q}_k \gets {\bf F}^T{\bf v}_k$
  \State $\alpha_k \gets \frac{\gamma_k}{\delta_k}$
  \State $x_{k+1} \gets x_k + \alpha_k p_k$
  \State ${\bf r}_{k+1} \gets {\bf r}_k - \alpha_k{\bf q}_k$
  \State $\gamma_{k+1} \gets Q({\bf r}_{k+1})$
  \State $\beta_{k+1} \gets \frac{\gamma_{k+1}}{\gamma_k}$
  \State ${\bf p}_{k+1}\gets {\bf P}{\bf r}_{k+1}+\beta_{k+1}{\bf p}_k$
  \State $k \gets k+1$
  \Until{Error is sufficiently small, or max iteration count exceeded} 
\end{algorithmic}
\end{algorithm}


\section{Prototype Numerical Examples}
\inputdir{project}

I present a collection of simple examples that illustrate the features of the surface source extension claimed in preceding sections. 

I used the IWAVE acoustic staggered grid package to carry out these calculations. This package implements (2,2k) schemes for k=1,2,..., and outputs traces (of either velocity or pressure at any point in space via multilinear interpolation. The discretized modeling operator is thus of second order accuracy, though as usual I have used higher order in space to reduce grid dispersion.

Source injection is implemented as the adjoint of trace sampling, resulting in another second-order error [REFERENCES]. 

The data is a single shot gather, with a source at coordinates $x_s=z_s=3000$ m (units of length are meters in all cases). The receiver line occupies $1500 \le x_r \le 5500$ m, with receiver depth $z_r=1000$ m.  Extended sources occupy $1500 \le x_s \le 5500$ m, with the same depth $z_s=3000$ m as the ``physical'' source used to generate the data. This region turns out to be adequate to represent the extended sources that approximately invert the data, for the cases examined below. An algorithm to automatically identify an appropriate region can be based on the ideas developed by \cite{Fu:Geo17}.

I have used absorbing boundary conditions (split-field PML) on all four sides of the 4000 m (vertical) $\times$ 8000 m (horizontal) simulation domain. Evidently includion of a free surface is important to the application of the ideas explained here to diving wave marine data, and I have not addressed the necessary modifications here. 

The script is set up to carry out the necessary computations on grids with spacings $\Delta x = \Delta z = $ 20, 10, and 5 m, with a jump of roughly 8 in computation time resulting from each refinement. For present purposes, the coarsest (20 m) grid seems to be sufficient, and that is the grid used in the examples presented below. The source pulses are chosen so that the computation is reasonably accurate. For the 20 m grid case, I use a zero-phase trapezoidal bandpass filter source with corner frequencies of 1.0, 2.0, 7.5, and 12.5 Hz.

The IWAVE asg driver has been set up to recognize the case {\tt deriv=0} as defining the map from source (right-hand side in the pressure equation) to data (pressure) traces. The adjoint to this map, as explained above, is reverse-time propagation of the data traces as pressure sources, followed by scaling (formula \ref{eqn:sadj}). The approximate inverse is computed by application of the Dirichlet-to-Neumann operator to the pressure traces to produce corresponding velocity traces, followed by injection as pressure sources and reverse time propagation, followed by another application of the Dirichlet-to-Neumann map and scaling (formulas \ref{eqn:appinv}, \ref{eqn:adj}).

The script for the first set of examples implements these operatations step-by-step via calls to IWAVE, Madagascar, and SU commands. A peculiarity of the {\tt asg.x} driver needs to be mentioned: it is based on an un-scaled version of the constitutive law defect source representation, that is, {\tt asg.x} approximately computes the solution of the system \ref{eqn:awedata} with the first equation replaced by
\begin{equation}
\label{eqn:asgdata}
\frac{\partial p}{\partial t}  =  - \kappa \nabla \cdot \bv +
h \delta(z-z_s).
\end{equation}
Denote by $S_{\rm asg}[c]$ the forward map produced by {\tt asg.x}. Then comparison of \ref{eqn:awedata} and \ref{eqn:asgdata} reveals that
\begin{equation}
\label{eqn:sreln}
S[c]=S_{\rm asg}[c]\kappa
\end{equation}
where $\kappa$ is shorthand for the operator of multiplication by $\kappa$. Accordingly, and approximate inverse for $S_{\rm asg}[c]$ is
\[
I \approx S_{\rm asg}[c]^{\dagger}S_{\rm asg}[c] = S_{\rm asg}[c]^{\dagger}S[c]\kappa^{-1}
\]
Since $S[c]S[c]^{\dagger} \approx I \approx S[c]^{\dagger}S[c]$, it follows that
\[
S_{\rm asg}[c]^{\dagger}=\kappa S[c]^{\dagger} = \kappa\Lambda[c]_s S[c]^T \Lambda[c]_r 
\]
\begin{equation}
\label{eqn:asginv}
= \kappa \Lambda[c]_s \kappa S_{\rm asg}[c]^T\Lambda[c]_r
\end{equation}
This is the approximate inverse computed in the examples. Note that only the values of $\kappa$ near the source datum $z=z_s$ play a role in the relation \ref{eqn:sreln} or in the definition \ref{eqn:asginv} of the IWAVE ASG approximate inverse.

These examples are based on two different material models. The first is a homogeneous model, with $\kappa=4.0$ GPa and $\rho=1.0$ g/cm$^{3}$. Figure \ref{fig:ptpwindh0} shows the shot gather computed in the configuration descritbed above for this choice of propagation medium. As mentioned in the preceding section, the velocity traces can be recorded in the same IWAVE workflow: these are displayed in 
\ref{fig:ptvzwindh0}. 

The first group of plots show pressure traces for source position $z_s=3$ km, $x_s=3$ km, 201 receivers at $z_r=1$ km, $2 \le x_r \le 6$ km. 

\plot{ptpwindh0}{width=15cm}{Receiver line at $z_r=1$ km for source at $z_s=3$ km, $x_s=3$ km. Homogeneous material model, $\kappa=4$ GPa, $\rho$=1 g/cm$^{3}$. Coarse grid: $dx=dz=20$ m, $dt=$ 8 ms, (2,4) staggered grid scheme. }
\plot{ptvzwindhh0}{width=15cm}{$v_z$ traces corresponding to the pressure traces of Figure \ref{fig:ptpwindh0}.}
\plot{srcvzghh0}{width=15cm}{Source reconstructed from $v_z$ traces in Figure \ref{fig:ptvzwindhh0} by time reversal (approximate inverse) algorithm explained in text.}
\plot{reptpwindhh0}{width=15cm}{Resimulated pressure data from reconstructed source if Figure \ref{fig:srcvzghh0}.} 
\plot{trcftrhh0}{width=15cm}{Middle trace from pressure, resimulated pressure traces in Figures \ref{fig:ptpwindh0}, \ref{fig:reptpwindhh0}.}.

\plot{bml0}{width=15cm}{Inhomogenous bulk modulus field, with low velocity lens.}
\plot{ptpwindl0}{width=15cm}{Receiver line at $z_r=1$ km for source at $z_s=3$ km, $x_s=3$ km. bulk modulus with low velocity lens as in Figure \ref{fig:bml0}, $\rho$=1 g/cm$^{3}$. Coarse grid: $dx=dz=20$ m, $dt=$ 8 ms, (2,4) staggered grid scheme. }
\plot{ptvzwindll0}{width=15cm}{$v_z$ traces corresponding to the pressure traces of Figure \ref{fig:ptpwindl0}.}

\plot{srcvzglh0}{width=15cm}{Source reconstructed from $v_z$ traces in Figure \ref{fig:ptvzwindll0} by time reversal (approximate inverse) algorithm explained in text, propagation in homogeneous material model (that is, ``wrong velocity'').}
\plot{reptpwindlh0}{width=15cm}{Resimulated pressure data from reconstructed source if Figure \ref{fig:srcvzglh0}, both approximate inversion and re-simulation in  homogeneous material model (``wrong velocity'').} 
\plot{trcftrlh0}{width=15cm}{Middle trace from pressure, resimulated pressure traces in Figures \ref{fig:ptpwindl0}, \ref{fig:reptpwindlh0}.}

\plot{srcvzgll0}{width=15cm}{Source reconstructed from $v_z$ traces in Figure \ref{fig:ptvzwindll0} by time reversal (approximate inverse) algorithm explained in text, propagation in bulk modulus of Figure \ref{fig:bml0} (that is, ``correct velocity'').}
\plot{reptpwindll0}{width=15cm}{Resimulated pressure data from reconstructed source if Figure \ref{fig:srcvzgll0}, both approximate inversion and re-simulation in bulk modulus of Figure \ref{fig:bml0} (``correct velocity'').} 
\plot{trcftrll0}{width=15cm}{Middle trace from pressure, resimulated pressure traces in Figures \ref{fig:ptpwindl0}, \ref{fig:reptpwindll0}.}



\bibliographystyle{seg}
\bibliography{../../bib/masterref}

\append{RWI (=Wang-Yingst WRI) - a Linear Algebra Viewpoint}

Define $S[c] = PL[c]^{-1}$ = operator mapping extended source to data traces.  

The Wang-Yingst version of the WRI objective function may be written
\[
\|S[c]g-d\|^2 + \alpha \|g-f\|^2
\]
``Most'' extended sources result in zero data traces - those are non-radiating sources, the null space of $S[c]$.

Rank Nullity Theorem: any $g$ may be written in a unique way as $g=S[c]^Th + n$, where $h$ is a collection of data traces, $S[c]^T$ is back propagation, and $n$ is in the null space of $S[c]$. Furthermore, the decomposition is orthogonal. Thus
\[
\|g-f\|^2 = \|S[c]^Th + n -f\|^2 = \|S[c]^Th - f_p\|^2 + \|n-f_n\|^2
\]
in which $f_p$ and $f_n$ are the orthoprojections on the orthocomplement of the null space, and the null space, of $S[c]$ respectively.

There is no use keeping $f_n$ around, as it radiates nothing - in fact, I suspect it is possible to see that point sources such as represented in \ref{eqn:ptsrc} are perpindicular to the null space already. In any case, simply choose $n=f_n$ and eliminate the second term, without affecting the data fit (by definition!).

Then WRI objective function can be re-written as
\begin{equation}
\label{eqn:wriobj}
\|S[c]S[c]^Th-d\|^2 + \alpha \|S[c]^Th -f_p\|^2
\end{equation}
and the extended model has been reduced from a full-space time volume ($g$) to a data gather ($h$).

{\bf Note added 2019.03.14:} Chao Wang's talk at GS 19 revealed that that they actually store the entire source ($g$ above). That is not so crazy, since the operator $S[c]S[c]^T$ poses the same complexity issue as time-domain RTM. 

To see this, need an explicit description of $S[c]^T$. Regard $L[c]$ as the 2nd ord wave operator with velocity $c$: $L[c]=\frac{1}{c^2}\frac{\partial}{\partial t^2} - \nabla^2$. $L[c]^{-1}$ is ambiguous: define 
\[
L[c]^{-1_+}g = u, \mbox{ for } L[c]u=g \mbox{ and } u=0 \mbox{ for } t<0
\]
that is, $L[c]^{-1}_+$ is the causal inverse of $L[c]$. Solutions defined on time interval $[0,T]$. Let $R$ be time-reversal: $Ru(t)=u(T-t)$. Then the anticausal inverse $L[c]^{-1}_-$ of $L[c]$, solution $=0$ for $t>T$, can be written as $L[c]^{-1}_- = RL[c]^{-1}_+R$.

Denote by $\langle \cdot,\cdot \rangle$ the $L^2$ inner product in space.. Suppose $u$ is a causal solution of $L[c]u=f$, $w$ an anti-causal solution of $L[c]w=g$. Then 
\[
\int_0^T \langle L[c]^{-1}_+f, g \rangle = int_0^T \langle u, g \rangle = \int_0^T \langle u, \left( \frac{1}{c^2}\frac{\partial}{\partial t^2} - \nabla^2\right)w\rangle
\]
\[
= \int_0^T \langle \left( \frac{1}{c^2}\frac{\partial}{\partial t^2} - \nabla^2\right)u, w \rangle = \int_0^T \langle f, w \rangle = \int_0^T \langle f, L[c]^{-1}_-g \rangle
\]
since the causal resp. anti-causal solutions have zero Cauchy data at $t=0$ resp. $t=T$, and since the integration is over all of $\bR^3$ and I am assuming everything vanishing at $\infty$. Conclude that
\[
(L[c]^{-1}_+)^T = L[c]^{-1}_-
\]
so
\[
S[c]^T = (PL[c]^{-1}_+)^T = L[c]^{-1}_-P^T
\]

For convenience, ignore sampling and aperture, and define $P$ to be the restriction operator on $z=0$. Then $(P^T h)(x,y,z,t) = h(x,y,t)\delta(z)$.

The troublesome operator in the definition \ref{eqn:wriobj} is 
\[
S[c]S[c]^T = PL[c]^{-1}_+L[c]^{-1}_-P^T
\]
The first solution operator involves evolution backwards in time, and provides the data for the second solution operator. This leadss to the same computational complexity problem as encountered in RTM. It could be handled by storing the output of $L[c]^{-1}_-$, as the IONGeo folks apparently do, or storing boundary data (for acoustics - as usual, this would not work if the approach is extended to a lossy model such as viscoacoustics), or using a checkpointing scheme.

Another approach: write $w = L[c]^{-1}_-P^T h$. The object is to compute $u=L[c]^{-1}_+w$, that is, the solution of 
\[
(c^{-2}\partial_t^2 - \nabla^2)u(t) = w(t), 0\le t \le T; \,u(0) = \partial_t u(0) = 0
\]
Duhamel's principle would provide a solution in the form of integrals over time of solutions of homogeneous initial value problems:
\[
u(t) = \int_0^t d\tau v(t,\tau), \, (c^{-2}\partial_t^2 - \nabla^2)v(t,\tau) = 0, 0\le t \le T; \,v(t,\tau)_{t=\tau} = 0, \partial_t v(t,\tau)_{t=\tau} = w(\tau)
\]
Note that $w$ solves the homogenous wave equation except at $z=0$. That makes it tempting to use $w$ directly to define $v$. In fact, set 
\[
W(t) = -int_t^T ds w(s),\,v_0(t,\tau) = \frac{1}{2}(W(t) - W(2\tau-t)) 
\]
Then $v_0$ satisfies the Duhamel initial conditions. Note that because $w$ has zero Cauchy data at $t=T$,
\[
\partial_t W(t)= -\int_t^T ds \partial_t w(s),\,\partial_t^2 W(t)=-\int_t^T ds \partial^2_t w(s)
\]
so 
\[
(c^{-2}\partial_t^2 - \nabla^2)v_0(t,\tau) = -\int_t^T(c^{-2}\partial^2_t -\nabla^2)w + \int_{2\tau-t}^T(c^{-2}\partial^2_t -\nabla^2)
\]
For $t>\tau$, $t > 2\tau-t$, so 
\[
= -\int_{2\tau-t}^t(c^{-2}\partial^2_t -\nabla^2)w = -\int_{2\tau-t}^t P^Th
\]
and 
\[ 
v_0(t,\tau) = \int_{2\tau-t}^t w  
\] 
Consequently $v=v_0 + v_1$, in which $v_1$ is defined by 
\[
(c^{-2}\partial_t^2 - \nabla^2)v_1(t,\tau) = \int_{2\tau-t}^t P^Th, \,  v_1(t,\tau)_{t=\tau} = 0, \,  \partial_t v_1(t,\tau)_{t=\tau} = 0
\]









\append{Transpose of Full Acoustic Modeling Operator}
In this section I compute the adjoint operator form modeling with both pressure and velocity source inputs and pressure and velocity trace outputs. The forward modeling operator is defined by solving
\begin{eqnarray}
\label{eqn:fawedata}
\frac{\partial p}{\partial t} & = & - \kappa( \nabla \cdot \bv +
h_s \delta(z-z_s)), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta ( \nabla p + l_s {\bf e}_z\delta(z-z_s)),\nonumber \\
p & =& 0 \mbox{ for } t<0\nonumber\\ 
\bv & = & 0 \mbox{ for } t<0 \nonumber\\
\end{eqnarray}
The new elements here are the point load time function $l_s(\bx,t;\bx_s)$, and the vertical unit vector ${\bf e}_z=(0,1)^T$ for 2D, $=(0,0,1)^T$ for 3D. The forward map ${\bf F}$ is defined as implicit in the discussion of the Intrinsically Preconditioned CG algorithm: let ${\bf h}_s=(h_s,l_s)^T$, then 
\begin{equation}
\label{eqn:ffwd}
{\bf F}{\bf h}_s =\left\{ \left[
\begin{array}{c}
p(\bx,t;\bx_s)\\
v_z(\bx,t;\bx_s)
\end{array}
\right]_{z=z_r}\right\}
\end{equation}

Let $\tilde{\bf h}_r=(\tilde{h}_r,\tilde{l}_r)^T$ be a function on $z=z_r$, 
and 
$\tilde{p},\tilde{\bv}$ solve the backwards-in-time boundary value problem:
\begin{eqnarray}
\label{eqn:faweadj}
\frac{\partial \tilde{p}}{\partial t} & = & \kappa (-\nabla \cdot \tilde{\bv} + 
 \tilde{h}_r \delta(z-z_r))\nonumber \\
\frac{\partial \tilde{\bv}}{\partial t} & = & \beta (\nabla \tilde{p} + \tilde{l}_r{\bf e}_z\delta(z-z_r),\nonumber \\
\tilde{p} & =& 0 \mbox{ for } t>T\nonumber\\ 
\tilde{\bv} & = & 0 \mbox{ for } t>T 
\end{eqnarray} 
Then
\[
0 = 
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  
\frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=T}
-
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=0}
\]
\[
= 
\int_{0}^{T} \,dt\, \frac{d}{dt}\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \frac{1}{\kappa} \frac{\partial p}{\partial t} \tilde {p} +  p \frac{1}{\kappa}\frac{\partial \tilde{p}}{\partial t} \right.
\]
\[
+
\left. \frac{1}{\beta} \frac{\partial \bv}{\partial t} \cdot \tilde{\bv} + \frac{1}{\beta} \bv \cdot \frac{\partial \tilde{\bv}}{\partial t} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 h_s \delta(z-z_s)\right) \tilde{p} + p \left(- \nabla \cdot \tilde{\bv} + 
 \tilde{h}_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  (- \nabla p + l_s{\bf e}_z\delta(z-z_s)) \cdot \tilde{\bv} + \bv \cdot (-\nabla \tilde{p} + \tilde{l}_r{\bf e}_z\delta(z-z_r)) \right)
\]
\[
= 
\int_{0}^{T}\,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv \tilde{p} + 
 h_s\tilde{p}|_{z=z_s} - p\nabla \cdot \tilde{\bv} + 
 \tilde{h}_r p|_{z=z_r}\right) \right.
\]
\[
+
\left.  p (\nabla \cdot \tilde{\bv}) + l_s \tilde{v}_z|_{z=z_r}+ (\nabla \cdot \bv) \tilde{p}  + l_r  v_z|_{z=z_r}\right)
\]
after integration by parts in the last two terms. Most of what is left cancels, leaving 
\[
0 = \int\,dt\,dx\,dy\, (h_s \tilde{p})_{z=z_s} + (p  \tilde{h}_r)_{z=z_r} + (l_s\tilde{v}_z)_{z=z_s} + (v_z \tilde{l}_r)_{z=z_r},
\]
whence
\begin{equation}
\label{eqn:fadj}
 {\bf F}^T {\bf h}_r = -\left[
\begin{array}{c}
\tilde{p}(\bx,t;\bx_s)\\
\tilde{\bf v}(\bx,t;\bx_s)
\end{array}
\right]_{z=z_s}
\end{equation}

%\begin{cor}\label{thm:bvps}
%Suppose that $d_s$ is a sufficiently regular function, = 0 for $t<0$, and denote by $p^{\pm},\bv^{\pm}$ the half-space solutions provided by Theorem \ref{thm:bvp}. Define the vector fields $p,\bv$ in $\bR^4$ by
%\begin{eqnarray}
%\label{eqn:putemtogether} 
%p(x,z) &=& p^{\pm}(x,z) \mbox{ for } \pm(z-z_s) > 0\\
%p(x,z_s)&=&d_s(x)\\
%\bv(x,z) &=& \bv^{\pm}(x,z) \mbox{ for } \pm(z-z_s) > 0\\
%\end{eqnarray}
%Then $p,\bv$ is a weak solution of the system \ref{eqn:awedata} with $h=[v]_n$.
%\end{cor}

Proof of Theorem \ref{thm:bvp}:

\begin{proof}
Since $\omega \cap \mbox{ supp }d$ is compact, it is possible to construct an extension of 
$d \in C^{\infty}_{0}(\bR^{d+1})$. Define $p_1^{\pm}, \bv_1^{\pm}$ to be strong solutions of the system
\begin{eqnarray}
\label{eqn:ichom}
\frac{\partial p^{\pm}_1}{\partial t} & = & - \kappa \nabla \cdot \bv^{\pm}_1 + F\mbox{ in } \Omega^{\pm}, \nonumber \\
\frac{\partial \bv^{\pm}_1}{\partial t} & = & - \beta \nabla p^{\pm}_1 + {\bf G}\mbox{ in } \Omega^{\pm},\nonumber \\
p^{\pm}_1 & = & 0 \mbox{ on } \omega \times \bR,\\
p^{\pm}_1 & = & 0, t \ll 0\\
\bv^{\pm}_1 & = & 0, t \ll 0 
\end{eqnarray}
with 
\[
F = -\gamma\frac{\partial d}{\partial t}, {\bf G} = -\nabla d.
\]
Existence, uniqueness, and regularity of these solutions are assured by a minor extension of results of \cite{BlazekStolkSymes:13}.
Set $p^{\pm} = p_1^{\pm} + d, \bv^{\pm} = \bv_1^{\pm}$; then $p^{\pm},\bv^{\pm}$ are the unique strong solutions of the boundary value problems \ref{eqn:awebdry}, and are smooth up to the boundary as indicated. 
\end{proof}

Proof of Corollary \ref{thm:bvps}:

\begin{proof}: Choose smooth functions $\phi \in C^{\infty}_0(\bR^{d+1}), {\bf \psi} \in  (C^{\infty}_0(\bR^{d+1}))^d$. Then
\[
\int_{\Omega^{\pm} \times \bR}\left (\gamma \frac{\partial \phi}{\partial t} +
\nabla \cdot {\bf \psi}\right)p^{\pm} + \left(\rho\frac{\partial {\bf
    \psi}}{\partial t}
  + \nabla \phi\right) \cdot \bv^{\pm}
\]
\[
= - \int_{\Omega \times \bR}\left (\gamma \frac{\partial p^{\pm}}{\partial t} +
\nabla \cdot \bv^{\pm}\right) \phi + \left(\rho\frac{\partial \bv^{\pm}}{\partial t}
  + \nabla p^{\pm}\right) \cdot {\bf \psi}
\]
\begin{equation}
\label{eqn:rhs1}
\pm \int_{\omega \times \bR} \bn \cdot {\bf \psi} p^{\pm} + \bn
  \cdot \bv^{\pm} \phi
\end{equation}
Adding equations \ref{eqn:rhs1} for the two choices of sign and taking into
account that $p^{\pm}$ coincide (by construction) on $\omega \times \bR$, obtain
\[
\int_{(\bR^d \times \bR}\left (\gamma \frac{\partial \phi}{\partial t} +
\nabla \cdot {\bf \psi}\right)p + \left(\rho\frac{\partial {\bf
    \psi}}{\partial t}
  + \nabla \phi\right) \cdot \bv
\]
\[
= \int_{\partial \Omega \times [0,T]} (\bn \cdot \bv^{+} -  \bn
  \cdot \bv^{-}) \phi
\]
Since $\phi$ and ${\bf \psi}$ are arbitrary, other than being smooth of compact support, this observation finishes the proof.
\end{proof}

\append{Variable Projection}

\[
J[c]=\min_h\frac{1}{2}(\|S[c]h-d\|^2 + \alpha^2\|Ah\|^2)
\]
\[
= \min_h \frac{1}{2}\|F[c]h-e\|^2
\]
\[
F[c]=\left(
\begin{array}{c}
S[c]\\
\alpha A
\end{array}
\right),\,\,
e=\left(
\begin{array}{c}
d\\
0
\end{array}
\right)
\]

\[
J[c]=\frac{1}{2}\|(F[c]N[c]^{-1}F[c]^T-I)e\|^2,\,\,N[c]=F[c]^TF[c]
\]
\[
DJ[c]\delta c = \langle (F[c]N[c]^{-1}F[c]^T-I)e, 
(DF[c]\delta c)N[c]^{-1}F[c]^Te +
F[c](-N[c]^{-1}(DN[c]\delta c)N[c]^{-1})F[c]^Te +
F[c]N[c]^{-1}(DF[c]\delta c)^Te \rangle 
\]

\[
= \langle (F[c]N[c]^{-1}F[c]^T-I)e, 
(DF[c]\delta c)N[c]^{-1}F[c]^Te -
F[c]N[c]^{-1}(DF[c]\delta c)^T F[c]N[c]^{-1})F[c]^Te -
F[c]N[c]^{-1}F[c]^T (DF[c]\delta c]N[c]^{-1})F[c]^Te +
F[c]N[c]^{-1}(DF[c]\delta c)^Te \rangle 
\]
\[
= \langle (F[c]N[c]^{-1}F[c]^T-I)e, 
(I-F[c]N[c]^{-1}F[c]^T) DF[c]\delta c)N[c]^{-1}F[c]^Te
F[c]N[c]^{-1}(DF[c]\delta c)^T(I-F[c]N[c]^{-1}F[c]^T) \rangle
\]
\[
=-\langle (I-F[c]N[c]^{-1}F[c]^T)^2e, DF[c]\delta c)N[c]^{-1}F[c]^Te\rangle
-\langle e,(I-F[c]N[c]^{-1}F[c]^T)F[c]N[c]^{-1}(DF[c]\delta c)^T (I-F[c]N[c]^{-1}F[c]^T)e
\]
\[
F[c]N[c]^{-1}F[c]^TF[c]N[c]^{-1}=F[c]N[c]^{-1}
\]
so 
\[
(I-F[c]N[c]^{-1}F[c]^T)F[c]N[c]^{-1} = 0
\]
and
\[
(I-F[c]N[c]^{-1}F[c]^T)^2 = (I-F[c]N[c]^{-1}F[c]^T)
\]
Whence
\[
DJ[c]\delta c =
-\langle (I-F[c]N[c]^{-1}F[c]^T)e, (DF[c]\delta c)N[c]^{-1}F[c]^Te\rangle
\]
Define $h[c]=N[c]^{-1}F[c]^Te$ to be the solution of the inner problem as function of $c$. Then
\[
DJ[c]\delta c = \langle F[c]h[c]-e,(DF[c]\delta c)h[c] \rangle
\]
which is the usual VPM gradient formula.

Gauss-Newton: define $G[c]=F[c]N[c]^{-1}F[c]^Te$. Need $DG[c], DG[c]^T$.
\[
DG[c]\delta c =( DF[c]\delta c)N[c]^{-1}F[c]^Te 
\]
\[
- F[c]N[c]^{-1}(DF[c]\delta c)^TF[c]N[c]^{-1}F[c]^Te + F[c]N[c]^{-1} F[c]^T(DF[c]\delta c))N[c]^{-1}F[c]^Te 
\]
\[
+ F[c]N[c]^{-1}(DF[c]\delta c)^Te
\]
\[
=(I-F[c]N[c]^{-1} F[c]^T)(DF[c]\delta c))N[c]^{-1}F[c]^Te + F[c]N[c]^{-1}(DF[c]\delta c)^T(I-F[c]N[c]^{-1} F[c]^T) e
\]
Reinterpretation: set
\begin{eqnarray}
h[c] &=&N[c]^{-1}F[c]^Te \nonumber\\
r[c] &= &F[c]h[c]-e \nonumber\\
E_1[c]\delta c & = & (DF[c]\delta c)h[c] \nonumber\\
H_1[c]\delta c &=&N[c]^{-1}F[c]^T E_1[c]\delta c \nonumber\\
R_1[c]\delta c &=& (F[c]H_1[c]-E_1[c])\delta c \nonumber\\
H_2[c]\delta c &=&N[c]^{-1}(DF[c]\delta c)^Tr[c] \nonumber\\
R_2[c]\delta c &=&F[c]H_2[c]\delta c
\end{eqnarray}
Then
\[
DG[c]\delta c = -(R_1[c]+R_2[c])\delta c
\]
\[
= (I-F[c]N[c]^{-1}F[c]^T)DF[c]\delta c)h[c] -F[c]N[c]^{-1}(DF[c]\delta c)^Tr[c]
\]
Adjoints: write $(DF[c]\delta c)h = DF[c](\delta c,h)$, 
\begin{eqnarray}
\langle F[c]h,\phi \rangle &=& \langle h,F[c]^T\phi\rangle \nonumber\\
\langle DF[c](\delta c,h),\phi \rangle &=& \langle \delta c, DF[c]^*(\phi,h) \rangle
\end{eqnarray}
\[
\langle DF[c](\delta c,h),\phi \rangle = \langle \delta c,DF[c]^*(\phi,h)\rangle
\]
\[
\langle E_1[c]\delta c,\phi \rangle = \langle DF[c](\delta c,h[c]),\phi \rangle
\]
\[
= \langle \delta c, DF[c]^*(\phi,h[c]) \rangle
\]
so $E_1[c]^* = DF[c]^*(\cdot,h[c])$, and
\[
\langle F[c]H_1[c]\delta c, \phi \rangle = \langle F[c] N[c]^{-1}F[c]^TE_1[c]\delta c,\phi \rangle
\]
\[
=\langle E_1[c]\delta c,F[c]N[c]^{-1}F[c]^T\phi\rangle
\]
\[
=\langle \delta c, DF[c]^*(F[c]N[c]^{-1}F[c]^T\phi,h[c])\rangle
\]
\[
\langle R_1[c]\delta c,\phi \rangle = \langle \delta c, DF[c]^*((F[c]N[c]^{-1}F[c]^T-I)\phi,h[c])
\]
That is,
\[
R_1[c]^*\phi = DF[c]^*((F[c]N[c]^{-1}F[c]^T-I)\phi,h[c])
\]
\[
\langle R_2[c]\delta c,\phi \rangle = \langle F[c]N[c]^{-1}(DF[c]\delta c)^Tr[c],\phi \rangle
\]
\[
\langle r[c],DF[c](\delta c,N[c]^{-1}F[c]^T\phi)\rangle = \langle \delta c, DF[c]^*(r[c], N[c]^{-1}F[c]^T\phi) \rangle
\]
That is,
\[
R_2[c]^*\phi = DF[c]^*(r[c], N[c]^{-1}F[c]^T\phi)
\]

\end{document}
\section{Figures for talk}
\plot{pulse00}{width=\textwidth}{Bandpass filter pulse}
\plot{fwitrbulkmodinvlh0}{width=\textwidth}{Inversion for bulk modulus of data displayed in Figure \ref{fig:ptpwindl0}. Initial model: constant bulk modulus = 4.0 GPa. Algorithm: 20 iterations of trust region Krylov-Gauss-Newton algorithm with preconditioning by box smoother of radius 5 points, repeated 10 times.  Each KGN step computed by up to 20 Conjugate Gradient iterations.} 
\plot{fwitrdatapreslh0}{width=\textwidth}{Data residual produced by 20 itersions of KGN FWI for bulk modulus, starting at constant bulk modulus. Residual RMS approximately 26\% of data RMS.}
\plot{cgwhestsourceplh0}{width=\textwidth}{Extended inversion for source on 1500 m $\le x \le$ 5500 m, $z$=3000 m, data from Figure \ref{fig:ptpwindl0}, homogeneous bulk modulus = 4.0 GPA.}
\plot{cgwhresdataplh0}{width=\textwidth}{Residual data from extended source inversion shown in Figure \ref{fig:cgwhestsourceplh0}. RMS is 3\% of data RMS.}
\plot{bmh0s}{width=\textwidth}{Iteration 1}
\plot{cgw1bulkupdlh0}{width=\textwidth}{Iteration 1 SSE bulkmod estimate}
\plot{cgw2bulkupdlh0}{width=\textwidth}{Iteration 2 SSE bulkmod estimate}
\plot{cgw3bulkupdlh0}{width=\textwidth}{Iteration 3 SSE bulkmod estimate}
\plot{cgw4bulkupdlh0}{width=\textwidth}{Iteration 4 SSE bulkmod estimate}
\plot{cgw5bulkupdlh0}{width=\textwidth}{Iteration 5 SSE bulkmod estimate}
\plot{cgw6bulkupdlh0}{width=\textwidth}{Iteration 6 SSE bulkmod estimate}
\plot{cgw7bulkupdlh0}{width=\textwidth}{Iteration 7 SSE bulkmod estimate}
\plot{cgw8bulkupdlh0}{width=\textwidth}{Iteration 8 SSE bulkmod estimate}
\plot{cgw9bulkupdlh0}{width=\textwidth}{Iteration 9 SSE bulkmod estimate}
\plot{cgw10bulkupdlh0}{width=\textwidth}{Iteration 10 SSE bulkmod estimate}
\plot{cgw11bulkupdlh0}{width=\textwidth}{Iteration 11 SSE bulkmod estimate}
\plot{cgw12bulkupdlh0}{width=\textwidth}{Iteration 12 SSE bulkmod estimate}
\plot{cgw13bulkupdlh0}{width=\textwidth}{Iteration 13 SSE bulkmod estimate}
\plot{cgw14bulkupdlh0}{width=\textwidth}{Iteration 14 SSE bulkmod estimate}
\plot{cgw15bulkupdlh0}{width=\textwidth}{Iteration 15 SSE bulkmod estimate}
\plot{cgw22bulkupdlh0}{width=\textwidth}{Iteration 22 SSE bulkmod estimate}
\plot{cgw30bulkupdlh0}{width=\textwidth}{Iteration 30 SSE bulkmod estimate}
\plot{fwitrsse1bulkmodinvlh0}{width=\textwidth}{Inversion for bulk modulus of data displayed in Figure \ref{fig:ptpwindl0}. Initial model: result of 15 steepest descent steps of SSE, Figure \ref{fig:cgw15bulkupdlh0}. Algorithm: 20 iterations of trust region Krylov-Gauss-Newton algorithm with preconditioning by box smoother of radius 5 points, repeated 10 times.  Each KGN step computed by up to 20 Conjugate Gradient iterations.} 
\plot{fwitrsse1datapreslh0}{width=\textwidth}{Data residual produced by 20 itersions of KGN FWI for bulk modulus, starting at output of 15 steepest descent steps of SSE, Figure \ref{fig:cgw15bulkupdlh0}. Residual RMS approximately 1.2\% of data RMS.}
\plot{cgw0estsourceplh0}{width=\textwidth}{Iteration 1 SSE source estimate alpha=4.36 $\times 10^{-6}$}
\plot{cgw1estsourceplh0}{width=\textwidth}{Iteration 2 SSE source estimate alpha=5.93 $\times 10^{-6}$}
\plot{cgw2estsourceplh0}{width=\textwidth}{Iteration 3 SSE source estimate alpha=6.82 $\times 10^{-6}$}
\plot{cgw3estsourceplh0}{width=\textwidth}{Iteration 4 SSE source estimat alpha=7.93 $\times 10^{-6}$e}
\plot{cgw4estsourceplh0}{width=\textwidth}{Iteration 5 SSE source estimate alpha=9.29 $\times 10^{-6}$}
\plot{cgw5estsourceplh0}{width=\textwidth}{Iteration 6 SSE source estimate alpha=1.1 $\times 10^{-5}$}
\plot{cgw6estsourceplh0}{width=\textwidth}{Iteration 7 SSE source estimate alpha=1.32 $\times 10^{-5}$}
\plot{cgw7estsourceplh0}{width=\textwidth}{Iteration 8 SSE source estimate alpha=1.32 $\times 10^{-5}$}
\plot{cgw8estsourceplh0}{width=\textwidth}{Iteration 9 SSE source estimate alpha=1.72 $\times 10^{-5}$}
\plot{cgw9estsourceplh0}{width=\textwidth}{Iteration 10 SSE source estimate alpha=2.2 $\times 10^{-5}$}
\plot{cgw10estsourceplh0}{width=\textwidth}{Iteration 11 SSE source estimate alpha=2.79 $\times 10^{-5}$}
\plot{cgw11estsourceplh0}{width=\textwidth}{Iteration 12 SSE source estimate alpha=2.79 $\times 10^{-5}$}
\plot{cgw12estsourceplh0}{width=\textwidth}{Iteration 13 SSE source estimate alpha=2.79 $\times 10^{-5}$}
\plot{cgw13estsourceplh0}{width=\textwidth}{Iteration 14 SSE source estimate alpha=2.79 $\times 10^{-5}$}
\plot{cgw14estsourceplh0}{width=\textwidth}{Iteration 15 SSE source estimate alpha=2.79 $\times 10^{-5}$}
\plot{cgw21estsourceplh0}{width=\textwidth}{Iteration 22 SSE source estimate alpha=5.73 $\times 10^{-5}$}
\plot{cgw29estsourceplh0}{width=\textwidth}{Iteration 30 SSE source estimate alpha=9.1 $\times 10^{-5}$}
\plot{cgw0resdataplh0}{width=\textwidth}{Iteration 1 SSE data residual}
\plot{cgw1resdataplh0}{width=\textwidth}{Iteration 2 SSE data residual}
\plot{cgw2resdataplh0}{width=\textwidth}{Iteration 3 SSE data residual}
\plot{cgw3resdataplh0}{width=\textwidth}{Iteration 4 SSE data residual}
\plot{cgw4resdataplh0}{width=\textwidth}{Iteration 5 SSE data residual}
\plot{cgw5resdataplh0}{width=\textwidth}{Iteration 6 SSE data residual}
\plot{cgw6resdataplh0}{width=\textwidth}{Iteration 7 SSE data residual}
\plot{cgw7resdataplh0}{width=\textwidth}{Iteration 8 SSE data residual}
\plot{cgw8resdataplh0}{width=\textwidth}{Iteration 9 SSE data residual}
\plot{cgw9resdataplh0}{width=\textwidth}{Iteration 10 SSE data residual}
\plot{cgw10resdataplh0}{width=\textwidth}{Iteration 11 SSE data residual}
\plot{cgw11resdataplh0}{width=\textwidth}{Iteration 12 SSE data residual}
\plot{cgw12resdataplh0}{width=\textwidth}{Iteration 13 SSE data residual}
\plot{cgw13resdataplh0}{width=\textwidth}{Iteration 14 SSE data residual}
\plot{cgw14resdataplh0}{width=\textwidth}{Iteration 15 SSE data residual}
