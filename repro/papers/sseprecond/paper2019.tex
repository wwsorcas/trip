\title{Efficient Computation of Extended Sources}
\author{William. W. Symes \thanks{The Rice Inversion Project,
Department of Computational and Applied Mathematics, Rice University,
Houston TX 77251-1892 USA, email {\tt symes@caam.rice.edu}.}}

\lefthead{Symes}

\righthead{Approximate Source Inversion}

\maketitle
\begin{abstract}
Source extension is a reformulation of inverse problems in wave propagation, that at least in some cases leads to computationally tractable iterative solution methods. The core subproblem in all source extension methods is the solution of a linear inverse problem for a source (right hand side in a system of wave equations) through minimization of data error in the least squares sense with soft imposition of physical constraints on the source via an additive quadratic penalty. A variant of the time reversal method from photoacoustic tomography provides an approximate solution that can be used to precondition Krylov space iteration for rapid convergence to the solution of this subproblem. An acoustic 2D example for sources supported on a surface, with a soft contraint enforcing point support, illustrates the effectiveness of this preconditioner.
\end{abstract}

\section{Introduction}
Full Waveform Inversion (FWI) can be described in terms of 
\begin{enumerate}
\item a linear wave operator $L[c]$, depending on an array of space-dependent coefficients $c$ and acting on wavefields $u$ vanishing in negative time:
\begin{equation}
\label{eqn:init}
u \equiv 0, t \ll 0; 
\end{equation}
\item a trace sampling operator $P$ acting on wavefields and producing data traces;
\item and a source function (of space and time) $f$ representing energy input to the system. 
\end{enumerate}
The basic FWI problem is: given data $d$, find $c$ so that 
\begin{equation}
\label{eqn:fwi}
Pu \approx d \mbox{ and } L[c]u = f.
\end{equation}

The energy source $f$ may also be largely undetermined, apart from some known characteristics such as localization in space and/or time. In fact, additional source degrees of freedom, beyond those needed to describe physically realized sources, may be useful in rendering the FWI problem \ref{eqn:fwi} more amenable to numerical solution, via so-called extended modeling (see \cite{geoprosp:2008}, \cite{LeeuwenHerrmannWRI:13}, \cite{HuangNammourSymesDollizal:SEG19}, and many references cited there). Therefore it is natural to view $f$ as also an unknown in formulating the problem \ref{eqn:fwi} via nonlinear least squares:
%A typical nonlinear least squares formulation is:
%\begin{equation}
%\label{eqn:ols}
%\mbox{choose } c \mbox{ to minimize } \|PL[c]^{-1}f -d \|^2.
%\end{equation}

%As is well-known, local optimization methods are the only feasible approach given the dimensions of a typical instance of \ref{eqn:fwi}, and those have a tendency to stall due to ``cycle-skipping''. Source extension is one approach to avoiding this problem. It consists in imposing the wave equation as a soft as opposed to hard constraint, by allowing the source to have more degrees of freedom than is permitted by a faithful  model model of the seismic experiment, and constraining these additional degrees of freedom by means of an additive penalty modifying the probem \ref{eqn:ols}:
\begin{equation}
\label{eqn:esi}
\mbox{choose } c, f \mbox{ to minimize } \|PL[c]^{-1}f -d \|^2 + \alpha^2 \|Af\|^2 
\end{equation}
The operator $A$ penalizes deviation from known (or assumed) characteristics of the source function - its null space consists of feasible source models. 

This paper presents a numerically efficient approach to solving the {\em source subproblem}
\begin{equation}
\label{eqn:esis}
\mbox{given } c, \mbox{ choose } f \mbox{ to minimize } \|PL[c]^{-1}f -d \|^2 + \alpha \|Af\|^2 
\end{equation}

The penalty operator $A$ is assumed linear in this work, so the source subproblem is a linear least squares problem. Under some additional assumptions to be described below, I shall show how to construct an accurate approximate solution operator for problem \ref{eqn:esis}. This approximate solution operator may be used to accelerate Krylov space methods for the solution of the source subproblem \ref{eqn:esis}. Numerical examples suggest the effectiveness of this acceleration.

The source subproblem is a necessary step in solving the overall FWI problem \ref{eqn:fwi}. In particular, the nonlinear least squares realization \ref{eqn:esi} of source-extended FWI is naturally approached via the {\em variable projection method} \cite[]{GolubPereyra:73,GolubPereyra:03,vanLeeuwenMulder:09,Rickett:SEG12}, in which the minimum over $f$ in \ref{eqn:esis} is treated as a function of $c$, which is in turn minimized. Variable projection thus treats the source subproblem as an inner problem, which must be solved for an estimate of $f$ at every step of an iterative method for the outer problem of minimization over $c$. Efficiency in solving the source subproblem \ref{eqn:esis} is critical to efficient implementation of variable projection for solving the regularized FWI problem \ref{eqn:esi}.

This paper restricts the choice of wave physics embodied in $L[c]$ to linear acoustics.  The principal characteristics assumed of sources are localization and isotropy. That is, physical sources will be assumed to be isotropic point radiators, described by the functional form
\begin{equation}
\label{eqn:ptsrc}
f_{\rm pt}(\bx,t;\bx_s) = w(t)\delta(\bx-\bx_s). 
\end{equation}
Source locations $\bx_s$ are assumed known (as is natural for active source methods), and the source {\em wavelet} $w(t)$ is to be found as part of the solution of \ref{eqn:esi}. A convenient choice of penalty operator $A$ is then multiplication by the {\em offset} $|\bx-\bx_s|$: its null space consists precisely of distributions of the form \ref{eqn:ptsrc}. 

More complex radiation characteristics may be accommodated with similar but more complex choice for the operator $A$. Since the core analysis is similar, this paper deals only with the simplest case \ref{eqn:ptsrc}.

The wavefield $u$ in this work is presumed to exist in all of Euclicean space-time: in other words, boundary effects have been removed from $u$, by surface-related multiple elimination or other techniques. Predicted data traces are simply the samples of one or more components of $u$ at receiver points $\bx_r$.

%The domain of wave propagation is denoted $\Omega$, and two subsets of its boundary are identified: $\Gamma_s$ contains the support (non-zeroes) of the (extended) sources and in particular the locations $\bx_s$ of physical sources. Data trace locations $\bx_r$ lie in another subset $\Gamma_r$. The data projection operator $P$ samples the acoustic field $u$ on $\Gamma_r$, and includes a finite aperture mute.  

Achieving a small residual in the source subproblem \ref{eqn:esis} for arbitrary $c$ is a critical part of the convergence mechanism for the extended source approach to FWI \cite[]{HuangNammourSymesDollizal:SEG19}. However, for arbitrary data $d$, coefficient array $c$, and source location $\bx_s$, there does not exist an isotropic point source $f_{\rm pt}$ of the form \ref{eqn:ptsrc} for which $PL[c]^{-1}f_{\rm pt} \approx d$. This is so even if there does exist a coefficient array $c^*$ and point source $f_{\rm pt}^*$ located at $\bx_s$ with wavelet $w^*(t)$ for which $d = PL[c^*]^{-1}f_{\rm pt}^*$, if $c$ differs substantially from $c^*$. It is this fact that underlies the effectiveness of the extended source approach to FWI:  driving the residual in the problem \ref{eqn:esi} towards zero necessarily requires modifying $c$ to resemble $c^*$, assuming the latter exists, as well as pushing the optimal $f$ towards the null space of $A$. Conversely, the source subproblem \ref{eqn:esis} does not have a small residual solution of point source form \ref{eqn:ptsrc}, in general, for a given choice of $c$, even if that were the case for a different choice ($c^*$). Therefore a more general class of source models than that specified by the condition \ref{eqn:ptsrc} must be admitted to the feasible set for the source subproblem \ref{eqn:esis} if it is to serve as the inner problem in a variable projection formulation of extended source FWI.

The class of non-point sources investigated here are those confined a hypersurface assumed to contain the locations of physical (point) sources, but not a priori required to have point support. These {\em surface extended sources} are able to yield a relatively small residual in the source subproblem \ref{eqn:esis} for more or less arbitrary coefficient array $c$. This observation is a by-product of the approximate solution for \ref{eqn:esis} constructed below. The construction of the approximate solution is closely related to the {\em time reversal} method of photoacoustic tomography (see \cite{StefanovUhlmannIP:09} and references cited there).

The next section gives a precise description of an acoustic version of problem \ref{eqn:esis}, and points out two interesting special cases, the {\em crosswell} and {\em diving wave} configurations. The assumptions detailed here imply that the data $d$ of the inverse problems \ref{eqn:esi}, \ref{eqn:esis} have the physical character of {\em transmitted waves}. There are no mathematical results at present concerning reflected wave inverse problems treated by source extension methods, though there are tantalizing numerical clues \cite[]{LeeuwenHerrmannWRI:13,Warner:14,Warner:16,LeeuwenHerrmann:16,HuangSymes:Geo17,HuangSymes:Geo18a,HuangSymes:Geo18b}.

The following section describes the construction of the approximate inverse and explains the conditions under which accuracy should be expected. I observe that the approximate inverse is approximately the adjoint of the modeling operator $PL[c]^{-1}$ with respect to weighted norms in its domain and range (see \cite{HouSymes:15} for a similar observation in a different context). This fact immediately suggests use of the approximate inverse as a preconditioner to accelerate convergence of Krylov space methods such as conjugate gradient iteration applied to the least squares problem \ref{eqn:esis}. However, its straightforward application in the preconditioned conjugate gradient (PCG) method \cite[]{Golub:2012} involves explicitly a version the so called Dirichlet-to-Neumann (D2N) operator, which is computationally awkward. I show how to reorganize the PCG iteration so that only solutions of $L[c]u=f$ are required. The penultimate section presents some 2D examples in the crosswell configuration, illustrating the accuracy of the approximate inversion and the accelerated convergence of PCG. The examples are chosen to emphasize that surface extended sources can be constructed to fit more or less arbitrary data, unlike point sources - as mentioned before, this approximate invertibility property of the modeling operator is of critical importance in the application to extended FWI. The paper ends with a discussion of several important matters not addressed here, and a restatement of the conclusions.

The intent of the main body of this paper is to present the formal structure of the extended source subproblem and the way in which this structure leads to accelerated numerical solution. This formal structure is however supported by a rigorous foundation. In order to avoid disrupting the formal account, this foundation material is relegated to an appendix.

%The penalty operator $A$ is chosen to penalize non-physical aspects of the trial source $g$: physical sources are supposed to constitute the null space of $A$.

%In some cases, the resulting  relaxed data fit problem has been observed to be more amenable to solution by local optimization. The main goal of this paper is explain why this should be so,  in a particular instance of source-extended FWI. I will show that for a specific choice of {\em annihilator} $A$, the optimization problem \ref{eqn:esi} possesses a {\em qualitatively} larger domain of convexity for consistent data ($PL[c]^{-1}g = d, Ag=0$) than does the FWI problem \ref{eqn:ols}, while possessing the same solution. 

%For sake of argument (and because almost all work so far has assumed it), adopt the isotropic point source as the model: for the data gather at source positon $\bx_s$, the source takes the form
%\begin{equation}
%\label{eqn:ptsrc}
%f(\bx,t;\bx_s) = w(t)\delta(\bx-\bx_s). 
%\end{equation}
%in which the {\em wavelet} $w(t)$ has whatever structure is necessary for $f$ to lie in the range of the wave operator $L[c]$. FWI can also accommodate more complex source models reflecting the nontrivial radiation patterns of actual sources. 
%The data projection operator $P$ involves the restriction of one or more components of the field $u$ to a hypersurface $X_r$. The simplest interesting choice of $X_r$ is a hyperplane, say $z=z_r$ (with the convention that $\bx=(x,y,z)$). To emulate the limited aperture of any experimental data system, include multiplication by a smooth cut-off or taper function $\phi$. Component selection is a special case of multiplication by a matrix $P_c \in \bR^{n_d \times n_s}$, in which $n_d$ is the number of data components and $n_s$ is the dimension of the state vector $u$.
%\begin{equation}
%\label{eqn:data}
%Pu(x,y,t) = \phi(x,y,z_r)P_cu(x,y,z_r,t)
%\end{equation}
%in which $P_c$ is a 


%Source extension replaces $f(\bx,t;\bx_s)$ with an artificial source $g(\bx,t; \bx_s)$ with more degrees of freedom than merely a single common source wavelet $w(t)$ as in definition \ref{eqn:ptsrc}. Assuming that \ref{eqn:ptsrc} is actually correct physics, these extra degrees of freedom have to be suppressed in the optimal solution, via an {\em annihilator} $A$, an operator acting on extended sources $g$, whose null space is precisely the physical sources (of the form \ref{eqn:ptsrc}). Extended source inversion replaces the FWI objective with a penalty function

%The way in which degrees of freedom are introduced, and the choice of annihilator, distinguish the various source extension methods, of which there are many - see \cite{HuangNammourSymesDollizal:SEG19}, who introduce the {\em surface source} extension. It is this surface source extension that is the topic of this paper. It is similar  to the volume extension \cite[]{HuangSymes:Geo18b}, but the source is spread over the source space-time surface, rather than over space at $t=0$.
%Similar to the space-time and volume extensions \cite[]{HuangSymes:Geo18a,HuangSymes:Geo18b}, the annihilator is given by the {\em localization penalty}:
%\begin{equation}
%\label{eqn:msste}
%A[g](\bx,t;\bx_s) = |\bx-\bx_s|g(\bx,t;\bx_s). 
%\end{equation} 
%Therefore an estimate of the the source wavelet is a by-product of the inversion, and does not need to be specified {\em a priori}. Like the volume extension (and unlike, for example, Wavefield Reconstruction Inversion \cite[]{LeeuwenHerrmannWRI:13,LeeuwenHerrmann:16}), the additional data volume required is on the same order of size as the data.


%The volume and surface extensions have a well-defined relation to tomographic principles (slope or traveltime, depending on type of annihilator). This relation holds regardless of ray geometry (with or without triplication) in transmission configuration.

%Again, nothing is known theoretically about the behaviour of any of these methods for reflection data, and numerical evidence is ambiguous.

\section{Extended Source Modeling}
For acoustic wave physics, the coefficient vector $c$ is the pair $c=(\kappa,\beta)$ of bulk modulus $\kappa$ and buoyancy (reciprocal density) $\beta$, and the state vector $u=(p,\bv)$ consists of pressure $p$ (a scalar space-time field) and particle velocity $\bv$ (a vector space-time field). The wave operator $L[c]$ is given by the linear acoustics system:
\begin{equation}
\label{eqn:awe}
L[c]u = 
\left(
\begin{array}{c}
\partial_t p  + \kappa \nabla \cdot \bv \\
\partial_t \bv + \beta \nabla p
\end{array}
\right) 
\end{equation}

The space dimension will be denoted by $d$. Most of what follows is valid for any $d >0$. The choice $d=2$ is computationally convenient, if non-physical. 

As mentioned earlier, in this paper the coefficients $c$ is regarded as defined throughout space $\bR^d$, the state vector $u$ throughout space-time $\bR^{d+1}$. 

Real-world sampling is discrete and direction-sensitive. This paper ignores such issues and presumes that the sampling operator $P$ simply restricts its argument to a cylindrical space-time hypersurface $\Gamma_r \times \bR$.  Here $\Gamma_r \subset \bR^d$ is an open subset of a smooth codimension 1 oriented hypersurface. To distinguish this output or receiver sampling operator from another similar operator, add the subscript $r$ to the notation, that is, write $P_r$ rather than $P$. For technical reasons (and to emulate actual acquisition practice) include multiplication by a cutoff or mute $\phi_r \in C_0^{\infty}(\bR^{d+1})$, and extract only samples of the pressure field:
\begin{equation}
\label{eqn:recp}
(P_ru)(\bx,t) = \phi_r(\bx,t)p(\bx,t),\,\bx \in \Gamma_r, t \in \bR. 
\end{equation}
A similar sampling operator for the velocity field will also be required below. Denote by ${\bf n}_r$ the outward unit normal field on $\Gamma_r$, and define
\begin{equation}
\label{eqn:recv}
(V_ru)(\bx,t) = \phi_r(\bx,t){\bf n}_r(\bx)\cdot \bv(\bx,t),\,\bx \in \Gamma_r, t \in \bR.
\end{equation}

Physical sources are modeled as an isotropic constitutive law defects of point support. As mentioned earlier, it is simple to accommodate more complex source models. Assume that all source lie in an open subset $\Gamma_s \subset \bR^d$ of a smooth orientable codimension 1 hypersurface in $\bR^d$, possibly different from the hypersurface containing $\Gamma_r$. Denote by $P_s, V_s$ sampling operators on $\Gamma_s \times \bR$ defined precisely analogously to $P_r$ and $V_r$ (equations \ref{eqn:recp}, \ref{eqn:recv}):
\begin{equation}
\label{eqn:recp}
(P_su)(\bx,t) = \phi_s(\bx,t)p(\bx,t),\,\bx \in \Gamma_s, t \in \bR. 
\end{equation}
and similarly for $V_s$. Denote by $\delta_{\bx_s}$ the delta distribution on $\Gamma_s$ at $\bx_s \in \Gamma_s$, and by $w \in C_0^{\infty}(\bR)$ a choice of source wavelet. The corresponding physical source is then the right-hand side in the acoustic system
\begin{equation}
\label{eqn:awe-src}
L[c]u = P_s^T(w\delta_{\bx_s}) 
\end{equation}
With dummy space and time variables, the right hand side above is exactly the form $w(t)e_1\delta(\bx-\bx_s)$ given in the point source definition \ref{eqn:ptsrc}, where , ${\bf e}_1$ the standard basis vector $e_1=(1,0,...,0) \in \bR^{d+1}$.

As explained in the introduction, unless the data $d$ and the coefficient array $c$ are compatible, the source subproblem \ref{eqn:esis} has no low-residual solution of the form $f = P_s^T(w\delta_{\bx_s}) $. A larger class of source models is required to fit data. A precise description of the class described in the introduction is simply the range of $P_s^T$: that is, for $h \in C^{\infty}(\Gamma_s \times \bR)$, define
\begin{equation}
S[c]h = P_rL[c]^{-1}P_s^T h.
\end{equation}
$S[c]$ is the surface source extension modeling operator. For this class of sources, the source subproblem becomes 
\begin{equation}
\label{eqn:esis-sse}
\mbox{given } c, \mbox{ choose } h  \mbox{ to minimize } \|S[c]h -d \|^2 + \alpha \|Ah\|^2 
\end{equation}
To complete the definition of this problem, a choice of {\em annihilator} $A$ is required. A simple choice is
\begin{equation}
\label{eqn:msste}
Ah(\bx,t) = |\bx-\bx_s|h(\bx,t). 
\end{equation} 

Formally, the solution of the source subproblem is also the solution of the normal equation
\begin{equation}
\label{eqn:normal}
(S[c]^TS[c] + \alpha^2 A^TA) h = S[c]^T d 
\end{equation}
This relation holds, and a solution $h$ is guaranteed to exist, if the $\alpha=0$ normal operator $S[c]^TS[c]$ is well-defined and continuous in $L^2(\bar{\Gamma}_s \times \bR)$, and there exists $C>0$ so that 
\begin{equation}
\label{eqn:coercive}
\|S[c]h\|^2 \le C|h|^2,\,\,\|S[c]h\|^2 + \alpha^2 \|Ah\|^2 \ge C^{-1}\|h\|^2
\end{equation}
for all $h \in L^2(\bar{\Gamma}_s)$. These estimates hold under various conditions. One particular setting in which the least squares problem \ref{eqn:esis} is well-posed is given by 

\noindent {\bf Assumption A:} There exists $\theta_*>0$ so that for every ray connecting a point in $\mbox{supp }\phi_s$ with a point in $\mbox{supp }\phi_r$, the angles $\theta_s$ and $\theta_r$ subtended by the ray and the tangent planes of $\Gamma_s$ and $\Gamma_r$ respectively are both greater than $\theta_*$ in absolute value.

The first appendix provides a proof of the following statement:

\begin{theorem} 
\label{thm:one}
Suppose that $\log \kappa, \log \beta \in C^{\infty}(\bR^d) \cap L^{\infty}(\bR^d)$, and that Assumption A holds. Then the estimates \ref{eqn:coercive} hold for $h \in C^{\infty}_0(\Gamma_s \times \bR)$, hence $S[c]$ extends to $L^2(\bar{\Gamma}_s \times \bR)$ with the same estimates.
\end{theorem}

The assumptions made in the hypotheses of \ref{thm:one} imply that the inverse wave problems under consideration in this paper are of the transmission type, that is, do not involve reflected waves. The reflected wave concept is well-defined only in terms of high-frequency asymptotics: because the coefficients are assumed smooth, there are no material singularities to generate reflections.  Two configurations of sources and receivers that naturally fit this framework, at least in an ideal sense, are
\begin{itemize}
\item the {\em crosswell} configuration, in which $\Gamma_s$ and $\Gamma_r$ are parallel hyperplanes. In reality, wells are approximately 1D structures embedded in 3D ambient media, so this configuration does not really model them well. However it has become commonplace to use the 2D version of this configuration as a model, assuming that the waves propagate in-plane between the wells  \cite[]{BregChapBai:86,Harr:93,Zhouetal:93,Pratt:99b,Plessix:00}.
\item the {\em diving wave} configuration, $\Gamma_s$ and $\Gamma_r$ are both subsets of a horizontal plane at the source, respectively receiver, depth (assumed well-defined). In marine towed streamer and ocean bottom seismometer surveys, a set of refracted waves arrives earlier than the direct wave in the water column, at sufficient distance from the source. The mutes $\phi_s, \phi_r$ isolate these refacted (``diving wave'') arrivals. This data is the focus of the ``classic'' period of FWI \cite[]{SirgPratt02,BrendersPratt:06a,BrendersPratt:06b,VirieuxOperto:09,Vigh:10,Vigh13}.
\end{itemize}
In both cases, it is possible (though not necessary) that Assumption A is satisfied. Note that in the diving wave configuration, the mutes $\phi_S,\phi_r$ are responsible for eliminating direct wave energy, hence enforcing the acute incidence of the waves connecting the admitted source and receiver points. The numerical examples presented below will fall into the crosswell category. 

Assumption A is not by any means the most general hypothesis under which the source subproblem has a well-behaved solution. However weaker hypotheses typically involve multidimensional filters applied to data (and extended sources) to eliminate waves propagating tangent to the source and receiver surfaces, so require more involved implementations. 





%Denote by $J_{\alpha}[g,c;d]$ the objective function defined in \ref{eqn:esi}:
%\begin{equation}
%\label{eqn:jdef}
%J_{\alpha}[g,c;d] = \frac{1}{2}( \|PL[c]^{-1}g -d \|^2 + \alpha \|A[g]\|^2) 
%\end{equation}
%To compute the objective 
%\begin{itemize}
%\item solve the acoustic wave system \ref{eqn:awe} with $g$ as in \ref{eqn:mssur}
%\item sample at receiver locations and compute mean-square difference with pressure data traces, take mean square
%\item add scaled mean square of annihilator applied to the source
%\end{itemize}

%The variable projection method \cite[]{GolubPereyra:03} applied to the objective \ref{eqn:esi} approaches its minimization via a nested algorithm: first minimize \ref{eqn:esi} over the source $g$ for given coefficients $c$, thus making $g = g[c]$ a function of $c$ (inner minimization); then substitute $g[c]$ in $J$ and minimize the resulting function of $c$ (outer minimization). The optimal $g[c]$ solves the first order necessary condition for minimization of $J$ over $g$: $\nabla_g J_{\alpha}[g,c;d] = 0$. For linear annihilators such as that defined by \ref{eqn:mssur}, $J$ is quadratic in $g$, so the first order necessary condition is also sufficient, and is equivalent to the linear {\em normal equation}:

%where I have written $S[c]=PL[c]^{-1}$ for the operator that maps the (extended) source to the data traces. Substitution of $g[c]$ in $J$ results in the {\em reduced objective}
%\begin{equation}
%\label{eqn:jred}
%\tilde{J}_{\alpha}[c;d] = J_{\alpha}[g[c],c;d].
%\end{equation}
%The gradient would appear to involve the derivative of the inner solution $g[c]$ with respect to $c$, computation of which implies solution of additional linear systems. However by fortunate accident, the chain rule and the normal equation \ref{eqn:normal}
%conspire to eliminate all terms involving derivatives of $g[c]$. Consequently the direction derivative of $\tilde{J}_{\alpha}$ in the direction $\delta c$ is
%\begin{equation}
%\label{eqn:tildejderiv}
%D_c\tilde{J}_{\alpha}[c;d]\delta c = \langle D(S[c]g)\delta c,S[c]g-d \rangle|_{g=g[c]}
%\end{equation}
%whence
%\begin{equation}
%\label{eqn:l2grad}
%\nabla \tilde{J}_{\alpha}[c;d] = D_c(S[c]g)^T|_{g=g[c]}(S[c]g[c]-d)
%\end{equation}
%Let us field-strip the expression on the right. The rightmost factor is the data residual, which is a by-product of many methods for the solution of the normal equation \ref{eqn:normal}. $D_cS$ is the linearization or Born approximation of $S$, or perturbation with respect to $c$, with $g$ fixed:
%\[
%S[c+\delta c]g \approx S[c]g + D_c(S[c]g)\delta c
%\]
%That is, $D_c(S[c]g)$ is the object commonly called the Born modeling operator, with source wavefield $g$. Its adjoint or transpose is $D_c(S[c]g)^T$ - as is well known, this is the RTM operator (or rather a version of it), computed via the adjoint state method in the time domain. Note that the Born and RTM operators need to match, i.e. both be derived from the same underlying full waveform modeling operator (in this case acoustodynamics, but the same would apply for any other wave theory).

%The upshot is that the computation of the gradient goes like this:
%\begin{itemize}
%\item solve the normal equation \ref{eqn:normal} for $g[c]$
%\item compute (or retain) the data residual $S[c]g[c]-d$
%\item reverse-time migrate the data residual, using $g[c]$ to generate the source wavefield in the adjoint state method - the output is the gradient of $\tilde{J}_{\alpha}$.
%\end{itemize}

%To make this algorithm practical and efficient, effective preconditioning needs to be applied to both inner and outer problems.


\subsection{Time Reversal}
%In contrast to the outer problem, a preconditioner for the inner problem is available that works regardless of medium smoothness, so for both transmission and reflection (the latter with some caveats). 
%First the transmission case - a sort of rotated crosswell configuration. Idealize the receiver set as a continuum plane $z=z_r$, with $z_r > z_s$ - not essential but simpler. 

%NB: the definition of the forward map (and the data) should include a taper $\phi(\bx_r,\bx_s)$ masking out the shot gather $d_r(\cdot,\cdot;\bx_s)$ for each source position $\bx_s$. However I will not explicitly mention this operation.

Change notation slightly, and write $S[c]h_s$ rather than $S[c]g$ with $g = h_s \delta(z-z_s)$, as $z_s$ will be presumed ot be the same for all shots. The pressure and velocity fields occuring in the computation of $d_r=S[c]h_s$ form the solution of
\begin{eqnarray}
\label{eqn:awedata}
\frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv +
h \delta(z-z_s), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t<0\nonumber\\ 
\bv & = & 0 \mbox{ for } t<0 \nonumber\\
 p|_{z=z_r}& = & d_r
\end{eqnarray}
Suppose that the recording time interval $[0,T]$ is long enough that $ d_r \approx 0$ for $t > T$. Note that $p,\bv$ are {\em incoming} (downgoing) in $z>z_s$. Therefore in $z<z_r$ {\em the same fields} approximately solve
\begin{eqnarray}
\label{eqn:awetr}
\frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p & =& 0 \mbox{ for } t>T\nonumber\\ 
\bv & = & 0 \mbox{ for } t>T \nonumber\\
p|_{z=z_r} &=&  d_r
\end{eqnarray} 
The last equation defines $p$ on the entire plane $z=z_r$ for $t<T$, and the previous two define final conditions at $t=T$. So the {\em time-reversed} system \ref{eqn:awetr} has unique solutions in $z<z_r, t<T$ and $z>z_r,t<T$. The solution in $z<z_r$ is incoming (downgoing) at $z=z_s$, and is {\em the same vector of fields} (approximately) as the solution of \ref{eqn:awedata} in the region $z_s<z<z_r$ between the source and receiver surfaces. 

Define $d_s =p|_{z=z_s}$ for the solution of \ref{eqn:awedata}. Then $p,\bv$ is also the solution of
\begin{eqnarray}
\label{eqn:awedir}
\frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p,\nonumber \\
p &=&0, t< 0,\nonumber\\
\bv &=&0,t<0,\nonumber\\
p|_{z=z_s }& = &d_s
\end{eqnarray}
in both $z>z_s$ and $z<z_s$, since the pressure field for the solution of \ref{eqn:awedata} is continuous across $z=z_s$. Since the same fields solve the two different systems, there must be a relation between the data. In fact, the normal component of velocity $v_z$ is discontinuous, and (recalling the $g(\bx,t;\bx_s) = h_s(x,y)\delta(z-z_s)$). A calculation using integration by parts shows that $h_s$ is precisely the jump in $v_z$ across $z=z_s$:
\begin{equation}
\label{eqn:vjump}
h_s = [v_z]|_{z=z_s}.
\end{equation}
A further simplification occurs if $\kappa, \rho$ are constant near $z=z_s$, then by symmetry
\begin{equation}
\label{eqn:vsimp}
[v_z]|_{z=z_s}=2 \lim_{z \rightarrow z_s^+} v_z
\end{equation}

With these preparations, define 
\begin{equation}
\label{eqn:psinv}
S[c]^{\dagger}d_r= [v_z]|_{z=z_s}.
\end{equation}
where the jump in $v_z$ at $z=z_s$ is computed by
\begin{itemize}
\item solving \ref{eqn:awetr} backwards in time,
\item reading off $d_s=p$ at $z=z_s$,
\item solving \ref{eqn:awedir} and computing $[v_z]|_{z=z_s}$, or if appropriate simply using \ref{eqn:vsimp}.
\end{itemize}
 The relations developed in the last few paragraphs may be summarized as follows:
\begin{equation}
\label{eqn:appinv}
d_r = S[c]h_s \,\,\Leftrightarrow \,\, h_s \approx S[c]^{\dagger}d_r
\end{equation}
that is, $S[c]^{\dagger}$ is an approximate inverse of $S[c]$.

As I will show in the example section, the approximation may be accurate enough without any further improvement. However, a closer examination shows that in fact it is possible to express the relation between $S[c]$ and $S[c]^{\dagger}$ in terms of a change of norm, that is, in terms of a preconditioner, and this relation can be used to considerably accelerate an iterative solution of the normal equation \ref{eqn:normal}. 

To see this, first compute the adjoint $S[c]^T$, by a variant of the adjoint state method, in this case a by-product of the conservation of energy. Suppose that 
$\tilde{p},\tilde{\bv}$ solve
\begin{eqnarray}
\label{eqn:aweadj}
\frac{\partial \tilde{p}}{\partial t} & = & \kappa (-\nabla \cdot \tilde{\bv} + 
 d_r \delta(z-z_r))\nonumber \\
\frac{\partial \tilde{\bv}}{\partial t} & = & - \beta \nabla \tilde{p},\nonumber \\
\tilde{p} & =& 0 \mbox{ for } t>T\nonumber\\ 
\tilde{\bv} & = & 0 \mbox{ for } t>T 
\end{eqnarray} 
Then
\[
0 = 
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  
\frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=T}
-
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=0}
\]
\[
= 
\int_{0}^{T} \,dt\, \frac{d}{dt}\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \frac{1}{\kappa} \frac{\partial p}{\partial t} \tilde {p} + \kappa p \frac{1}{\kappa}\frac{\partial \tilde{p}}{\partial t} \right.
\]
\[
+
\left. \frac{1}{\beta} \frac{\partial \bv}{\partial t} \cdot \tilde{\bv} + \frac{1}{\beta} \bv \cdot \frac{\partial \tilde{\bv}}{\partial t} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 \frac{h_s}{\kappa} \delta(z-z_s)\right) \tilde{p} + p \left(- \nabla \cdot \tilde{\bv} + 
 d_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  (- \nabla p) \cdot \tilde{\bv} + \bv \cdot (-\nabla \tilde{p}) \right)
\]
\[
= 
\int_{0}^{T}\,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 \frac{h_s}{\kappa} \delta(z-z_s)\right) \tilde{p} + p \left(- \nabla \cdot \tilde{\bv} + 
 d_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  p (\nabla \cdot \tilde{\bv}) + (\nabla \cdot \bv) \tilde{p} \right)
\]
after integration by parts in the last two terms. Most of what is left cancels, leaving 
\[
0 = \int\,dt\,dx\,dy\, h_s \left(\frac{\tilde{p}}{\kappa}\right)_{z=z_s} + (p  d_r)_{z=z_r}
\]
whence
\begin{equation}
\label{eqn:sadj}
 S[c]^T d_r = -\left(\frac{\tilde{p}}{\kappa}\right)|_{z=z_s}
\end{equation}
Both $S[c]^T$ and $S[c]^{\dagger}$ are defined by time-reversed solutions of the same acoustic system, except with different data. System \ref{eqn:aweadj} places $d_r$ as a pressure source on $z=z_r$, with an additional factor of $\kappa$, and its trace on $z=z_s$ must be scaled by $1/\kappa$. System \ref{eqn:awetr} employs $d_r$ as a Dirichlet boundary condition, and the trace of its pressure component must be employed as a Dirichlet boundary condition for the forward-in-time problem \ref{eqn:awedir}, after which the jump in the normal velocity is extracted. 

The relation between a Dirichlet boundary condition and a source has already been explained, in the context of justifying the approximation property of $S[c]^{\dagger}$:
\begin{quote}
The surface source producing the same field as a Dirichlet boundary condition (on either side of the boundary surface) is the jump in the normal velocity component of the Dirichlet field. 
\end{quote}
Call the operator that produces the jump in normal velocity from Dirichlet pressure data $\Lambda[c]_s$ for the source surface $z=z_s$, $\Lambda[c]_r$ for the receiver surface $z=z_r$ (in the math literature, this is the ``Dirichlet-to-Neumann operator'', more or less). The discussion so far can be summarized as follows:
\begin{equation}
\label{eqn:wadj}
S[c]^{\dagger} = W[c]_m^{-1}S[c]^TW[c]_d 
\end{equation}
in which 
\begin{equation}
\label{eqn:weights}
W[c]_m = \kappa^{-1},\,\, W[c]_d = \kappa^{-1}\Lambda[c]_r
\end{equation}
in which ``$\kappa$'' means the operator ``multiply by $\kappa$''. This means that $S[c]^{\dagger}$ {\em is} the adjoint of $S[c]$ with respect to weighted norms 
\begin{itemize}
\item on the source space ($h_s$): weight $W[c]_m$
\item on the data space ($d_r$): weight $W[c]_d$
\end{itemize}
Since $S[c]^{\dagger}S[c] \approx I$ as shown above, $S[c]$ is approximately unitary in the Hilbert spaces of source and data traces with norms defined by the weighting operators $W_m$ and $W_d$ respectively. Therefore a Krylov space method employing these norms will converge rapidly.

\cite{HouSymes:EAGE16} demonstrated a very similar preconditioner for Least Squares Migration, also for its subsurface offset extension \cite[]{HouSymes:16}, motivated by \cite{tenKroode:12}. These constructions all involve the Dirichlet-to-Neumann operator. This concept also turns up in hidden form in the work of Yu Zhang and collaborators on true amplitude migration \cite[]{YuZhang:14,TangXuZhang:13,XuWang:2012,XuZhangTang:11,Zhang:SEG09,ZhangYuSun:08,ZhangSunGray:07,ZhangBleistein:05,Bleisteinetal:05}.

\section{Prototype Numerical Examples}
I present a collection of simple examples that illustrate the features of the surface source extension claimed in preceding sections. 

I used the IWAVE acoustic staggered grid package to carry out these calculations. This package implements (2,2k) schemes for k=1,2,..., and outputs traces (of either velocity or pressure at any point in space via multilinear interpolation. The discretized modeling operator is thus of second order accuracy, though as usual I have used higher order in space to reduce grid dispersion.

Source injection is implemented as the adjoint of trace sampling, resulting in another second-order error [REFERENCES]. 

The data is a single shot gather, with a source at coordinates $x_s=z_s=3000$ m (units of length are meters in all cases). The receiver line occupies $1500 \le x_r \le 5500$ m, with receiver depth $z_r=1000$ m.  Extended sources occupy $1500 \le x_s \le 5500$ m, with the same depth $z_s=3000$ m as the ``physical'' source used to generate the data. This region turns out to be adequate to represent the extended sources that approximately invert the data, for the cases examined below. An algorithm to automatically identify an appropriate region can be based on the ideas developed by \cite{Fu:Geo17}.

I have used absorbing boundary conditions (split-field PML) on all four sides of the 4000 m (vertical) $\times$ 8000 m (horizontal) simulation domain. Evidently inclusion of a free surface is important to the application of the ideas explained here to diving wave marine data, and I have not addressed the necessary modifications here. 

The Dirichlet-to-Neumann operator $\Lambda$ is an essential part of the inner problem preconditioner just presented. Implementation can be accomplished in several ways:
\begin{itemize}
\item \cite{tenKroode:12} suggests using a one-way operator;
\item if both pressure and normal partical velocity are measured (or simulated), then the two are related by $\Lambda$ and the velocity component can simply be used as the output;
\item presence of a free surface implies all of the usual problems, such as the need for removal of receiver-side ghosts. On the other hand, if the free surface is within a quarter-wavelength throughout the useful bandwidth of the data, then the ghosted data differs from $\Lambda p$ by a time integration and a scale factor, a fact used to good effect by \cite{HouSymes:15}.
\end{itemize}

In the examples, I have used the second observation. With a finite difference implementation of the pressure-velocity system \ref{eqn:awe}, velocity components are available ``for free'', short-circuiting explicit computation of $\Lambda$.

The  {\tt project/SConstruct} script is set up to carry out the necessary computations on grids with spacings $\Delta x = \Delta z = $ 20, 10, and 5 m, with a jump of roughly 8 in computation time resulting from each refinement. For present purposes, the coarsest (20 m) grid seems to be sufficient, and that is the grid used in the examples presented below. The source pulses are chosen so that the computation is reasonably accurate. For the 20 m grid case, I use a zero-phase trapezoidal bandpass filter source with corner frequencies of 1.0, 2.0, 7.5, and 12.5 Hz.

The IWAVE asg driver has been set up to recognize the case {\tt deriv=0} as defining the map from source (right-hand side in the pressure equation) to data (pressure) traces. The adjoint to this map, as explained above, is reverse-time propagation of the data traces as pressure sources, followed by scaling (formula \ref{eqn:sadj}). The approximate inverse is computed by application of the Dirichlet-to-Neumann operator to the pressure traces to produce corresponding velocity traces, followed by injection as pressure sources and reverse time propagation, followed by another application of the Dirichlet-to-Neumann map and scaling (formulas \ref{eqn:appinv}, \ref{eqn:adj}).

The script implements these operatations step-by-step via calls to IWAVE, Madagascar, and SU commands. A peculiarity of the {\tt asg.x} driver needs to be mentioned: it is based on an un-scaled version of the constitutive law defect source representation, that is, {\tt asg.x} approximately computes the solution of the system \ref{eqn:awedata} with the first equation replaced by
\begin{equation}
\label{eqn:asgdata}
\frac{\partial p}{\partial t}  =  - \kappa \nabla \cdot \bv +
h \delta(z-z_s).
\end{equation}
Denote by $S_{\rm asg}[c]$ the forward map produced by {\tt asg.x}. Then comparison of \ref{eqn:awedata} and \ref{eqn:asgdata} reveals that
\begin{equation}
\label{eqn:sreln}
S[c]=S_{\rm asg}[c]\kappa
\end{equation}
where $\kappa$ is shorthand for the operator of multiplication by $\kappa$. Accordingly, and approximate inverse for $S_{\rm asg}[c]$ is
\[
I \approx S_{\rm asg}[c]^{\dagger}S_{\rm asg}[c] = S_{\rm asg}[c]^{\dagger}S[c]\kappa^{-1}
\]
Since $S[c]S[c]^{\dagger} \approx I \approx S[c]^{\dagger}S[c]$, it follows that
\[
S_{\rm asg}[c]^{\dagger}=\kappa S[c]^{\dagger} = \kappa\Lambda[c]_s S[c]^T \Lambda[c]_r 
\]
\begin{equation}
\label{eqn:asginv}
= \kappa \Lambda[c]_s \kappa S_{\rm asg}[c]^T\Lambda[c]_r
\end{equation}
This is the approximate inverse computed in the examples. Note that only the values of $\kappa$ near the source datum $z=z_s$ play a role in the relation \ref{eqn:sreln} or in the definition \ref{eqn:asginv} of the IWAVE ASG approximate inverse.
\bibliographystyle{seg}
\bibliography{../../bib/masterref}

\append{Proof of Theorem 1}
Following the notation of \cite{BaoSy:91b}, denote by $\gamma_s$ ($\gamma_r$)the closed conic subset of $T^*(\Gamma_s\times \bR)|_{\mbox{supp } \phi_s}$ ($T^*(\Gamma_r\times \bR)|_{\mbox{supp } \phi_r}$) consisting of tangential components of null bicharacteristics passing over points of $\mbox{supp }\phi_s$ and $\mbox{supp }\phi_r$. That is, the ray with initial condition $(\bx,t,\xi,\omega) \in T^*(\Gamma_s\times \bR)|_{\mbox{supp } \phi_s}$ passes over $\mbox{supp }\phi_r$ if and only if $(\bx,t,\xi,\omega) \in \gamma_s$. Since each such ray is paired with the ray with the same spatial projection traversed backwards, it is also the case that $(\bx,t,\xi,\omega) \in T^*(\Gamma_r\times \bR)|_{\mbox{supp } \phi_r}$ passes over $\mbox{supp }\phi_s$ if and only if $(\bx,t,\xi,\omega) \in \gamma_r$. 

Define $s=\sqrt{\kappa/\rho}$ to be the wave slowness (reciprocal velocity), and
\[
c(\bx,t,\xi,\omega) = s(\bx)^2\omega^2-|\xi|^2
\]
to be the characteristic polynomial of the acoustic wave system, and $C=c^{-1}(0)$ the characteristic variety.  The angle condition in Assumption A implies the existence of a conic neighborhood $\tilde{\gamma}_s$ of $\gamma_s$, so that for  $(\bx,t,\xi,\omega) \in \tilde{\gamma}_s$, the polynomial $\tau \mapsto c(\bx,t,\xi + \tau{\bf n}_s(\bx),\omega)$ has two distinct roots, $\pm \tau_s$. Let $\gamma_{1,s}$ be a conic neighborhood of the complement $\tilde{\gamma}_s^C$, chosen so that $\gamma_{1,s} \cap \gamma_s = \emptyset$. Let $\gamma_s' = \Pi_s^*(\gamma_{1,s})$, where $\Pi_s^*$ is the pullback map: $T^*(\Gamma_s\times \bR) \rightarrow T^*(\bR^{d+1})$. Similarly, construct $\gamma_r'$ and $\gamma_{1,r}$. 

The null bicharacteristic through $(\bx,\xi,\omega,\xi) \in \gamma_r' \cap C$ does not pass over $\mbox{supp } \phi_s$ by construction, hence the RHS of the system $L[c]u=P_r^Th$ is microlocally of class $H^1$ (in fact, smooth) near this bicharacteristic,  thanks to the microlocal propagation of singularities theorem \cite[]{BealsReed:82}: that is, $p \in H^1_{ml}(\gamma_r')$. 

Need to know $p \in L^2$. Restart: consider $F_{r,s}$ mapping Cauchy data at $t=T$ by reverse time to $P_{r,s} p$, $\mbox{supp }\phi_s \cup \mbox{supp }\phi_r \subset \{t<T\}$. Only relevant rays are continuations of those in $\gamma_r$. Represent $S$ as composition: $S=F_rF_s^T$. $F_s$ $L^2$-bounded $\Rightarrow$ for $u, v \in C_0^{\infty}$,
\[
|\langle F_s u, v \rangle| = \le C\|u\|\|v\|=|\langle u, F_s^T v\rangle|
\]
Indep arg establishes that $F_s^Tv$



%for which $(\bx,t,\xi\pm \tau_s{\bf n}_s(\bx),\omega)$ is the initial condition on $\Gamma_s \times \bR$ of a ray passing over $\mbox{supp }\phi_r$. Choose a conic neighborhood $\tilde{\gamma}_s$ of $\gamma_s$ 

Denote by 


%,  $T^*(\Gamma_r\times \bR)|_{\mbox{supp \phi_r}}$ for which Note that the angle condition mentioned in Theorem \ref{thm:one} implies that if $(\bx,t,\xi,\omega) \in \gamma' $ and $\bx \in \Gamma_s$ or $\Gamma_r$, then $\omega$ is a simple root of are distinct, and even a lower bound on the difference proportional to $\epsilon$. It also implies that $\gamma'$ is disjoint from a neighborhood of the normal bundles of $\Gamma_s \times \bR$, $\Gamma_r \times \bR$.

\append{Transpose of Full Acoustic Modeling Operator}

In this section I compute the adjoint operator form modeling with both pressure and velocity source inputs and pressure and velocity trace outputs. The forward modeling operator is defined by solving
\begin{eqnarray}
\label{eqn:fawedata}
\frac{\partial p}{\partial t} & = & - \kappa( \nabla \cdot \bv +
h_s \delta(z-z_s)), \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta ( \nabla p + l_s {\bf e}_z\delta(z-z_s)),\nonumber \\
p & =& 0 \mbox{ for } t<0\nonumber\\ 
\bv & = & 0 \mbox{ for } t<0 \nonumber\\
\end{eqnarray}
The new elements here are the point load time function $l_s(\bx,t;\bx_s)$, and the vertical unit vector ${\bf e}_z=(0,1)^T$ for 2D, $=(0,0,1)^T$ for 3D. The forward map ${\bf F}$ is defined as implicit in the discussion of the Intrinsically Preconditioned CG algorithm: let ${\bf h}_s=(h_s,l_s)^T$, then 
\begin{equation}
\label{eqn:ffwd}
{\bf F}{\bf h}_s =\left\{ \left[
\begin{array}{c}
p(\bx,t;\bx_s)\\
v_z(\bx,t;\bx_s)
\end{array}
\right]_{z=z_r}\right\}
\end{equation}

Let $\tilde{\bf h}_r=(\tilde{h}_r,\tilde{l}_r)^T$ be a function on $z=z_r$, 
and 
$\tilde{p},\tilde{\bv}$ solve the backwards-in-time boundary value problem:
\begin{eqnarray}
\label{eqn:faweadj}
\frac{\partial \tilde{p}}{\partial t} & = & \kappa (-\nabla \cdot \tilde{\bv} + 
 \tilde{h}_r \delta(z-z_r))\nonumber \\
\frac{\partial \tilde{\bv}}{\partial t} & = & \beta (\nabla \tilde{p} + \tilde{l}_r{\bf e}_z\delta(z-z_r),\nonumber \\
\tilde{p} & =& 0 \mbox{ for } t>T\nonumber\\ 
\tilde{\bv} & = & 0 \mbox{ for } t>T 
\end{eqnarray} 
Then
\[
0 = 
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  
\frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=T}
-
\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)|_{t=0}
\]
\[
= 
\int_{0}^{T} \,dt\, \frac{d}{dt}\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \frac{1}{\kappa} \frac{\partial p}{\partial t} \tilde {p} +  p \frac{1}{\kappa}\frac{\partial \tilde{p}}{\partial t} \right.
\]
\[
+
\left. \frac{1}{\beta} \frac{\partial \bv}{\partial t} \cdot \tilde{\bv} + \frac{1}{\beta} \bv \cdot \frac{\partial \tilde{\bv}}{\partial t} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv + 
 h_s \delta(z-z_s)\right) \tilde{p} + p \left(- \nabla \cdot \tilde{\bv} + 
 \tilde{h}_r \delta(z-z_r)\right) \right.
\]
\[
+
\left.  (- \nabla p + l_s{\bf e}_z\delta(z-z_s)) \cdot \tilde{\bv} + \bv \cdot (-\nabla \tilde{p} + \tilde{l}_r{\bf e}_z\delta(z-z_r)) \right)
\]
\[
= 
\int_{0}^{T}\,dt\, \left(\int\, dx\,dy\,dz\, \left(- \nabla \cdot \bv \tilde{p} + 
 h_s\tilde{p}|_{z=z_s} - p\nabla \cdot \tilde{\bv} + 
 \tilde{h}_r p|_{z=z_r}\right) \right.
\]
\[
+
\left.  p (\nabla \cdot \tilde{\bv}) + l_s \tilde{v}_z|_{z=z_r}+ (\nabla \cdot \bv) \tilde{p}  + l_r  v_z|_{z=z_r}\right)
\]
after integration by parts in the last two terms. Most of what is left cancels, leaving 
\[
0 = \int\,dt\,dx\,dy\, (h_s \tilde{p})_{z=z_s} + (p  \tilde{h}_r)_{z=z_r} + (l_s\tilde{v}_z)_{z=z_s} + (v_z \tilde{l}_r)_{z=z_r},
\]
whence
\begin{equation}
\label{eqn:fadj}
 {\bf F}^T {\bf h}_r = -\left[
\begin{array}{c}
\tilde{p}(\bx,t;\bx_s)\\
\tilde{\bf v}(\bx,t;\bx_s)
\end{array}
\right]_{z=z_s}
\end{equation}

\append{Transpose of Cauchy Data to Trace operator}

For $u_0=(p_0,\bv_0) \in (C_0^{\infty}(\bR^d))^{d+1}$, define ${\bf F}u_0 = (P_r p, P_r {\bf n}_r\cdot \bv)$ where $u=(p,\bv)$ solves
\begin{eqnarray}
\label{eqn:fawedata}
\frac{\partial p}{\partial t} & = & - \kappa \nabla \cdot \bv, \nonumber \\
\frac{\partial \bv}{\partial t} & = & - \beta \nabla p ,\nonumber \\
p(\cdot,T) & =& p_0 \\ 
\bv(\cdot,T) & = & \bv_0\\
\end{eqnarray}
(Note: assumed that $\mbox{supp }\phi_r \subset \bR^d \times (0,T)$.)

Let $\tilde{\bf h}_r=(\tilde{p}_r,\tilde{v_{nr}})^T \in (C_0^{\infty}(\Gamma_r \times (0,T)))^2$ 
and 
$\tilde{u}=(\tilde{p},\tilde{\bv})$ solve 
\begin{eqnarray}
\label{eqn:faweadj}
\frac{\partial \tilde{p}}{\partial t} & = & \kappa (-\nabla \cdot \tilde{\bv} + 
 \tilde{p}_r \phi_r \delta_{\Gamma_r \times \bR})\nonumber \\
\frac{\partial \tilde{\bv}}{\partial t} & = & \beta (-\nabla \tilde{p} + \tilde{v}_{nr}{\bf n}_r \phi_r \delta_{\Gamma_r \times \bR},)\nonumber \\
\tilde{p} & =& 0 \mbox{ for } t<0\nonumber\\ 
\tilde{\bv} & = & 0 \mbox{ for } t<0 
\end{eqnarray} 
Then
\[
0 = 
\left(\int\, dx\,dy\,dz\, \frac{p_0 \tilde{p}}{\kappa} +  
\frac{\bv_0 \cdot \tilde{\bv}}{\beta} \right)|_{t=T}
\]
\[
= 
\int_{0}^{T} \,dt\, \frac{d}{dt}\left(\int\, dx\,dy\,dz\, \frac{p \tilde{p}}{\kappa} +  \frac{\bv \cdot \tilde{\bv}}{\beta} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \left(\int\, dx\,dy\,dz\, \frac{1}{\kappa} \frac{\partial p}{\partial t} \tilde {p} +  p \frac{1}{\kappa}\frac{\partial \tilde{p}}{\partial t} \right.
\]
\[
+
\left. \frac{1}{\beta} \frac{\partial \bv}{\partial t} \cdot \tilde{\bv} + \frac{1}{\beta} \bv \cdot \frac{\partial \tilde{\bv}}{\partial t} \right)
\]
\[
= 
\int_{0}^{T} \,dt\, \int\, dx\,dy\,dz\, \left((- \nabla \cdot \bv) \tilde{p} + p (- \nabla \cdot \tilde{\bv})\right.
\]
\[
\left.- \nabla p  \cdot \tilde{\bv} + \bv \cdot( -\nabla \tilde{p})\right)
\]
\[
+ \int_0^T \,dt \, \int_{\Gamma_r}\, dS_{\Gamma_r}( p \tilde{p}_r + \bv \cdot {\bf n}_r \tilde{v}_{nr})
\]
The first term vanishes after integration by parts. Accordingly,
\begin{equation}
\label{eqn:fadj}
 {\bf F}^T {\bf h}_r= -\tilde{u}(\cdot,T).
\end{equation}

\append{$L^2$ estimates}
This appendix lays out the proof of 

\begin{theorem} 
\label{thm:one}
Suppose that $\log \kappa, \log \beta \in C^{\infty}(\bR^d) \cap L^{\infty}(\bR^d)$, and that Assumption A holds. Then the estimates \ref{eqn:coercive} hold for $h \in C^{\infty}_0(\Gamma_s \times \bR)$, hence $S[c]$ extends to $L^2(\bar{\Gamma}_s \times \bR)$ with the same estimates.
\end{theorem}

For convenience, I restate

\noindent {\bf Assumption A:} There exists $\theta_*>0$ so that for every ray connecting a point in $\mbox{supp }\phi_s$ with a point in $\mbox{supp }\phi_r$, the angles $\theta_s$ and $\theta_r$ subtended by the ray and the tangent planes of $\Gamma_s$ and $\Gamma_r$ respectively are both greater than $\theta_*$ in absolute value.

Recall that $\Gamma_r \subset \bR^d$ is a smooth embedded surface containing the receiver locations. Denote by 
${\cal I}_r$ the injection of $\Gamma_r \times \bR$ into $\bR^{d+1}$. Define the angle $\theta(\xi)$ made by a covector $\xi \in T^*(\bR^d)|_{\Gamma_r}$ with $\Gamma_r$ by 
\begin{equation}
\label{eqn:angledef}
\cos \theta(\xi) = \frac{\|  (D{\cal I}_r)^T\xi\|}{\|\xi\|}.
\end{equation}
in which $\| \cdot \|$ denotes the induced Euclidean norm.

For a covector $(\xi,\omega) \in  T^*(\bR^{d+1})$, define the angle with $\Gamma_r \times \bR$ to be the angle made by $\xi$ with $\Gamma_r$.

Choose $\theta^*>0$, and denote by $\lambda_r$ the open conic subset of $T^*(\bR^{d+1}) |_{\Gamma_r \times \bR}$ of covectors making angle $> \theta*$ with $\Gamma_r \times \bR$. The projection $\gamma_r = (D{\cal I}_r)^T\lambda_r$ consists of tangential components of these covectors; it is easy to see that $\gamma_r$ is also open. 

Let $P$ denote the symbol of the acoustic system
\begin{equation}
P(\bx,\omega,\bk) = 
i\left(
\begin{array}{cc}
\frac{\omega}{\kappa(\bx)} & \bk^T\\
\bk &  \rho(\bx)\omega I_{3\times3}
\end{array}
\right)
\end{equation}
For a normal covector $\nu \in {\cal N}(\Gamma_r \times \bR)  = ((D{\cal I}_r)^T)^{-1}(0)$ and $\bk \in \gamma_r$, the polynomial
\[
\omega \mapsto \det P(\bx,\omega,\bk)
\]
has two distinct nonzero roots $\pm \omega(\bx,\bk)$. 

Let $\beta_r$ be an open conic neighborhood of $\gamma_r^C$, and define $\alpha_r = ((D{\cal I}_r)^T)^{-1}\beta_r \subset T^*(\bR^{d+1}) |_{\Gamma_r \times \bR}$.

Let $H \in OPS^0(\bR^{d+1})$ be elliptic with symbol asymptotic to 1 near the characteristic variety 
\begin{equation}
\label{eqn:chardef}
C = \{(\bx,t,\bk,\omega): \det P(\bx,\bk,\omega)=0\}
\end{equation}. That is,
 $ES(H)$ is a small nbhd of characteristic variety. Note that
\[ ES(H) \cap {\cal N}(\Gamma_r \times \bR) = \emptyset\]
From the second of these facts and the definition of $\beta_r$, the Theorem
in \cite{BaoSy:91b} implies that for $\Phi \in C_0^{\infty}(\bR^{d+1})$,
\begin{equation}
\label{eqn:baosymes}
Pu=0 \mbox{ and } u \in L^2(\bR^{d+1}) \cap H^1_{ml}(ES(H) \cap \beta_r) \,\Rightarrow\, {\cal I_r}^*(\Phi u) \in L^2(\Gamma_r \times \bR)
\end{equation}

Unpack this: $u \in H^1_{ml}(ES(H) \cap \beta_r),\, s \ge 0$ means that there exists $A \in OPS^1(\bR^{d+1})$, elliptic in $ES(H) \cap \beta_r$, so that $Au \in L^2_{\rm loc}(\bR^{d+1})$. Choose $\Phi \in C_0^{\infty}(\bR^{d+1})$, that is, $\Phi Au \in L^2_{\rm loc}(\bR^{d+1})$, that is, $u \in {\rm Dom}(\Phi A)$. Denote by $\| \cdot \|_{\Phi A}$ the graph norm of $\Phi A$, and by $H^s_{\Phi A}$ the completion of $C_0^{\infty}(\bR^{d+1})$ with respect to $\| \cdot \|_{\Phi A}$, with its natural injection into $L^2(\bR^{d+1})$. Then  
\[
u \in H^s_{ml}(ES(H) \cap \beta_r) \,\Leftrightarrow \, u \in H^s_{\Phi A}
\]
for any $\Phi$, suitable choice of $A$. Further, the proof of the Theorem in \cite{BaoSy:91b} actually implies that for $\Phi$, $A$ as above with $s=1$,
\[
\|{\cal I}_r^*(\Phi u)\| \le C \|u\|_{\Phi A},
\]
That is, the localized restriction map is continuous on $H^1_{\Phi A}$.

 $\lambda_T \subset T^*(\bR^{d+1})_{t=T}$ = intersection with flow-out of $\lambda_r$, ${\cal I}_t =$ injection $\bR^d \rightarrow \bR^d \times \{t\} \subset \bR^{d+1}$, $\gamma_T = (D{\cal I})^T\lambda_T$. $\gamma_T$ = projection of flow-out to $\{t=T\}$ of lift of $\gamma_r$.

$R_T \in OPS^1(\bR^d)$ elliptic of order 1 in nbhd of $\bar{\gamma}_T^c$ so $ES(R_T)^c \subset \gamma_T$. $R \in C^{\infty}(\bR, OPS^1(\bR^d))$ flow-out of $R_T$. Write (lifted essential support) 
\[
LES(R) = \cup_{t \in \bR} ((D({\cal I}_t)^T)^{-1}ES(R_t). 
\]
\begin{lemma}
\label{thm:lem1} $(D{\cal I}_r)^T (LES(R) \cap T^*(\bR^{d+1})_{\Gamma_r \times \bR})$ contains a neighborhood of $\bar{\gamma}_r^c$.
\end{lemma}
\begin{proof}
$(\bx,t,\xi,\omega) \in T^*(\bR^{d+1})_{\Gamma_r \times \bR}, \notin LES(R)$ means that $(D({\cal I}_r)^T(\bx,t,\xi,\omega) = (\bx,\xi) \notin ES(R_t)$. Let ${\bf H}$ denote the flow. Then ${\bf H}_{T-t}(\bx,\xi) \notin ES(R_T)$ so ${\bf H}_{T-t}(\bx,\xi) = (\bx_T,\xi_T) \in \gamma_T$ so there are $\omega_T, \omega_r$ for which $(\bx_T,T,\xi_T,\omega_T) \in \lambda_T \cap \mbox{ Char }$ is on ray through $(\bx,t,\xi,\omega_r) \in \lambda_r$, whence $(D{\cal I}_r)^T (\bx,t,\xi,\omega_r) \in \gamma_r$.
\end{proof}

Then Prop. 2 in BaoSymes implies that for $ \Phi \in C_0^{\infty}(\bR^{d+1})$, $R\Phi H \in OPS^1(\bR^{d+1})$ and $ \lambda_r^C \subset ES(R\Phi H) \subset LES(R) \cap ES(H)$. Standard energy estimates imply if $R_Tu_T \in L^2(\bR^d)$ then $Ru \in L^2(\bR^{d+1})$. Also $(I-H)\Psi u \in H^m_{\rm loc}(\bR^{d+1})$ for any $m>0$ whence $R\Phi(I-H)\Psi u \in H^m_{\rm loc}(\bR^{d+1})$ for any $m \ge 0$ assume $\Psi \Phi = \Phi $. So $R\Phi Hu \in L^2(\bR^{d+1})$.

\bibliographystyle{seg}
\bibliography{../../bib/masterref}
