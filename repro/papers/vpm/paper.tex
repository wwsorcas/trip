\title{Notes on the Variable Projection Method}
\author{William W. Symes}

\begin{abstract}
The variable projection reduction of a separable least squares objective function has a gradient of simple form. Stationary points of the reduced function and the original, un-reduced function are in 1-1 correspondence. An approximate Hessian analogous to the Gauss-Newton Hessian of nonlinear least squares guarantees a non-increase direction and involves less computation than the full Hessian.
\end{abstract}

\section{Introduction}
The Variable Projection Method (VPM) is a useful algorithm for solving separable least squares problems, that is, those whose objectives $f(x,w)$ are quadratic in some of their unknowns, say $w$, and (possibly) non-quadratic in others, say $x$. A common approach to minimizing such objectives is {\em alternating direction minimiazation}. This iterative method starts with an initial guess $(x_0,w_0)$ at the solution, and constructs a sequence $\{(x_k,w_k)\}$, guaranteed to converge to a stationary point if it converges (to anything). The sequence is constructed as follows: given a current guess $(x_k,w_k)$ at the solution, find $w_{k+1}$ by minimizing $f(x_k,w)$ over $w$, then find $x_{k+1}$ by minimizing $f(x,w_{k+1})$ over $x$. This approach is conceptually simple and makes use of whatever might be known about the two separate minimization problems (over $w$, respectively $x$), but tends to be quite inefficient. That is, it can require many more function and gradient evaluations to achieve a good approximation of a stationary point that would Newton's method (say) applied directly to $f(x,w)$.

Variable projection is an alternative approach roughly as rapidly convergent as Newton's method but uses solutions of the (computationally smaller) $w$ and $x$ subproblems.
It takes advantage of special algorithms for quadratic minimization to eliminate the quadratic unknowns ($w$), leaving a function of the nonlinear unknowns ($x$) to be minimied. The utility of VPM rests largely on the simplicitly of the derivative, or gradient, of the reduced function: it is the same as the gradient of the original, unreduced function, evaluated with the quadratic variables fixed at their inner solution value. Furthermore, stationary points of the reduced function are in 1-1 correspondence (under certain widely-met conditions) with stationary points of the unreduced function. These observation due to Golub and Pereyra underlie the widespread popularity of VPM  \cite[]{GolubPereyra:73,GolubPereyra:03}.

VPM has been widely used in conjunction with generalized secant (quasi-Newton) methods such as Limited Memory Broyden-Fletcher-Goldfarb-Shanno \cite[]{NocedalWright} in application to scientific and engineering separable least-squares problems, often without explicit acknowledgement that VPM is involved or with drastic approximation to the reduction \cite[]{Ghattas:IP25,LeeuwenHerrmann:16,vanLeeuwenMulder:09,Warner:16}. Other approximations to Newton's method in the VPM context are less often found, since the Hessian computation is relatively involved compared to the gradient. However \cite{Kaufman:75} developed a simplified Hessian, with roughly the same relation to the full Hessian as the Gauss-Newton Hessian to the full Hessian for ordinary nonlinear least squares, 

In this note, I review the construction of the reduced gradient, with somewhat more care than is common in the literature. Then I present a complete derivation of the Kaufman Hessian. I end with a comment on the use of this structure in a Krylov-Newton context, in which all of the inner linear systems are solved by iterative methods such as conjugate gradients. 

\section{The Variable Projection Objective}
The variable projection method is a minimization algorithm for a scalar function on a product space $f:X \oplus W \rightarrow \bR$ of class $C^2$, $X$ and $W$ being Hilbert spaces. I will assume that $f$ is defined on the whole product space; the refinements necessary to accommodate constraints on $x$ are similar to those needed for solution of other nonlinear optimization problems. While not strictly necessary, I will also assume that $f$ is quadratic in the second variable. Put another way, $f(x,w) = 0.5*\|A(x)w-b\|^2$, where $b \in Y$ (another Hilbert space with norm $\|\cdot\|$) and the values of $A$ are linear operators $: W \rightarrow Y$. Further, it's usually assumed that $A(x)$ is of full column rank (or coercive, in the infinite dimensional case), so that that for each $x$, there is a unique minimiser $\tilde{w}(x)$ of $w \mapsto f(x,w)$, the solution of the normal equation: $A(x)^T(A(x)\tilde{w}(x) - b)=0$. Define the ``variable projection (VP) reduction'' $\tilde{f}$ by 
$$
\tilde{f}(x) = 0.5*\|A(x)\tilde{w}(x)-b\|^2= \min_w f(x,w)
$$
Since $A(x)$ is assumed coercive for every $x \in X$, $A(x)^TA(x)$ is invertible, and $\tilde{w}(x) = (A(x)^TA(x))^{-1}A(x)^Tb$. So
$$ 
\tilde{f}(x) = 0.5*\|(A(x)(A(x)^TA(x))^{-1}A(x)^T - I)b\|^2
$$
The operator in parenthesis projects $Y$ onto the orthocomplement of the range of $A(x)$: call it $P(x)$. That is,
\begin{equation}
  \label{eqn:proj}
P(x) = I-A(x)(A(x)^TA(x))^{-1}A(x)^T
\end{equation}
and
$$
\tilde{f}(x) = 0.5*\|P(x)b\|^2
$$
So the reduced objective is half the length squared of the projection of the data vector $b$ onto the orthocomplement of the range of $A(x)$, which of course depends on $x$. That fact accounts for the name "variable projection".

\section{Gradient: the Observation of Golub and Peyreyra}

One of the main results of the Golub and Pereyra 1973 paper is that $x$ is a stationary point of $\tilde{f}$ if and only if $(x,\tilde{w}(x))$ is a stationary point of $f$. It's worth spelling out the argument because it highlights several important points about the VP reduction.

Note that $f$ is differentiable, if $(x,w)\mapsto A(x)w$ is differentiable, which I will assume. Suppose $s \in X$. Then the directional derivative of $\tilde{w}(x)=(A(x)^TA(x))^{-1}A(x)^Tb$ at $x$ in direction $s$ is 
$$
\frac{d}{dt}((A(x+ts)^TA(x+ts))^{-1}A(x+ts)^Tb)|_{t=0}
$$
$$
=-(A(x)^TA(x))^{-1}\frac{d}{dt}(A(x+ts)^TA(x+ts)(A(x)^TA(x))^{-1}A(x)^Tb)|_{t=0}
$$
$$
+ (A(x)^TA(x))^{-1}\frac{d}{dt}(A(x+ts)^Tb)|_{t=0}
$$
and in particular $\tilde{w}$ is differentiable. Therefore
$$
\frac{d}{dt}\tilde{f}(x+ts)|_{t=0} = \left\langle\frac{d}{dt}A(x+ts)\tilde{w}(x), A(x)\tilde{w}(x)-b\right\rangle + 
$$
$$
0.5\left\langle A(x)\frac{d}{dt}\tilde{w}(x+ts),A(x)\tilde{w}-b\right\rangle
$$
Since the normal equation is equivalent to the assertion that the residual $A(x)\tilde{w}(x)-b$ is orthogonal to the range of $A(x)$, the last term vanishes.

The first term can be re-written as the directional derivative of $f(x,w)$ for fixed $w=\tilde{w}(x)$, that is,
$$
\frac{d}{dt}\tilde{f}(x+ts)|_{t=0} = \frac{d}{dt}f(x+ts,w)|_{t=0,w=\tilde{w}(x)}.
$$
So $x$ is a stationary point of $\tilde{f}$ if and only if the directional derivative of $f$ at $(x,\tilde{w}(x))$ in all directions $(s,0)$ is zero. But the directional derivative of $f$ at $(x,\tilde{w}(x))$ in all directions $(0,\delta w)$ is also zero - that is the definition of $\tilde{w}(x)$. Since the directional derivative is linear in the direction, the directional derivative of $f$ at $(x,\tilde{w}(x))$ is zero in all directions, that is, $(x,\tilde{w}(x))$ is a stationary point of $f$, if and only if $x$ is a stationary point of $\tilde{f}$. 

The derivative of the linear-operator-value function $A$ is naturally a bilinear-operator-valued function, since it's linear in the argument $w$ and in the direction $s$ separately. Call it $DA$:
$$
\frac{d}{dt}A(x+ts)w|_{t=0} = DA(x)(w,s)
$$ 
In terms of $DA$, the directional derivative of $\tilde{f}$ is
$$
\frac{d}{dt}\tilde{f}(x+ts)|_{t=0} = \left\langle DA(x)(\tilde{w}(x),s), A(x)\tilde{w}(x)-b\right\rangle 
$$
The expression on the right is the same as the derivative of the function
$$
x \mapsto f(x,w) = 0.5*\|A(x)w-b\|^2,
$$ 
evaluated at $w=\tilde{w}(x)$, that is, $f(x,w)$ {\em for fixed w'}. 
The gradient of $\tilde{f}$ is the Riesz representer of the directional derivative:
$$
\langle s, \mbox{grad} \tilde{f}(x)\rangle = \frac{d}{dt}\tilde{f}(x+ts)|_{t=0}
$$
$$
= \left\langle s, DA(x)^*(\tilde{w}(x),A(x)\tilde{w}(x)-b)\right\rangle 
$$
in which $DA(x)^*$ denotes the {\em partial adjoint} in the second (nonlinear) argument, defined by
$$
\langle s, DA(x)^*(w,y) \rangle = \langle DA(x)(w,s),y\rangle
$$
That is, $y \mapsto DA(x)^*(w,y)$ is the adjoint of the map $s \mapsto DA(x)(w,s)$, the latter being the derivative of $x \mapsto A(x)w$. 

Thus
$$
\mbox{grad} \tilde{f}(x) = DA(x)^*(\tilde{w}(x),A(x)\tilde{w}(x)-b).
$$

In fact, this is also the gradient of a least-squares objective.
For a fixed choice $w \in W$, define $F_w(x) = A(x)w$. Then $DA(x)^*(w,y) = DF_w(x)^Ty$. Moreover, if $f_w(x) = 0.5*\|F_w(x)-b\|^2$, then 
$$
\mbox{grad} f_w(x)|_{w=\tilde{w}(x)} =  \mbox{grad}\tilde{f}(x).
$$
From the preceding section,
$$
\mbox{grad} f_w(x) = DF_w(x)^T(F_w(x)-b).
$$ 
Therefore computing the gradient of the VP reduction can be accomplished by combining a computation of the gradient of a nonlinear least-squares objective with a solution of the normal equation.

What's more, $x$ is a stationary point of the VPM objective $\tilde{f}$ if and only if $(x,\tilde{w})$ is a stationary point of the original objective $f$.

\subsection{Computing the gradient}
To compute the gradient of $\mbox{grad} \tilde{f}(x)$,

1. Calculate the minimizer 
\begin{equation}
  \label{eqn:g1}
\tilde{w}(x) = \mbox{argmin}_w 0.5*|A(x)w-b|^2
\end{equation}
and the optimal residual
\begin{equation}
  \label{eqn:g2}
  \tilde{r}(x) = A(x)\tilde{w}(x) - b.
\end{equation}

2.  Then
$$
\mbox{grad} \tilde{f}(x) = D_x(A(x)w)^T_{w=\tilde{w}(x)} \tilde{r}(x).
$$
\begin{equation}
  \label{eqn:g3}
= DA(x)^*(\tilde{w}(x),\tilde{r}(x))
\end{equation}

\noindent {\bf Notes:}
1. Note again that the gradient is identical to the gradient of the fixed-$w$ nonlinear least squares objective $\mbox{grad}_{x} f(x,w)$ with $w$ set equal to $\tilde{w}(x)$.

2. For large-scale problems, iterative solutions of the minimization \ref{eqn:g1} is likely imperative, hence $\tilde{w}(x)$ will be an approximation.

3. Software for least-squares problems often returns the residual as in equation \ref{eqn:g2} as a by-product of the minimization \ref{eqn:g1}.

\section{The Kaufman Hessian Approximation}

Of course $\tilde{f}$ is itself a nonlinear least squares objective: if you define $F(x)=A(x)\tilde{w}(x)$, then 
$$
\tilde{f}(x) = 0.5*\|F(x)-b\|^2.
$$
so it is natural to use the Gauss-Newton algorithm to minimize $\tilde{f}$. The Gauss-Newton step $s$ solves $DF(x)^T(DF(x)s-(F(x)-b))=DF(x)^TDF(x)s+\mbox{grad}\tilde{f}(x)=0$. This is a simplification over the Newton step, but for the special case of the VP reduction can be simplified still further.
$$
DF(x)s = D(A(x)\tilde{w}(x))s = DA(x)(\tilde{w}(x),s) + A(x)D\tilde{w}(x)s
$$
From the differentiability analysis of $\tilde{w}$,
$$
D\tilde{w}(x)s = 
$$
$$
=-(A(x)^TA(x))^{-1}(DA(x)^T(A(x)((A(x)^TA(x))^{-1}A(x)^Tb),s)
$$
$$
+ A(x)^T DA(x)((A(x)^TA(x)^{-1}A(x)^Tb,s) + (A(x)^TA(x)^{-1}DA(x)^T(b,s)|
$$
$$
= -(A(x)^TA(x))^{-1}[DA(x)^T(A(x)\tilde{w}(x),s) + A(x)^TDA(x)(\tilde{w}(x),s)]
$$
$$
+ (A(x)^TA(x))^{-1}DA(x)^T(b,s)
$$
So 
$$
DF(x)s = DA(x)(\tilde{w},s) - A(x) (A(x)^TA(x))^{-1}
$$
$$
\times [DA(x)^T(A(x)\tilde{w}(x),s) + A(x)^TDA(x)(\tilde{w}(x),s)] + A(x)(A(x)^TA(x))^{-1}DA(x)^T(b,s)
$$
$$
= (I-A(x)(A(x)^TA(x))^{-1}A(x)^T)DA(\tilde{w}(x),s)
$$
$$
+ A(x)(A(x)^TA(x))^{-1}DA(x)^T(b-A(x)\tilde{w}(x),s)
$$
The second term has the residual $b-A(x)\tilde{w}(x)=b-F(x)$ as the first argument of the bilinear operator $DA(x)^T$. Kaufman first pointed this out in 1974, and proposed that this term be dropped with the same justification as underlies the transition from Newton to Gauss-Newton: that is, if the residual is small (nearly noise-free data and close to the solution), this term should be negligible. Accepting this proposal, obtain 
$$
DF(x)s \approx (I-A(x)(A(x)^TA(x))^{-1}A(x)^T)DA(x)(\tilde{w}(x),s)
$$
$$
= P(x)DA(x)(\tilde{w}(x),s)
$$
where $P(x)=I-A(x)(A(x)^TA(x))^{-1}A(x)^T$ is the projection of $Y$ onto the orthocomplement of the range of $A(x)$, introduced earlier.
Since $P(x)$ is a projection, it is symmetric, positive semi-definite, and idempotent, that is $P(x)^TP(x)=P(x)^2=P(x)$. Thus the Gauss-Newton operator is approximately
\begin{equation}
  \label{eqn:kauf}
DF(x)^TDF(x)s \approx \mbox{Hess}_{VP}\tilde{f}(x)s = DA(x)^*(\tilde{w}(x),P(x)DA(x)(\tilde{w}(x),s)).
\end{equation}
The right-hand side of this equation is the Kaufman Hessian approximation.

The solution $s$ of the modified Newton step equation
\begin{equation}
  \label{eqn:NewtVP}
  \mbox{Hess}_{VP}\tilde {f}(x)s=-\mbox{grad}\tilde{f}(x)
\end{equation}
is a descent (or at least non-ascent) direction for $\tilde{f}$:
$$
\langle \mbox{grad}\tilde{f}(x), s \rangle 
= -\langle \mbox{Hess}_{VP}\tilde{f}(x)s, s\rangle 
$$
$$
=-\langle DA(x)^*(\tilde{w}(x),P(x)DA(x)(\tilde{w}(x),s)),s\rangle
$$
$$
= - \langle DA(x)(\tilde{w}(x),s),P(x)DA(x)(\tilde{w}(x),s)\rangle
$$
$$
= -\|P(x)DA(x)(\tilde{w}(x),s)\|^2 \le 0
$$
since $P(x)$ is a projector. 

\subsection{Computing the Kaufman Hessian}

To calculate the Kaufman Hessian approximation $\mbox{Hess}_{VP}\tilde{f}(x)s$ for a direction vector $s$ at position $x$,

1. Calculate the minimizer $\tilde{w}(x)$ and residual $\tilde{r}(x)$ per equations \ref{eqn:g1} and \ref{eqn:g2};

2. Compute the auxiliary minimizer
\begin{equation}
  \label{eqn:h1}
\tilde{v}(x,s) = \mbox{argmin}_v 0.5*\|A(x)v-DA(x)(\tilde{w}(x),s)\|^2
\end{equation}
and auxiliary residual
\begin{equation}
  \label{eqn:h2}
  \tilde{q}(x,s) = A(x)\tilde{v}(x,s) - DA(x)(\tilde{w}(x),s).
\end{equation}

3. Then
\begin{equation}
  \label{eqn:h3}
  \mbox{Hess}_{VP}\tilde{f}(x)s = DA(x)^*(\tilde{w}(x),A(x)\tilde{q}(x,s)).
\end{equation}

\section{Krylov-Newton optimization of VP reduced objectives}
The numerical analysis literature on VPM has mostly discussed the cost based on direct solution of the Newton equation, with all of the linear operators appearing above represented as matrices and the linear systems solved by Gaussian elimination \cite[]{Kaufman:75,GolubPereyra:03}. In this case, each Newton iteration has approximately the same cost, so that $N_N$ Newton iterations have cost proportional to $N_N$. This direct approach is (by definition!) not practical for large-scale problems.

Alternatives to direct Newton solves are generalized secant or quasi-Newton iteration, replacing the Hessian by a low-rank approximation that can be explicitly inverted (\cite[]{NocedalWright}), and Newton-Krylov iteration in which the Newton equation \ref{eqn:NewtVP} is solved by an iterative (Krylov) method. The quasi-Newton approach ia most popular for VPM-reduced large scale problems, since it requires neither explicit formation, application, nor inversion of the Hessian (for instance \cite{Warner:16}. The value and gradient are required at each quasi-Newton step, and these can be computed by completion of a single inner iteration, as noted above. If this inner iteration requires $N_I$ applications of the operator $A(x)$, then the total cost is proportional to $N_NN_I$.

Newton-Krylov iteration for nonlinear least squares problems uses iterative solution of the Newton equation \ref{eqn:NewtVP} via a Krylov subspace algorithm, for example conjugate gradient iteration \cite[]{Ghattas:IP25}. If $N_M$ iterations are required for the Newton step, then the total cost is proportional to $N_NN_M$.  For some least-squares problems based on solution of partial differential equations (simulation-driven optimization), the Newton-Krylov approach can be superior in both quality of output and cost to the quasi-Newton approach \cite[]{Metivier:2014}. $N_M$ and $N_I$ are similar for many problems, but $N_N$ can be substantially smaller for Newton-Krylov than for a quasi-Newton algorithm.

To employ the Newton-Krylov approach in the context of VPM, each step in the Krylov iteration requires solution of the Newton equation \ref{eqn:NewtVP}. Accomplishing this task by a Krylov iteration as well, as would presumably be necessary for large scale problems, involvs a number $N_H$ of applications of $A(x)$ and its transpose for each step. Thus the total cost is roughly proportional to $N_NN_MN_H$. If $N_M$ and $N_H$ are both roughly the same as the number of Krylov iterations $N_I$ required for solution of the inner least squares problem \ref{eqn:g1}, then the cost is proportional to $N_NN_I^2$, which appears to compare unfavorably to the cost of the quasi-Newton approach. This apparent cost penalty makes Newton-Krylov with the Kaufman Hessian unattracive, or even infeasible.

For some simulation-driven optimization problems, a way around this difficulty might be found in specific choices of norm in the quadratic model space ($W$) and possibly in the data Hilbert space $Y$, making $A(x)$ approximately orthogonal: $A(x)^TA(x) \simeq I$ (the change in norms changes the meaning of the transpose or adjoint, of course). Computable norms with this property have been constructed for a number of simulation driven optimization problems based on wave propagation, and used to accelerate convergence of Newton-llike methods \cite[]{HouSymes:16,Herve2017,DafniSymes:SEG18b}. For application to the VPM-reduced problem, the projector $P(x)$ (definition \ref{eqn:proj}) in the Kaufman Hessian formula \ref{eqn:kauf} is $\approx I-A(x)A(x)^T$. Evaluation of this approximation requires no linear system soluition, and the cost estimate for VPM-Newton-Krylov collapses to $N_nN_M$, that is, the same as Newton-Krylov applied to a similar nonlinear least squares problem.

In practice, some modifications are necessary, for example to ensure that the approximate Hessian is positive definite. Also, for any approaches based on iterative solution of the inner problem \ref{eqn:g1}, some criterion must be supplied to stop the iteration. Further, the accuracy with which $\tilde{w}(x)$ is computed affects the accuracy of the computed gradient $\mbox{grad} \tilde{f}(x)$ and any version of the Hessian, hence the rate (or even existence) of convergence of any Newton-like method. All of these issues require further attention.
\bibliographystyle{seg}
\bibliography{../../bib/masterref}



