{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook overviews a Python library based on \"Object structure of vector calculus\", hereinafter *VC*. The central thrust of VC is that a mimetic approach to vector calculus, with computation imitating mathematics as closely as possible, is necessarily object-oriented. The essential objects are also structurally constrained.\n",
    "\n",
    "The core mathematical types of vector calculus are normed vector spaces, vectors, linear operators or maps, and differentiable functions. My particular objective is mimetic expression of continuous optimization algorithms, which generally presume an inner product, so norms are assumed here to be inner product norms. This notebook reviews a Python realization of the structure explained in VC, with a few simple examples based on NumPy. \n",
    "\n",
    "# Spaces and Vectors\n",
    "\n",
    "As explained in VC, (pre-Hilbert) vector spaces combine a set of data objects with two operations on them, linear combination and inner (dot) product. The set of data objects has infinite cardinality except in one obvious instance, so is not computationally realizable. However it is sufficient to be able to determine whether an object is a member of the set, and to obtain a new object in the set on request (\"let $v \\in V$\"). These observations translate into four pseudo-code attributes of a Space object:\n",
    "1. a boolean function that takes an object argument and returns True if the argument refers to a data object for this space;\n",
    "2. a function with no arguments that returns a new data object;\n",
    "3. a linear combination function that evaluates $y=ax + by$, with arguments $a$, $b$ (scalars), $x$, and $y$ (data objects). Error if either $x$ or $y$ is not a valid data object;\n",
    "4. an inner product function that returns a scalar $\\langle x, y \\rangle$ for arguments $x$ and $y$. Error if either $x$ or $y$ is not a data object.\n",
    "\n",
    "All Spaces have these attributes, so the collection of Spaces is naturally expressed as an abstract class. In Python,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Space(ABC):\n",
    "    @abstractmethod\n",
    "    def isData(self,x):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getData(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def linComb(self, a, x, y, b=1.0):\n",
    "        pass\n",
    "    @abstractmethod    \n",
    "    def dot(self,x,y):\n",
    "        pass\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Space* class declaration in *vcl.py* includes several other convenient methods, including a self-description interface and a *cleanup* method to remove any part of a data object that Python garbage collection does not automatically remove.\n",
    "\n",
    "The almost-universal mathematical vernacular calls the data objects of a vector space \"vectors\". As explained in VC, this is logically incorrect, but also misleading: data objects must be combined with the other attributes of their vector space to act functionally as vectors. So a vector is not a data object alone, but a composite of a data object *together with* a vector space with its linear combination and inner product functions, for which the data object belongs to its proper set of data objects.\n",
    "\n",
    "While *vcl.Space* is an abstract type, asserting behaviour but not implementing it, every aspect of vector behaviour is determined by attributes of the corresponding space. So the Python realization *vcl.Vector* is a concrete, rather than abstract, class - its attributes are defined, not merely declared:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector:\n",
    "    def __init__(self, sp):\n",
    "        self.space = sp\n",
    "        self.data = sp.getData()\n",
    "    def __del__(self):\n",
    "        self.space.cleanup(self.data)            \n",
    "    def linComb(self,a,x,b=1.0):\n",
    "        self.space.linComb(a,x.data,self.data,b)\n",
    "    def dot(self,x):\n",
    "        return self.space.dot(self.data,x.data)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full definition in *vcl.py* includes a mechanism for assigning a *Vector* to an existing data object, rather than the new data object assigned to it on construction. This possibility is convenient in applications. Several other convenience attributes are also provided.\n",
    "\n",
    "For a simple example, I will construct a vector space based on NumPy. This choice makes a point: NumPy is already a fine environment for matrix algebra. However it does not offer interfaces for functions on subsets of vector spaces, nor for expression of algorithms defined in terms of vector functions, such as Newton's method. NumPy's array manipulation capabilities support straightforward construction of a vector space type, in the form of a *Space* as defined (partly) above. The key choice is use of NumPy *ndarrays* as data objects. Each space corresponds mathematically to ${\\bf R}^n$, so is characterized by its dimension. So an object is a data object of an *npSpace* if it is an column *numpy.ndarray* of the right dimension. Note that the *ndarray* is NOT a vector - it only becomes one when wrapped up with the appropriate linear algebra operations, as is done in *npvc.Space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vcl\n",
    "import numpy as np\n",
    "\n",
    "# numpy vector space\n",
    "class npSpace(vcl.Space):\n",
    "    def __init__(self,n):\n",
    "        self.dim = n\n",
    "    def getData(self):\n",
    "        return np.zeros(self.dim).reshape(self.dim,1)\n",
    "    def isData(self,x):\n",
    "        return (isinstance(x,np.ndarray) and x.shape() == (self.dim,1))\n",
    "    def linComb(self,a,x,y,b=1.0):\n",
    "        y = a*x + b*y\n",
    "    def dot(self,x,y):\n",
    "        return np.dot(x.T,y)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*npSpace* is a sub- (or derived) class of *Space*, as indicated by the first line in the definition. So it can be used in any context that calls for a *Space*.\n",
    "\n",
    "The full definition in *npvc.py* includes several other useful functions, that are (or could be) defined in terms of the core functions described above. The *lincomb* and *dot* functions (along with the others) are also written to provide error messages on failure.\n",
    "\n",
    "Here is a very simple use of the *npSpace* and *Vector* classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vector in 2D NumPy-based Space:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vcl\n",
    "import npvc\n",
    "\n",
    "dom = npvc.Space(2)\n",
    "x=vcl.Vector(dom)\n",
    "x.data[0]=1\n",
    "x.data[1]=1\n",
    "print('A vector in 2D NumPy-based Space:')\n",
    "x.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example, simple as it is, makes two important points. First, there are NumPy *Space*s (namely instances of *npvc.Space*), but there is no NumPy *Vector*: there are only *Vector*s in *npvc.Space*s. The vector concept does not need to be specialized: it is a \"function\" of the choice of space, and is otherwise completely defined.\n",
    "\n",
    "Second, a feature characteristic of all uses of VCL: its classes and functions manipulate data, but some data has to be initialized by external means. In this case, it's the very simple use of Python assignment to set the components of the data object (a NumPy *ndarray*). In more complex examples, external initialization is correspondingly more complex: for example, VCL applications based on RSF will need RSF utilities to initialize key data objects (RSF file pairs), which are input to VCL-coded processes.\n",
    "\n",
    "# Linear Operators\n",
    "\n",
    "The third fundamental concept of linear algebra, after vector space and vector, is that of linear map or operator. These are simply functions whose domains and ranges are vector spaces, and which satisfy the linearity condition. Since the vector spaces at issue are presumed to be inner product spaces, linear operators really come in adjoint pairs.\n",
    "\n",
    "A linear operator should be able to identify its domain and range spaces, and apply itself to a vector, producing vector output. The obvious methods to identify domain and range are functions returning these as *vcl.Space* s. In the class *vcl.LinearOperator* these methods are named *getDomain* and *getRange*. While left abstract in the class constructed here, they are naturally implemented by simply returning stored references to the domain and range spaces.\n",
    "\n",
    "Operarator application (or evaluation) is a bit more subtle. The usual mathematical syntax for linear operator application is juxtaposition: if $A$ is a linear operator and $x$ is a vector in its domain, then the value of $A$ on $x$ is $Ax$. That syntax can't be reproduced precisely in code: Python needs some indication that a method is to be called. The most appropriate option appears to be asterisk, that is, the symbol that also signifies scalar multiplication (which is essentially a special case). Thus if *A* is a *vcl.LinearOperator* and *x* is an input *vcl.Vector* in its domain, the operator output is *A\\*x*. This syntax is available through *\\_\\_mul\\_\\_*, one of Python's \"magic methods\", which *overloads* the asterisk operator: essentially, it lets you (re)define a multiplication operator appropriate to a type you have defined, with the same syntax as scalar multiplication.\n",
    "\n",
    "If the *\\_\\_mul\\_\\_* method is to return the operator output, then the output object needs to be allocated, as part of the method definition. It's convenient to divide the evaluation task into two parts: allocation of the output object, and modification of its data object to contain the correct output data. The allocation part is always accomplished by the same code, namely the vector constructor (remember, *vcl.Vector* is a fully defined class!). Computation of output data is peculiar to the particular operator, so is natually isolated in an abstract function, which the *\\_\\_mul\\_\\_* method calls.\n",
    "\n",
    "These considerations lead to the (partial) abstract class definition. Note that *vcl.LinearOperator* is a subclass of *vcl.Function*, for the obvious logical reason; the *vcl.Function* class is described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearOperator(vcl.Function):\n",
    "    @abstractmethod\n",
    "    def getDomain(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getRange(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def applyFwd(self,x, y):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def applyAdj(self,x, y):\n",
    "        pass    \n",
    "    \n",
    "    def __mul__(self,x):\n",
    "        try:\n",
    "            if x.space != self.getDomain():\n",
    "                raise Exception('Error: input not in domain')\n",
    "            y = Vector(self.getRange())\n",
    "            self.applyFwd(x,y)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            raise Exception('called from vcl.LinearOperator *')\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of *\\_\\_mul\\_\\_* tests that the input vector is actually a member of the domain space. I have used Python exception handling to implement this test, and arranged the raised exceptions to offer a traceback message if invoked. This pattern is followed in all of the *vcl* classes: any obvious error conditions are tested, and exceptions raised as instances of the *Exception* class with information about the error and where it occurred. \n",
    "\n",
    "To pass the test, the input vector's *vcl.Space* data member must be a reference to *the same* object as is the value returned by the *getDomain* method. This choice implies that *vcl.LinearOperator*s will store references to externally defined domain and range spaces.\n",
    "\n",
    "The output (*y* in the listing) is *constructed* as a member of the range space, so there is no need to test its membership.\n",
    "\n",
    "The actual computations to apply the operator to the input vector are localized in the *applyFwd* method, to which are passed (already allocated) input and output vectors. It is assumed that the *applyFwd* method is used *only* in this way, so no further checks on membership in domain and range need be implemented in it. In C++ for example, this pattern could be enforced by identifying *applyFwd* as a private (or perhaps protected) method, not accessible to non-member objects. Since Python doesn't provide that kind of access control, this design depends on the usual rule for Python design: don't do anything stupid!\n",
    "\n",
    "As noted above, since all of the vector spaces considered in VCL are inner product spaces, linear operators occur (implicitly) as adjoint pairs. As will become apparent shortly, the natural location for the calculations required to apply the adjoint is also *vcl.LinearOperator*, in the form of the *applyAdj* method. As for *applyFwd*, *applyAdj* is not intended for direct use. \n",
    "\n",
    "Probably the simplest specialization of the linear operator class uses *npSpace* performs matrix-vector multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixOperator(vcl.LinearOperator):    \n",
    "    def __init__(self,dom,rng,mat):\n",
    "        self.dom = dom\n",
    "        self.rng = rng\n",
    "        self.mat = np.copy(mat)\n",
    "        #....    \n",
    "    def applyFwd(self,x, y):\n",
    "        y.data = self.mat@x.data\n",
    "    def applyAdj(self,x, y):\n",
    "        y.data = self.mat.T@x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elided code in the constructor checks that the *npSpaces* *dom* and *rng* have the number of columns of the *numpy.ndarray*, respectively its number of rows, as their dimensions, raising an exception if either is false.\n",
    "\n",
    "Note that the domain and range are passed as references to externallly defined *Space* objects that exist independently of the *MatrixOperator*, whereas the matrix argument is copied, internal to the object. This construction requires the error-checking just mentioned, but has several advantages. Obviously the domain and range spaces could also be copied, or even generated internally to the *MatrixOperator* object, but that would be logically incorrect as well as inconvenient. With the *MatrixOperator* storing references to externally defined spaces, membership in those spaces can be verified by simple comparison, as in the implementation of *\\_\\_mul\\_\\_* above. Note that it cannot be enough to check that the input vector and the domain have the same dimension. For example, subclasses of *npvc.Space* could be equipped with units or other auxiliary information necessary for their proper use. Only checking equality of dimensions could lead to dimensional errors. Insisting that the spaces involved should be *the same objects* avoids such egregious errors.\n",
    "\n",
    "The matrix, on the other hand, is naturally internal data. This construction maintains the identity of the *MatrixOperator* even if the NumPy array passed to the constructor is subsequently changed.\n",
    "\n",
    "Here is a simple example of matrix multiplication via the *npvc.MatrixOperator* class. The textual overhead from expressing matrix multiplication as the action of a linear operator (as compared to straight NumPy) is: 3 lines of code, out of 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector x\n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[1.]\n",
      " [1.]]\n",
      "Matrix Operator matop\n",
      "NUMPY Matrix Operator with matrix:\n",
      "[[1. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]]\n",
      "domain:\n",
      "npvc.Space of dimension 2\n",
      "range:\n",
      "npvc.Space of dimension 3\n",
      "Output vector y = matop(x)\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[2.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vcl\n",
    "import npvc\n",
    "\n",
    "# domain space and vector in it\n",
    "dom = npvc.Space(2)\n",
    "x=vcl.Vector(dom)\n",
    "x.data[0]=1\n",
    "x.data[1]=1\n",
    "\n",
    "# range space\n",
    "rng = npvc.Space(3)\n",
    "\n",
    "# 3 x 2 matrix - \n",
    "mat=np.zeros((3,2))\n",
    "mat[0,0]=1\n",
    "mat[0,1]=1\n",
    "mat[1,1]=1\n",
    "\n",
    "# matrix operator based on mat\n",
    "matop=npvc.MatrixOperator(dom,rng,mat)\n",
    "\n",
    "# matrix-vector product as matrix operator \n",
    "# application\n",
    "y = matop*x\n",
    "\n",
    "print('Input vector x')\n",
    "x.myNameIs()\n",
    "print('Matrix Operator matop')\n",
    "matop.myNameIs()\n",
    "print('Output vector y = matop(x)')\n",
    "y.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I emphasize that the output *y* of the matrix product operator *matop* is *created* by *matop*. You *could* create *y* before calling the operator evaluation, that is *y=vcl.Vector(rng); y=matop*x*.\n",
    "However, after the second statement, the variable y would refer to the data created internally by the * operator, and the data created by the first statement (*vcl.Vector* constructor) would be \"orphaned\", i.e. *y* no longer refers to it - no external references exist. So the data allocated in the first statement would be garbage-collected, never having been used. The first statement is redundant, and worse involves some wasted computational work (memory allocation, initialization).\n",
    "\n",
    "This is not just a peculiarity of the way Python handled variables - it is logically correct, and exactly parallels the corresponding mathematics. Suppose you were to write \"Let $y \\in Y$, and $y=Ax$\". The second statement already presumes that $y$ is in the range (must be $Y$) of the linear operator $A$, so the first statement is redundant - exactly as in the corresponding code.\n",
    "\n",
    "Access to the adjoint operator is provided through the *vcl.transp* class. This class takes a *vcl.LinearOperator* as argument to its constructor, which returns another *vcl.LinearOperator*. This latter implements the adjoint (transpose) operator by accessing the methods of the argument, especially the *applyAdj* method. The methods are arranged so that *transp(transp(A))* duplicates the action of *A*, as one would hope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector z = transp(matop)(y)\n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[2.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "# apply transpose of matop to y\n",
    "z = vcl.transp(matop)*y\n",
    "print('Output vector z = transp(matop)(y)')\n",
    "z.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Having constructed a class for linear operators (i.e. linear functions), it's obvious how to build a class for (possibly) non-linear functions. First, absent linearity there is no natural adjoint concept, so there is only a \"forward\" application function. Second, for differentiable functions a derivative (a linear operator) is another attribute. \n",
    "\n",
    "These considerations suggest a very simple abstract interface. The natural syntax for evaluation of the function $F$ at $x$ is $F(x)$. As was the case with linear operators, there is a Python \"magic method\" interface for function call syntax, which incorporates sanity checking. The computations specific to an instance are relegated to the abstract *apply* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import vcl\n",
    "\n",
    "class Function(ABC):\n",
    "    @abstractmethod\n",
    "    def getDomain(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def getRange(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def apply(self,x,y):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,x):\n",
    "        try:\n",
    "            if x.space != self.getDomain():\n",
    "                raise Exception('Error: input vec not in domain')\n",
    "            y = vcl.Vector(self.getRange())\n",
    "            self.apply(x,y)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            raise Exception('called from vcl.Function operator()')\n",
    "        else:\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to the derivative is through a method *deriv*, which should return a *vcl.LinearOperator*. Error-checking goes as before, but the actual computations are specific to individual subtypes, so an abstract interface *raw_deriv* is provided. As is the case with *apply*, *raw_deriv* is not intended for direct use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # should return linear op\n",
    "    @abstractmethod\n",
    "    def raw_deriv(self,x):\n",
    "        pass\n",
    "    \n",
    "    def deriv(self,x):\n",
    "        try:\n",
    "            if x.space != self.getDomain():\n",
    "                raise Exception('Error: input vec not in domain')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            raise Exception('called from vcl.Function.deriv')\n",
    "        else:        \n",
    "            return self.raw_deriv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple NumPy-based example is provided as *npvc.OpExpl1*. It realizes the function $f: {\\bf R}^2 \\rightarrow {\\bf R}^3$ given by \n",
    "$$\n",
    "f((x_0,x_1)^T) = (x_0*x_1, -x_1+x_0^2, x_1^2)^T.\n",
    "$$\n",
    "Its code is written according the principles outlined above. For instance, the domain and range spaces are constructed externally to the function object, and passed to its constructor as arguments. Their dimensions are checked to be 2 and 3 respectively. The function object stores references to these externally defined spaces. The *\\_\\_call\\_\\_* and *deriv* methods sanity-check their arguments. See the code for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vector:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[ 1.]\n",
      " [-2.]]\n",
      "output of apply method:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-2.]\n",
      " [ 3.]\n",
      " [ 4.]]\n",
      "output of deriv method:\n",
      "NUMPY Matrix Operator with matrix:\n",
      "[[-2.  1.]\n",
      " [ 2. -1.]\n",
      " [ 0. -4.]]\n",
      "domain:\n",
      "npvc.Space of dimension 2\n",
      "range:\n",
      "npvc.Space of dimension 3\n"
     ]
    }
   ],
   "source": [
    "dom = npvc.Space(2)\n",
    "rng = npvc.Space(3)\n",
    "f = npvc.OpExpl1(dom,rng)\n",
    "x = vcl.Vector(dom)\n",
    "x.data[0]=1\n",
    "x.data[1]=-2\n",
    "print('input vector:')\n",
    "x.myNameIs()\n",
    "y = f(x)\n",
    "print('output of apply method:')\n",
    "y.myNameIs()\n",
    "dfx = f.deriv(x)\n",
    "print('output of deriv method:')\n",
    "dfx.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth pointing out a feature also present in the previous example. While the input vector (*x* in both examples) is constructed and initialized externally to the *Function*, the output vector (*y*) is constructed and initialized internally, and returned to the calling environment by assignment. The same goes for the derivative, represented by a *LinearOperator*. Of course *x* also appears in the environment as the left-hand side of an assigment, namely the *Vector* constructor.\n",
    "\n",
    "Linear functions (operators) are also functions, so really *LinearOperator* should subclass *Function*. The derivative is a linear operator, so the definition of *Function* might seem to depend of the definition of *LinearOperator* which in turn depends on *Function*. However it is possible to take advanage of Python's genericity to break this cyclic dependence: the return type of the *Function.deriv* method isn't specified, because return types are not specified in Python! The *raw_deriv* method is implemented for *LinearOperator*, and simply returns a reference to the object itself, since linear operators are their own derivatives. Also, the *Function.apply* method is implemented by delegation to *applyFwd*. So the function call sytax also works for *LinearOperator*s, though the asterisk syntax is preferable for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jets\n",
    "\n",
    "The *jet* concept solves an obvious consistency vs. efficiency problem involving the attributes of functions. Suppose that *f* is a *vcl,Function*, and *x* is a *vcl.Vector*. Record the function's value in the *vcl.Vector* *y*, and the derivative in the *vcl.LinearOperator* *df*. Some number of lines later in the program, access these same objects. Is there any guarantee that they are still related in the same way? In fact, there is not. If *x* is changed but the function is not re-evaluated, then the three objects have become inconsistent. The only way to make sure that this inconsistency does not occur appears to be re-computing the value and derivative each time they are accessed. That could amount to considerable wasted computational effort.\n",
    "\n",
    "Jets, in the sense meant here, borrow a concept from differential geometry, and offer a way around this problem. There are several equivalent definitions, of which I cite the one most relevant to computation. The $k$-jet of a $C^{\\infty}$ function $f$ at a point $x$ in its domain is the sequence of values $D^{\\alpha}f(x)$ of $f$ and all of its derivatives of orders $|\\alpha| \\le k$. This definition suggests an obvious container class, implemented in $vcl.Jet$. For the time being, I have implemented only the 1-jet. The constructor arguments are the *vcl.Function* *f* and *vcl.Vector* *x*. The methods *getPoint*, *getValue*, and *getDeriv* return the computational analogues of $x$, $f(x)$, and $Df(x)$ respectively. These are stored as internal copies, so changing *x* after creation of *vcl.Jet(f,x)* does not change the return values of these methods. Thus an instance of *vcl.Jet* provides access to a consistent set of values.\n",
    "\n",
    "An earlier implementation of this concept in the C++ library RVL used access control and the *const* keyword to prevent any violation of the jet's internal data, so really offered a guarantee that the return values of its methods are coherent. Such guarantees are impossible to provide in Python, which does not implement *const* and makes all class data public. So *vcl.Jet* really only provides guidance to help the programmer maintain the coherence of function values. Vector data is also private in RVL, and can only be altered through the action of a few specified function classes. This restriction makes a *versioning* system possible. The jet methods compare the version index of a vector with a recorded index to tell whether the vector had been altered, thus update values and derivatives automatically whenever necessary. So RVL implements the jet concept as *f(x) for variable x*. Such automation is impossible in a Python framework: the VCL user is responsible for updating *vcl.Jet* instances as needed. \n",
    "\n",
    "Partly for this reason, I have not used the Jet class provided in *vcl* in formulating algorithms, at least so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Functions\n",
    "\n",
    "Scalar-valued functions are central players in optimization, but the classes described so far do not encompass them. ${\\bf R}$ can be identified with a 1-D vector space over the reals, but it is not *the same* as that vector space. This is even more so in the computational context: *float* objects are not interchangeable with 1-D *ndarray*s. In VCL, the latter are data of *Vector*s, not themselves *Vector*s. So there are a couple of conceptual layers between scalars and vectors, and scalar valued functions require a their own proper class.\n",
    "\n",
    "The analogue of *Function.apply* is *ScalarFunction.value*. Unlike  *apply*, *value* simply returns a value, rather tha alteraing an argument. Like *apply*, it is intended for \"private\" use, with sanity testing done by the *\\_\\_call\\_\\_* method. So the rule is: \n",
    "\n",
    "for a scalar function class *fun*, implement *fun.value*. For an instance *f* of *fun*, call *f(x)* (not *f.value(x)*). \n",
    "\n",
    "Another feature of this class deserving of mention is the representation of the derivative by the gradient, its Riesz representer. Thus *ScalarFunction.gradient* replaces *Function.deriv*. As in the *Function* case, there is a \"raw\" version (*raw_gradient*) to be implemented, whereas the \"cooked\" version *gradient* (with standard sanity test) is to be used in code.\n",
    "\n",
    "Because it occurs so often, I include a definition of the standard least-squares function\n",
    "$$\n",
    "J(x) = \\|F(x)-y\\|^2\n",
    "$$ \n",
    "in which $F: X \\rightarrow Y$ is a map between Hilbert spaces $X$ and $Y$, $y \\in Y$, and $J: X \\rightarrow {\\bf R}$. \n",
    "\n",
    "(Of course, there's nothing \"least\" about it, but minimizing $J$ is the standard least-squares optimization problem, and the objective function is stuck with the same name, in the vernacular.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function x -> 0.5*|f(x)-b|^2\n",
    "class LeastSquares(vcl.ScalarFunction):\n",
    "    def __init__(self,f,b):\n",
    "        self.f = f\n",
    "        self.b = b\n",
    "    def value(self,x):\n",
    "        res = self.f(x)\n",
    "        res.linComb(-1,0,self.b)\n",
    "        return 0.5*res.dot(res)\n",
    "    def raw_gradient(self,x):\n",
    "        res = self.f(x)\n",
    "        res.linComb(-1,0,self.b)\n",
    "        df = self.f.deriv(x)\n",
    "        return vcl.transp(df)*res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example using once again the function *npvc.OpExpl1* Note that *J(x)* and *J.gradient(x)* appear in this \"application\", rather than *J.value(x)* and *J.raw_gradient(x)*, in order that the input be checked for membership in the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares Function\n",
      "*** operator:\n",
      "OpExpl1: npvc example of vcl.Function class\n",
      "implements (x0,x1) -> (x0*x1, -x1+x0^2, x1^2)\n",
      "domain = R^2, range = R^3\n",
      "*** rhs vector\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[ 3.]\n",
      " [ 2.]\n",
      " [-3.]]\n",
      "input vector x:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[ 1.]\n",
      " [-2.]]\n",
      "J.value(x) =37.5\n",
      "J.gradient(x) = \n",
      "Vector in space:\n",
      "npvc.Space of dimension 2\n",
      "Data object:\n",
      "[[ 12.]\n",
      " [-34.]]\n"
     ]
    }
   ],
   "source": [
    "dom = npvc.Space(2)\n",
    "rng = npvc.Space(3)\n",
    "f = npvc.OpExpl1(dom,rng)\n",
    "x = vcl.Vector(dom)\n",
    "y = vcl.Vector(rng)\n",
    "y.data[0]=3\n",
    "y.data[1]=2\n",
    "y.data[2]=-3\n",
    "x.data[0]=1\n",
    "x.data[1]=-2\n",
    "J = vcl.LeastSquares(f,y)\n",
    "J.myNameIs()\n",
    "print('input vector x:')\n",
    "x.myNameIs()\n",
    "print('J.value(x) =' + str(J(x)))\n",
    "g=J.gradient(x)\n",
    "print('J.gradient(x) = ')\n",
    "g.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Spaces and Partial Derivatives\n",
    "\n",
    "Most scientific data lives in (Cartesian) product spaces, and they turn up in lots of other ways. The computational realization *ProductSpace* simply makes a list of *Space* objects behave as a *Space*. A *ProductSpace* data object is naturally a list of data objects, one for each *Space* factor. *ProductSpace* methods implement standard induced operations: linear combinations are lists of linear combinations, dot product is the sum of dot products, etc. The list of *Space* factors is available as the *spl* data member - which can be queried for length, individual factors, etc. since it's publicly accessible, like all class data in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vcl.ProductSpace\n",
      "*** component 0\n",
      "npvc.Space of dimension 1\n",
      "*** component 1\n",
      "npvc.Space of dimension 1\n",
      "Vector in space:\n",
      "vcl.ProductSpace\n",
      "*** component 0\n",
      "npvc.Space of dimension 1\n",
      "*** component 1\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[1.]]\n",
      "[[-2.]]\n"
     ]
    }
   ],
   "source": [
    "dom1=npvc.Space(1)\n",
    "dom2=npvc.Space(1)\n",
    "spacelist=[dom1,dom2]\n",
    "pdom=vcl.ProductSpace(spacelist)\n",
    "pdom.myNameIs()\n",
    "x=vcl.Vector(pdom)\n",
    "x.data[0][0]=1\n",
    "x.data[1][0]=-2\n",
    "x.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of this example with the preceding one reminds the reader that $R^2$ is not *the same* as $R^1 \\oplus R^1$ - the two are isomorphic, but not identical. This mathematical truth is precisely reflected in the computational structure. Both examples construct a vector whose data array(s) has (have) two components with values -1 and 2, but in the first example it's an array reals of length 2, whereas in the second it's an array of length 2 of real arrays of length 1.\n",
    "\n",
    "A further consequence of this reasoning: there is no such thing as a \"product vector\". Instead, there are vectors in product spaces. However, vectors in product spaces have components, each of which is a vector. Also, product spaces have components, each of which is a space. The \"magic method\" *\\_\\_getitem\\_\\_* provides access via the usual indexing interface (operator[]) in each case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector in space:\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[1.]]\n",
      "Vector in space:\n",
      "vcl.ProductSpace\n",
      "*** component 0\n",
      "npvc.Space of dimension 1\n",
      "*** component 1\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[2.]]\n",
      "[[-2.]]\n"
     ]
    }
   ],
   "source": [
    "#x0=x.component(0)\n",
    "x0 = x[0]\n",
    "x0.myNameIs()\n",
    "x0.data[0]=2\n",
    "x.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function on a product space may (or may not) be provided with partial derivatives - that's one of those features that are implicit in the mathematical concept, but must be added explicitly to its computational homolog. If it is, the derivative is implemented as a *vcl.RowLinearOperator*, which provides a *vcl.LinearOperator* interface for a list of *vcl.LinearOperator* s. The individual operators making up a *RowLinearOperator* are accessed by index via the indexing operator[], another instance of *\\_\\_getitem\\_\\_*. Thus the *i*th partial derivative of *f* at *x* is *f.deriv(x)[i]*.\n",
    "\n",
    "I have modified *npvc.OpExpl1* to make the domain $R^1 \\oplus R^1$ rather than $R^2$, and implemented the derivative as a *vcl.RowLinearOperator*. This change costs a couple of extra lines of code to build up the list of partial derivatives. Note that the output of the derivative, applied to a particular vector, is the same as the sum of the partial derivatives applied to its components, as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input vector:\n",
      "Vector in space:\n",
      "vcl.ProductSpace\n",
      "*** component 0\n",
      "npvc.Space of dimension 1\n",
      "*** component 1\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[1.]]\n",
      "[[-2.]]\n",
      "output of apply method:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-2.]\n",
      " [ 3.]\n",
      " [ 4.]]\n",
      "output of deriv method:\n",
      "RowLinearOperator length = 2\n",
      "*** Component 0:\n",
      "NUMPY Matrix Operator with matrix:\n",
      "[[-2.]\n",
      " [ 2.]\n",
      " [ 0.]]\n",
      "domain:\n",
      "npvc.Space of dimension 1\n",
      "range:\n",
      "npvc.Space of dimension 3\n",
      "*** Component 1:\n",
      "NUMPY Matrix Operator with matrix:\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-4.]]\n",
      "domain:\n",
      "npvc.Space of dimension 1\n",
      "range:\n",
      "npvc.Space of dimension 3\n",
      "input to deriv\n",
      "Vector in space:\n",
      "vcl.ProductSpace\n",
      "*** component 0\n",
      "npvc.Space of dimension 1\n",
      "*** component 1\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[2.]]\n",
      "[[-3.]]\n",
      "output of deriv\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-7.]\n",
      " [ 7.]\n",
      " [12.]]\n",
      "input of partial deriv 0\n",
      "Vector in space:\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[2.]]\n",
      "output of partial deriv 0\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-4.]\n",
      " [ 4.]\n",
      " [ 0.]]\n",
      "input of partial deriv 1\n",
      "Vector in space:\n",
      "npvc.Space of dimension 1\n",
      "Data object:\n",
      "[[-3.]]\n",
      "output of partial deriv 1\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-3.]\n",
      " [ 3.]\n",
      " [12.]]\n",
      "sum of partial deriv outputs\n",
      "Vector in space:\n",
      "npvc.Space of dimension 3\n",
      "Data object:\n",
      "[[-7.]\n",
      " [ 7.]\n",
      " [12.]]\n"
     ]
    }
   ],
   "source": [
    "f = npvc.OpExpl2(pdom,rng)\n",
    "x.data[0][0]=1\n",
    "x.data[1][0]=-2\n",
    "print('input vector:')\n",
    "x.myNameIs()\n",
    "y=f(x)\n",
    "print('output of apply method:')\n",
    "y.myNameIs()\n",
    "dfx = f.deriv(x)\n",
    "print('output of deriv method:')\n",
    "dfx.myNameIs()\n",
    "dx=vcl.Vector(pdom)\n",
    "dx.data[0][0]=2\n",
    "dx.data[1][0]=-3\n",
    "print('input to deriv')\n",
    "dx.myNameIs()\n",
    "dy=dfx*dx\n",
    "print('output of deriv')\n",
    "dy.myNameIs()\n",
    "dy0=dfx[0]*dx[0]\n",
    "print('input of partial deriv 0')\n",
    "dx[0].myNameIs()\n",
    "print('output of partial deriv 0')\n",
    "dy0.myNameIs()\n",
    "dy1=dfx[1]*dx[1]\n",
    "print('input of partial deriv 1')\n",
    "dx[1].myNameIs()\n",
    "print('output of partial deriv 1')\n",
    "dy1.myNameIs()\n",
    "# sum the outputs of the partial derivs\n",
    "dy1.linComb(1.0,dy0)\n",
    "print('sum of partial deriv outputs')\n",
    "dy1.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "\n",
    "The main excuse for inventing this machinery is to make algorithms based on vector calculus easier to express. This section describes several examples.\n",
    "\n",
    "## Conjugate Gradient Algorithm for Linear Least Squares\n",
    "\n",
    "This algorithm is adapted from the Hestenes-Stiefel conjugate gradient algorithm for positive definite symmetric linear systems. It gives an approximate solution of the least-squares problem\n",
    "$$\n",
    "\\mbox{Given }A\\mbox{ and }b \\in B, \\,\\min_{x \\in X} \\|Ax-b\\|^2\n",
    "$$ \n",
    "in which $X$ and $B$ are Hilbert spaces and $A: X \\rightarrow B$ is a (bounded) linear operator. Assuming that $A$ is coercive (full column rank in the finite-dimensional case), $x$ is also the unique solution of the linear system (normal equation)\n",
    "$$\n",
    "A^TA x = A^Tb\n",
    "$$\n",
    "The conjugate gradient algorithm, as described in Golub and van Loan, sections 10.2 and 10.3; Nocedal and Wright, algorithm 5.2, can be applied directly to this system. The only difference between that algorithm and the one described here is the introduction of a vector variable ($q$ below) to avoid explicit construction of the normal operator $A^TA$. Hanke, section 2.3, describes essentially this algorithm.\n",
    "\n",
    "As I have written it here, five auxiliary vectors are required: $r, p, s \\in X$, and $e, q \\in B$. At each iteration, $e=b-Ax$ is the residual vector, $r=A^T(b-Ax)$ the normal residual vector. The iteration terminates when the length of either of these two vectors falls below a factor $\\epsilon, \\rho \\in (0,1)$ of its original length. \n",
    "\n",
    "The algorithm proceeds in two phases. In each of the following steps, the equality sign \"$=$\" represents assignment, that is, the right hand side is first evaluated, the overwritten on the left hand side.\n",
    "\n",
    "Initialization:\n",
    "\n",
    "1. $x = 0$\n",
    "2. $e = b$\n",
    "3. $r = A^Tb$\n",
    "4. $p = r$\n",
    "7. $\\gamma_0 = \\langle r, r \\rangle_X$\n",
    "8. $\\gamma = \\gamma_0$\n",
    "9. $k=0$\n",
    "    \n",
    "Iteration: Repeat while $k<k_{\\rm max}$, $\\|e\\|>\\epsilon \\|b\\|$, and $\\|r\\|>\\rho \\|A^Tb\\|$\n",
    "\n",
    "1. $q = Ap$\n",
    "2. $s = A^Tq$\n",
    "3. $\\alpha = \\gamma / \\langle q, q\\rangle_B$\n",
    "4. $x = x+\\alpha p$\n",
    "5. $e = e-\\alpha q$\n",
    "6. $r = r-\\alpha s$\n",
    "7. $\\delta = \\langle r, r \\rangle_X$\n",
    "8. $\\beta = \\delta / \\gamma$\n",
    "9. $p = r + \\beta p$\n",
    "10. $\\gamma = \\delta$\n",
    "11. $k = k+1$\n",
    "\n",
    "The translation into VCL code is straightforward. I pick out a few examples to illustrate how this goes, all from the iteration phase.\n",
    "\n",
    "(step 1) $q = Ap$ becomes *A.applyFwd(p,q)*\n",
    "\n",
    "(step 2) $s = A^Tq$ becomes *A.applyAdj(q,s)*\n",
    "\n",
    "(step 3) $\\alpha = \\gamma / \\langle q, q\\rangle_B$ becomes *alpha = gamma/q.dot(q)*\n",
    "\n",
    "(step 4) $x = x+\\alpha p$ becomes *x.linComb(alpha,p)* (*linComb* is \n",
    "often called *axpy*, standing for \"(y = ) a x plus y\")\n",
    "\n",
    "The complete algorithm (function *cg* in the module *vcalg*) includes several levels of screen output, from none to printing the norms of $e$ and $r$ at every iteration.\n",
    "\n",
    "Properties of the algorithm are described in the cited references and many others. Key facts:\n",
    "\n",
    "1. the residual (norm of $e$) decreases monotonically. \n",
    "\n",
    "2. the normal residual (norm of $r$) decreases eventually, but not monotonically (in general).\n",
    "\n",
    "3. for a system of dimension $n$, in exact arithmetic, the algorithm terminates at a solution of the normal equations in $n$ or fewer iterations. In floating point arithmetic, the residual in the normal equations is generally on the order of the square root of macheps in $n$ iterations. For very large problems, useful convergence is governed by the the distribution of eigenvalues of $A^TA$, not by the dimension. \n",
    "\n",
    "To illustrate some of these features, I constructed a least squares problem using *MatrixOperator* with 4-dimensional domain and 6-dimensional range. I built noise free data and solved the corresponding least squares problem using *vcalg.cg*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  k       |e|       |r|=\n",
      "  0  5.4772e+00  1.8815e+01\n",
      "  1  2.0912e+00  5.0185e+00\n",
      "  2  1.0929e+00  1.9103e+00\n",
      "  3  6.0569e-01  6.6167e-01\n",
      "  4  1.2545e-15  4.3724e-15\n",
      "----------------------------------------------------\n",
      "  k       |e|     |e|/|e0|      |r|        |r|/r0|\n",
      "  4  1.2545e-15  2.2903e-16  4.3724e-15  2.3239e-16\n",
      "\n",
      "solution vector:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 4\n",
      "Data object:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import vcl\n",
    "import npvc\n",
    "import vcalg\n",
    "import numpy as np\n",
    "\n",
    "# domain space and vector in it\n",
    "dom = npvc.Space(4)\n",
    "xstar=vcl.Vector(dom)\n",
    "xstar.data[0]=1\n",
    "xstar.data[1]=1\n",
    "xstar.data[2]=1\n",
    "xstar.data[3]=1\n",
    "\n",
    "# range space and vector in it\n",
    "rng = npvc.Space(6)\n",
    "\n",
    "# 3 x 2 matrix - initialize as outer product\n",
    "#mat=y.data@x.data.T\n",
    "mat=np.zeros((6,4))\n",
    "mat[0,0]=1\n",
    "mat[1,1]=2\n",
    "mat[2,2]=3\n",
    "mat[3,3]=4\n",
    "\n",
    "# matrix operator based on mat\n",
    "matop=npvc.MatrixOperator(dom,rng,mat)\n",
    "\n",
    "# initialize rhs\n",
    "b = matop*xstar\n",
    "\n",
    "# solution, residual, normal residual vectors (if desired)\n",
    "x = vcl.Vector(dom)\n",
    "\n",
    "\n",
    "# set cg parameters and run\n",
    "kmax=20\n",
    "eps=0.01\n",
    "rho=0.01\n",
    "vcalg.conjgrad(x=x, b=b, A=matop, kmax=kmax, eps=eps,\\\n",
    "               rho=rho, verbose=2)\n",
    "\n",
    "# view result\n",
    "print('\\nsolution vector:')\n",
    "x.myNameIs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Gauss-Newton Algorithm for Nonlinear Least Squares\n",
    "\n",
    "Suppose that $F:U \\rightarrow B$ is a (possibly nonlinear) function from an open subset $U$ of the Hilbert space $X$ to another Hilbert space $B$. \"Nonlinear least squares\" refers to the optimization problem,\n",
    "$$\n",
    "\\mbox{Given }b \\in B, \\,\\min_{x \\in U} J(x),\n",
    "$$ \n",
    "where $J(x)=0.5*\\|F(x)-b\\|^2$. Given a current estimate $x$ of the solution, Newton's method produces an update by using the gradient\n",
    "$$\n",
    "g(x) = DF(x)^T(F(x)-b)\n",
    "$$\n",
    "and Hessian\n",
    "$$\n",
    "H(x) = D(DF^T)(x)(F(x)-b) + DF(x)^TDF(x)\n",
    "$$\n",
    "by solving for the Newton step $s$: \n",
    "$$\n",
    "x \\leftarrow x+s; H(x)s = -g.\n",
    "$$\n",
    "The Gauss-Newton variant modifies the Hessian by dropping the first term. There are three reasons for doing this:\n",
    "\n",
    "1. If the residual $F(x)-b$ is small at the solution (so that data has small noise), then this term should be small when $x$ is close to the minimizer;\n",
    "\n",
    "2. Without that term, it is easy to see that $s$ is always a descent direction, assuming that $DF(x)$ has full column rank; and\n",
    "\n",
    "3. Without that term, it is not necessary to compute the second derivative of $F$.\n",
    "\n",
    "Also, the Gauss-Newton step is the solution of the linear least squares problem\n",
    "$$\n",
    "\\min_s \\|DF(x)s-g\\|^2\n",
    "$$\n",
    "which suggests the possibility of computing $s$ via the Conjugate Gradient algorithm, which is particularly attractive for large-scale problems. Even better, it suggests a refinement for taking a partial step when far from the solution, where a full Newton step is not likely to be constructive, and furthermore reduces the total computational work. \n",
    "\n",
    "This refinement is due to Steihaug (see Nocedal and Wright, sections 4.1 and 6.4). Suppose that the quadratic model of $J$ based on the Gauss-Newton Hessian at $x$ is presumed to be sufficiently accurate in a ball $\\{x+s:\\|s\\|\\le \\Delta\\}$ that actual reduction in $J$ from taking the step is a significant fraction of the predicted reduction in $J$ based on the quadratic model. The step isn't allowed to exceed the \"trust radius\" $\\Delta$.\n",
    "\n",
    "The quadratic model around $x$ is\n",
    "$$        \n",
    "J(x+s)\\approx J(x) + \\langle s, g(x)\\rangle  + 0.5\\langle s, H(x)s\\rangle\n",
    "$$\n",
    "where $H(x)=DF(x)^TDF(x)$. and the step $s$ solves $H(x)s = -g(x) = DF(x)^T(b-F(x))$ approximately. The predicted reduction is\n",
    "$$\n",
    "\\mbox{predred} = J(x) - 0.5s^Tg(x)\n",
    "$$\n",
    "The actual reduction is\n",
    "$$\n",
    "\\mbox{actred} = J(x) - J(x+s)\n",
    "$$\n",
    "Steihaug's algorithm uses the trust radius $\\Delta$ as a maximum step length. The usual CG termination criterion (residual less than fraction of initial) is augmented by testing the step length: it is it greater than $\\Delta$ then the iteration is stopped and the step is scaled back to have length $\\Delta$. Otherwise the iteration terminates as usual. If this step $s$ of length at most $\\Delta$, resulting from this modified CG, produces actual objective reduction (actred) that is too much less than predicted reduction (predred), then $\\Delta$ is reduced and $s$ is re-computed. Otherwise, $x$ is updated to $x+s$. If the step is very successful, in that the actual reduction is close to the predicted reduction, than $\\Delta$ is increased before then next update. \n",
    "\n",
    "As $x$ converges to a stationary point of $J$, eventually all steps are accepted. On the other hand, if the early iterates are far from a stationary point, $\\Delta$ may be reduced so much that the CG stops at the first iteration: then $s$ is precisely the negative gradient, and a short enough step in that direction must produce actual reduction close to predicted reduction. Thus this \"trust region\" algorithm must converge to a stationary point from any initial guess, and ends with full Gauss-Newton steps.  \n",
    "\n",
    "A single step of this algorithm involves an inner conjugate gradient iteration, and depends on a normal residual tolerance $0<\\rho<1$, reduction (\"Goldstein-Armijo\") parameters $0<\\gamma_{\\rm red} < \\gamma_{\\rm inc} <1$, scaling parameters $\\mu_{\\rm red} < 1 < \\mu_{\\rm inc}$ satisfying $\\mu_{\\rm red}*\\mu_{\\rm inc}<1$, and gradient tolerance $0<\\epsilon<1$. To update a current estimate $x$,\n",
    "\n",
    "1. Apply the C-G algorithm to update $s$, starting at $s=0$. Stop when either (a) $\\|H(x)s-g(x)\\|<\\rho\\|g\\|$, or (b) $\\|s\\| > \\Delta$. In case (b), replace $s$ by $\\Delta s /\\|s\\|$. Evaluate actred and predred as defined above.\n",
    "\n",
    "2. if $\\mbox{actred} < \\gamma_{\\rm red}*\\mbox{predred}$, replace $\\Delta$ by $\\mu_{\\rm red}*\\Delta$ and repeat step 1.\n",
    "\n",
    "3. Otherwise update $x$ (replace $x$ by $x+s$).\n",
    "\n",
    "4. if $\\mbox{actred} > \\gamma_{\\rm inc}*\\mbox{predred}$, replace $\\Delta$ by $\\mu_{\\rm inc}*\\Delta$. \n",
    "\n",
    "The module *vcalg.py* contains a modified CG algorithm, and a truncated Gauss-Newton algorithm using it to implement the algorithm described here. Typical parameters for the various constants are $\\rho=10^{-2}, \\gamma_{\\rm red}=0.1, \\gamma_{\\rm inc}=0.9, \\mu_{\\rm red}=0.5, \\mu_{\\rm inc}=1.8, \\epsilon=10^{-2}$.\n",
    "\n",
    "I apply this algorithm to the so-called Rosenbrock function, a moderately difficult nonlinear least-squares test problem (Nocedal and Wright, Excercise 9.1). The basic example is 2x2 and depends on a scale factor, usually set to 10. The function to be minimized is then \n",
    "$$\n",
    "f(x_0,x_1) = 100(x_1-x_0^2)^2 + (1-x_0)^2\n",
    "$$ \n",
    "which is equivalent to $J$ as defined above with\n",
    "$$\n",
    "F(x)=(10(x_1-x_0^2),-x_0)^T, \\, b=(0,-1)\n",
    "$$\n",
    "I have doubled this function to make a 4x4 problem, with scale factors 10 and 2, that is,\n",
    "$$\n",
    "F(x)=(10(x_1-x_0^2),-x_0,2(x_3-x_2^2),-x_2)^T, \\, b=(0,-1,0,-1)\n",
    "$$\n",
    "The minimum is at $(1,1,1,1)^T$, where $J=0$, so it is a global minimizer, also the unique stationary point of the Rosenbrock function.\n",
    "\n",
    "This function is defined in *npvc.DoubleRosie*, as a function on the 4-dimensional *npvc.Space*. The *deriv* method returns a *npvc.MatrixOperator*, as in the other examples above.\n",
    "\n",
    "I initialize $x$ at a point far from the global stationary point. The inner CG iteration is given enough iterations that it returns a very precise solution (in error by square root of macheps or less) of the Gauss-Newton equation $H(x)s + g(x)=0$ - if it is allowed to converge, instead of being stopped by the trust radius condition. I have suppressed the verbose output of the CG algorithm, to make the overall course of the GN iteration easier to see (cgverbose=0). For a large-scale problem, it will be important to monitor the behaviour of the CG iteration.\n",
    "\n",
    "Note that the initial trust radius $\\Delta=10$ is reduced four times before a successful step occurs, that is, $\\mbox{actred} > \\gamma_{\\rm red}\\mbox{predred}$.  Iteration 3 requires a further reduction of the trust radius, then the step is very successful ($\\mbox{actred} > \\gamma_{\\rm inc}\\mbox{predred}$), so the trust radius is increased. That turns out to be too long, so iteration 4 again decreases the trust radius, then takes another very successful step. This pattern repeats in Iterations 5 and 6. After iteration 6, all steps are successful.\n",
    "\n",
    "The reader can change the verbosity level to see the actred/predred comparison that drives the modification of the trust radius (gnverbose=2), and display the details of the CG iterations (cgverbose=1 or 2) to see whether the usual convergence criterion or the trust radius condition is active. In fact only in the last few iterations does CG iteration run to completion: in previous G-N iterations, CG is truncated by the trust radius constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i      J        |grad J|      Delta\n",
      "  0  1.4907e+01  1.1662e+02  1.0000e+01\n",
      "  0  1.4907e+01  1.1662e+02  5.0000e+00\n",
      "  0  1.4907e+01  1.1662e+02  2.5000e+00\n",
      "  0  1.4907e+01  1.1662e+02  1.2500e+00\n",
      "  0  1.4907e+01  1.1662e+02  6.2500e-01\n",
      "  1  1.4031e+01  1.0102e+02  6.2500e-01\n",
      "  2  1.3251e+01  8.4256e+01  6.2500e-01\n",
      "  3  1.2748e+01  6.6337e+01  6.2500e-01\n",
      "  3  1.2748e+01  6.6337e+01  3.1250e-01\n",
      "  4  2.4665e+00  7.1220e+00  5.6250e-01\n",
      "  4  2.4665e+00  7.1220e+00  2.8125e-01\n",
      "  5  1.6241e+00  3.5881e+00  5.0625e-01\n",
      "  6  1.2041e+00  8.4635e+00  5.0625e-01\n",
      "  6  1.2041e+00  8.4635e+00  2.5312e-01\n",
      "  7  9.6584e-01  9.6969e+00  2.5312e-01\n",
      "  8  8.0398e-01  1.1596e+01  2.5312e-01\n",
      "  9  5.0942e-01  1.0316e+01  4.5563e-01\n",
      " 10  4.5372e-01  1.5153e+01  4.5563e-01\n",
      " 11  1.9967e-01  1.2305e+01  4.5563e-01\n",
      " 12  6.4199e-03  2.4658e+00  8.2012e-01\n",
      " 13  7.8562e-19  2.8051e-08  1.4762e+00\n",
      "\n",
      "solution estimate:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 4\n",
      "Data object:\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import vcl\n",
    "import npvc\n",
    "import vcalg\n",
    "\n",
    "sp = npvc.Space(4)\n",
    "x = vcl.Vector(sp)\n",
    "b = vcl.Vector(sp)\n",
    "\n",
    "x.data[0]=-1.2\n",
    "x.data[1]=1.0\n",
    "x.data[2]=-1.2\n",
    "x.data[3]=1.0\n",
    "b.data[0]=0.0\n",
    "b.data[1]=-1.0\n",
    "b.data[2]=0.0\n",
    "b.data[3]=-1.0\n",
    "\n",
    "F = npvc.DoubleRosie(sp)\n",
    "\n",
    "vcalg.trgn(x, b, F, imax=40, eps=0.001, kmax=10, rho=1.e-6, \\\n",
    "           Delta=10.0, mured=0.5, muinc=1.8, \\\n",
    "           gammared=0.1, gammainc=0.95, \\\n",
    "           gnverbose=1, cgverbose=0)\n",
    "\n",
    "print('\\nsolution estimate:')\n",
    "x.myNameIs()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Composition and Constrained Optimization via Change of Variable\n",
    "\n",
    "Constraints on the solution vectors of optimization problem occur either because of physical or other limits on the vector components, or because objective functions are undefined or ill-behaved outside of so-called feasible sets. The most common constraints are so called simple bounds, which mandate that the components $x$ of solution vectors (with respect to a specified basis) lie between lower and upper limits, either inclusive: $l \\le x \\le u$, or exclusive: $l \\lt x \\lt u$. There is actually an important distinction between the two cases: the former admits the possibility of a solution lying on the boundary of the feasible set, whereas the latter does not. In the former case, the notion of solution is generalized: a solution on the boundary is not necessarily a stationary point of the objective, but instead satisfies the so-called KKT conditions (see for example Nocedal and Wright). A great deal of effort has gone into optimization with inclusive bounds, the quintessential example being linear programming (for which the solution *always* lies on the boundary). Little has been devoted to the exclusive case, which would occur when a physical field is known to lie strictly between two bounds, and possibly when an objective function definition is only correct when the bounds are strictly obeyed, so that any optimum or stationary point must be found in the interior of the cube defined by the bounds. This section describes an approach to this interior optimization problem, based on the observation that search for interior minima (or stationary points) is not really a constrained optimization problem, because it is equivalent to an unconstrained problem by change of variable.\n",
    "\n",
    "The first step is to devise a function that detects whether a vector lies in the interior of the cube defined by the bounds. For the NumPy-based vector class used in this notebook, that's simple: an implementation is given in *npvc.testbounds*. The use is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import vcl\n",
    "import npvc\n",
    "import vcalg\n",
    "import numpy as np\n",
    "\n",
    "sp = npvc.Space(4)\n",
    "x = vcl.Vector(sp)\n",
    "u = vcl.Vector(sp)\n",
    "l = vcl.Vector(sp)\n",
    "\n",
    "u.data=2.0*np.ones((4,1))\n",
    "l.data=-2.0*np.ones((4,1))\n",
    "\n",
    "x.data[0]=-1.2\n",
    "x.data[1]=1.0\n",
    "x.data[2]=-1.2\n",
    "x.data[3]=1.0\n",
    "\n",
    "print(npvc.testbounds(u,l,x))\n",
    "\n",
    "x.data[2]=3.0\n",
    "                   \n",
    "print(npvc.testbounds(u,l,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a somewhat artificial example of an objective function that is only well-defined in the interior of a cube, by adding a bounds test to the *DoubleRosie* function of the previous example. If you attempt to evaluate it at a point in the complement of the cube interior, it throws an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds violated\n",
      "called from npvc.DoubleRosieWithBounds.apply\n",
      "called from vcl.Vector operator()\n"
     ]
    }
   ],
   "source": [
    "FB = npvc.DoubleRosieWithBounds(sp,u,l)\n",
    "\n",
    "try:\n",
    "    print(FB(x).data)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For points in the interior, evaluation is as in the unconstrained case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.4 ]\n",
      " [ 1.2 ]\n",
      " [-0.88]\n",
      " [ 1.2 ]]\n"
     ]
    }
   ],
   "source": [
    "x.data[2]=-1.2\n",
    "\n",
    "print(FB(x).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is of course totally artificial, since the Rosenbrock function is well-defined in ${\\bf R}^n$. However many objective functions of simulation-driven optimization, based on numerical solution of stiff ordinary differential equations or partial differential equations, may fail to return a value if the parameter vectors are chosen outside of appropriate open cubes. \n",
    "\n",
    "This is important becasue unconstrained optimization methods, such as the Gauss-Newton method described in the last section, offer no means to confine their iterates to the feasible cubes, so that the iterations fail, often at the first step. Here for example is what happens if the Trust-Region Gauss-Newton algorithm is applied to *DoubleRosieWithBounds*, using the same parameters and initial solution vector as in the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i      J        |grad J|      Delta\n",
      "  0  1.4907e+01  1.1662e+02  1.0000e+01\n",
      "bounds violated\n",
      "called from npvc.DoubleRosieWithBounds.apply\n",
      "called from vcl.Vector operator()\n",
      "called from trgn\n"
     ]
    }
   ],
   "source": [
    "b = vcl.Vector(sp)\n",
    "\n",
    "b.data[0]=0.0\n",
    "b.data[1]=-1.0\n",
    "b.data[2]=0.0\n",
    "b.data[3]=-1.0\n",
    "\n",
    "try:\n",
    "    vcalg.trgn(x, b, FB, imax=40, eps=0.001, kmax=10, rho=1.e-6, \\\n",
    "               Delta=10.0, mured=0.5, muinc=1.8, \\\n",
    "               gammared=0.1, gammainc=0.95, \\\n",
    "               gnverbose=1, cgverbose=0)\n",
    "\n",
    "    print('\\nsolution estimate:')\n",
    "    x.myNameIs()  \n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The open cube defined by the bounds can be viewed as the image of ${\\bf R}^n$ under a differentiable map with a differentiable inverse. An example of such a map in 1D is\n",
    "$$\n",
    "g(x) = \\frac{u+l}{2} + \\frac{u-l}{2} \\frac{x}{\\sqrt{1+x^2}}\n",
    "$$\n",
    "(obviously not the only choice): for any $x \\in {\\bf R}$, $l <g(x)<u$, and $g$ is $C^{\\infty}$ and invertible with $C^{\\infty}$ inverse. For $n>1$ dimensions, simply apply the same transformation on each axis. The resulting diagonal map has a diagonal Jacobian with positive entries.\n",
    "\n",
    "Suppose $f$ is a function on the open cube defined by vectors $l,u$. Then the composition $f \\circ g$ is defined on ${\\bf R}^n$ ($f \\circ g (x) = f(g(x))$). The gradient of $f \\circ g$ is \n",
    "$$\n",
    "\\mbox{grad }f\\circ g(x) = Dg(x)^T \\mbox{grad }f(g(x))\n",
    "$$\n",
    "so $g(x)$ is a stationary point of $f$ if and only if $x$ is a stationary point of $f \\circ g$. Thus you can find the stationary points of $f$ by finding the stationary points of $f \\circ g$ and mapping them to the feasible cube by $g$.\n",
    "\n",
    "So here is the proposed algorithm, in a nutshell: find the stationary points of $f \\circ g$ using an unconstrained minimization algorithm (like trust-region GN), then map to stationary points of $f$ in the feasible cube. \n",
    "\n",
    "I've implemented the mapping described above in *npvc.ulbounds* - it defines a *vcl.Function*. To transpose the initial solution estimate from the open cube to ${\\bf R}^n$, you need the inverse function, implemented in *npvc.invulbounds*. For example, *xx* is the initial datum in ${\\bf R}^n$ corresponding to the initial point for GN in the last section's example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.75      ]\n",
      " [ 0.57735027]\n",
      " [-0.75      ]\n",
      " [ 0.57735027]]\n",
      "[[-1.2]\n",
      " [ 1. ]\n",
      " [-1.2]\n",
      " [ 1. ]]\n"
     ]
    }
   ],
   "source": [
    "ful = npvc.ulbounds(sp,u,l)\n",
    "iul = npvc.invulbounds(sp,u,l)\n",
    "\n",
    "xx = iul(x)\n",
    "\n",
    "print(xx.data)\n",
    "\n",
    "yy = ful(xx)\n",
    "\n",
    "print(yy.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation mapping of constrained to unconstrained minimization requires computational realization of function composition. This realization is supplied in *vcl.comp*, which returns a *vcl.Function* instance. *comp(f,g)* has the domain of *g*, the range of *f*, and derivative computed by the chain rule (a composition of linear maps, implemented in *vcl.lopcomp*).\n",
    "\n",
    "Here is the suggested algorithm, applied to the constrained version of the problem from the last section - same parameters, same initial vector, very close to the same final estimate. The residual is somewhat larger, but note that it is the residual of the ${\\bf R}^n$ problem that is displayed. The final estimate is very close (four digits) to the solution of the Rosenbrock problem, and the residual is the same - the gradient is very close also, as the Jacobian of the change of variable is a rather tame matrix at that point. So this is a solution of the accuracy specified by the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i      J        |grad J|      Delta\n",
      "  0  1.4907e+01  1.2450e+02  1.0000e+01\n",
      "  0  1.4907e+01  1.2450e+02  5.0000e+00\n",
      "  0  1.4907e+01  1.2450e+02  2.5000e+00\n",
      "  0  1.4907e+01  1.2450e+02  1.2500e+00\n",
      "  0  1.4907e+01  1.2450e+02  6.2500e-01\n",
      "  0  1.4907e+01  1.2450e+02  3.1250e-01\n",
      "  1  1.4468e+01  1.3704e+02  3.1250e-01\n",
      "  2  1.3421e+01  1.4054e+02  5.6250e-01\n",
      "  3  1.3037e+01  1.2262e+02  5.6250e-01\n",
      "  3  1.3037e+01  1.2262e+02  2.8125e-01\n",
      "  4  1.9624e+00  2.4428e+01  5.0625e-01\n",
      "  4  1.9624e+00  2.4428e+01  2.5312e-01\n",
      "  4  1.9624e+00  2.4428e+01  1.2656e-01\n",
      "  5  1.6984e+00  2.3961e+01  1.2656e-01\n",
      "  6  1.5194e+00  2.6620e+01  1.2656e-01\n",
      "  7  1.2855e+00  2.9754e+01  1.2656e-01\n",
      "  8  1.0135e+00  3.0919e+01  2.2781e-01\n",
      "  9  7.3727e-01  3.2197e+01  2.2781e-01\n",
      " 10  3.8639e-01  2.4951e+01  4.1006e-01\n",
      " 11  1.1081e-01  1.3575e+01  4.1006e-01\n",
      " 12  2.4490e-04  6.2951e-01  7.3811e-01\n",
      " 13  3.6877e-10  7.6970e-04  1.3286e+00\n",
      "\n",
      "solution estimate:\n",
      "Vector in space:\n",
      "npvc.Space of dimension 4\n",
      "Data object:\n",
      "[[1.        ]\n",
      " [0.99999735]\n",
      " [1.        ]\n",
      " [0.99999696]]\n"
     ]
    }
   ],
   "source": [
    "FBC = vcl.comp(FB,ful)\n",
    "\n",
    "try:\n",
    "    xx = iul(x)\n",
    "    \n",
    "    vcalg.trgn(xx, b, FBC, imax=40, eps=0.001, kmax=10, rho=1.e-6, \\\n",
    "               Delta=10.0, mured=0.5, muinc=1.8, \\\n",
    "               gammared=0.1, gammainc=0.95, \\\n",
    "               gnverbose=1, cgverbose=0)\n",
    "\n",
    "    x = ful(xx)\n",
    "\n",
    "    print('\\nsolution estimate:')\n",
    "    x.myNameIs()  \n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish this discussion of constraint implementation, I emphasize again that the change-of-variables approach does *NOT* solve constrained optimization problems in the usual sense, that is, identify points on the constraint boundary that satisfy the KKT conditions (in addition to stationary points in the interior). Many other approaches, such as L-BFGS-B, and the Coleman-Li reflection algorithm (both implemented in SciPy, are constrained optimization solvers. The approch I have just described is not. If for example you were to change the bounds in the *DoubleRosieWithBounds* example to $u_2 = 0, l_2=-1$, then the change-of-variable algorithm cannot find a stationary point, since there are none in the feasible set. There are KKT points on the boundary, but our algorithm won't find them. So what does it do? Try it and find out.\n",
    "\n",
    "The utility of the change-of-variable approach is in finding points at which the objective value is small and the bounds are strictly observed. This will usually not be a KKT point, or even (exactly) a stationary point, but that doesn't matter. If (1) the objective function is strictly convex, and (2) the minimum value is smaller than all values on the boundary, then the change-of-variables algorithm will approximate a minimizer. That is really all that can be said.\n",
    "\n",
    "A practical use pattern could specify two sets of bounds, an outer set $(\\bar{u},\\bar{l})$ and an inner set $(u,l)$, satisfying $\\bar{l} < l < u < \\bar{u}$. The outer bounds define a feasible set for evaluation of the objective function, that is, for models satisfying the outer bounds, the objective function returns a value rather than an error condition. The inner set defines a feasible set for the known or posited properties of a solution, that is, conditions that a physically sensible model should satisfy. The algorithm uses the mapping from ${\\bf R}^n$ to the open cube defined by the outer bounds, but terminates if the iteration exceeds the inner bounds. Under the conditiopns mentioned in the last paragraph, if such termination occurs, no stationary point satisfying the inner bounds exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated Gauss-Newton Algorithm for Variable Projection Reduction of Separable Nonlinear Least Squares\n",
    "\n",
    "The variable projection method is a minimization algorithm for a scalar function on a product space $f:X \\oplus W \\rightarrow Y$ of class $C^2$. I will assume that $f$ is defined on the whole product space; the refinements necessary to accommodate constraints on $x$ are similar to those needed in the nonlinear least squares problem discussed in the last section. While not strictly necessary, I will also assume that $f$ is quadratic in the second variable. Put another way, $f(x,w) = 0.5*\\|A(x)w-b\\|^2$, where the values of $A$ are linear operators $: W \\rightarrow Y$. Further, it's usually assumed that $A(x)$ is of full column rank (or coercive, in the infinite dimensional case), so that that for each $x$, there is a unique minimiser $\\tilde{w}(x)$ of $w \\mapsto f(x,w)$, the solution of the normal equation: $A(x)^T(A(x)\\tilde{w}(x) - b)=0$. Define the *variable projection (VP) reduction* $\\tilde{f}$ by \n",
    "$$\n",
    "\\tilde{f}(x) = 0.5*\\|A(x)\\tilde{w}(x)-b\\|^2= \\min_w f(x,w)\n",
    "$$\n",
    "Since $A(x)$ is assumed coercive for every $x \\in X$, $A(x)^TA(x)$ is invertible, and $\\tilde{w}(x) = (A(x)^TA(x))^{-1}A(x)^Tb$. So\n",
    "$$ \n",
    "\\tilde{f}(x) = 0.5*\\|(A(x)(A(x)^TA(x))^{-1}A(x)^T - I)b\\|^2\n",
    "$$\n",
    "The operator in parenthesis projects $Y$ onto the orthocomplement of the range of $A(x)$: call it $P(x)$. That is,\n",
    "$$\n",
    "P(x) = I-A(x)(A(x)^TA(x))^{-1}A(x)^T\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\tilde{f}(x) = 0.5*\\|P(x)b\\|^2\n",
    "$$\n",
    "So the reduced objective is half the length squared of the projection of the data vector $b$ onto the orthocomplement of the range of $A(x)$, which of course depends on $x$. That fact accounts for the name \"variable projection\".\n",
    "\n",
    "One of the main results of the Golub and Pereyra 1973 paper is that $x$ is a stationary point of $\\tilde{f}$ if and only if $(x,\\tilde{w}(x))$ is a stationary point of $f$. It's worth spelling out the argument because it highlights several important points about the VP reduction.\n",
    "\n",
    "Note that $f$ is differentiable, if $(x,w)\\mapsto A(x)w$ is differentiable, which I will assume. Suppose $s \\in X$. Then the directional derivative of $\\tilde{w}(x)=(A(x)^TA(x))^{-1}A(x)^Tb$ at $x$ in direction $s$ is \n",
    "$$\n",
    "\\frac{d}{dt}((A(x+ts)^TA(x+ts))^{-1}A(x+ts)^Tb)|_{t=0}\n",
    "$$\n",
    "$$\n",
    "=-(A(x)^TA(x))^{-1}\\frac{d}{dt}(A(x+ts)^TA(x+ts)(A(x)^TA(x))^{-1}A(x)^Tb)|_{t=0} + (A(x)^TA(x))^{-1}\\frac{d}{dt}(A(x+ts)^Tb)|_{t=0}\n",
    "$$\n",
    "and in particular $\\tilde{w}$ is differentiable. Therefore\n",
    "$$\n",
    "\\frac{d}{dt}\\tilde{f}(x+ts)|_{t=0} = \\left\\langle\\frac{d}{dt}A(x+ts)\\tilde{w}(x), A(x)\\tilde{w}(x)-b\\right\\rangle + \n",
    "$$\n",
    "$$\n",
    "0.5\\left\\langle A(x)\\frac{d}{dt}\\tilde{w}(x+ts),A(x)\\tilde{w}-b\\right\\rangle\n",
    "$$\n",
    "Since the normal equation is equivalent to the assertion that the residual $A(x)\\tilde{w}(x)-b$ is orthogonal to the range of $A(x)$, the last term vanishes.\n",
    "\n",
    "The first term can be re-written as the directional derivative of $f(x,w)$ for fixed $w=\\tilde{w}(x)$, that is,\n",
    "$$\n",
    "\\frac{d}{dt}\\tilde{f}(x+ts)|_{t=0} = \\frac{d}{dt}f(x+ts,w)|_{t=0,w=\\tilde{w}(x)}.\n",
    "$$\n",
    "So $x$ is a stationary point of $\\tilde{f}$ if and only if the directional derivative of $f$ at $(x,\\tilde{w}(x))$ in all directions $(s,0)$ is zero. But the directional derivative of $f$ at $(x,\\tilde{w}(x))$ in all directions $(0,\\delta w)$ is also zero - that is the definition of $\\tilde{w}(x)$. Since the directional derivative is linear in the direction, the directional derivative of $f$ at $(x,\\tilde{w}(x))$ is zero in all directions, that is, $(x,\\tilde{w}(x))$ is a stationary point of $f$, if and only if $x$ is a stationary point of $\\tilde{f}$. \n",
    "\n",
    "The derivative of the linear-operator-value function $A$ is naturally a bilinear-operator-valued function, since it's linear in the argument $w$ and in the direction $s$ separately. Call it $DA$:\n",
    "$$\n",
    "\\frac{d}{dt}A(x+ts)w = DA(x)(s,w)\n",
    "$$ \n",
    "In terms of $DA$, the directional derivative of $\\tilde{f}$ is\n",
    "$$\n",
    "\\frac{d}{dt}\\tilde{f}(x+ts)|_{t=0} = \\left\\langle DA(x)(\\tilde{w}(x),s), A(x)\\tilde{w}(x)-b\\right\\rangle \n",
    "$$\n",
    "The expression on the right is the same as the derivative of the function\n",
    "$$\n",
    "x \\mapsto f(x,w) 0.5*\\|A(x)w-b\\|^2,\n",
    "$$ \n",
    "evaluated at $w=\\tilde{w}(x)$, that is, $f(x,w)$ *for fixed w* \n",
    "The gradient of $\\tilde{f}$ is the Riesz representer of the directional derivative:\n",
    "$$\n",
    "\\langle s, \\mbox{grad} \\tilde{f}(x)\\rangle = \\frac{d}{dt}\\tilde{f}(x+ts)|_{t=0}\n",
    "$$\n",
    "$$\n",
    "= \\left\\langle s, DA(x)^*(\\tilde{w}(x),A(x)\\tilde{w}(x)-b)\\right\\rangle \n",
    "$$\n",
    "in which $DA(x)^*$ denotes the *partial adjoint* defined by\n",
    "$$\n",
    "\\langle s, DA(x)^*(w,y) \\rangle = \\langle DA(x)(w,s),y\\rangle\n",
    "$$\n",
    "That is, $y \\mapsto DA(x)^*(w,y)$ is the adjoint of the map $s \\mapsto DA(x)(w,s)$, the latter being the derivative of $x \\mapsto A(x)w$. \n",
    "\n",
    "Thus\n",
    "$$\n",
    "\\mbox{grad} \\tilde{f}(x) = DA(x)^*(\\tilde{w}(x),A(x)\\tilde{w}(x)-b).\n",
    "$$\n",
    "\n",
    "In fact, this is also the gradient of a least-squares objective.\n",
    "For a fixed choice $w \\in W$, define $F_w(x) = A(x)w$. Then $DA(x)^*(w,y) = DF_w(x)^T$. Moreover, if $f_w(x) = 0.5*\\|F_w(x)-b\\|^2$, then \n",
    "$$\n",
    "\\mbox{grad} f_w(x)|_{w=\\tilde{w}(x)} =  \\mbox{grad}\\tilde{f}(x).\n",
    "$$\n",
    "From the preceding section,\n",
    "$$\n",
    "\\mbox{grad} f_w(x) = DF_w(x)^T(F_w(x)-b).\n",
    "$$ \n",
    "Therefore computing the gradient of the VP reduction can be accomplished by combining a computation of the gradient of a nonlinear least-squares objective with a solution of the normal equation.\n",
    "\n",
    "Of course $\\tilde{f}$ *is* itself a nonlinear least squares objective: if you define $F(x)=A(x)\\tilde{w}(x)$, then \n",
    "$$\n",
    "\\tilde{f}(x) = 0.5*\\|F(x)-b\\|^2.\n",
    "$$\n",
    "so it is natural to use the Gauss-Newton algorithm to minimize $\\tilde{f}$. The Gauss-Newton step $s$ solves $DF(x)^T(DF(x)s-(F(x)-b))=DF(x)^TDF(x)s+\\mbox{grad}\\tilde{f}(x)=0$. This is a simplification over the Newton step, but for the special case of the VP reduction can be simplified still further.\n",
    "$$\n",
    "DF(x)s = D(A(x)\\tilde{w}(x))s = DA(x)(\\tilde{w}(x),s) + A(x)D\\tilde{w}(x)s\n",
    "$$\n",
    "From the differentiability analysis of $\\tilde{w}$,\n",
    "$$\n",
    "D\\tilde{w}(x)s = \n",
    "$$\n",
    "$$\n",
    "=-(A(x)^TA(x))^{-1}(DA(x)^T(A(x)((A(x)^TA(x))^{-1}A(x)^Tb),s) + A(x)^T DA(x)((A(x)^TA(x)^{-1}A(x)^Tb,s) + (A(x)^TA(x)^{-1}DA(x)^T(b,s)|\n",
    "$$\n",
    "$$\n",
    "= -(A(x)^TA(x))^{-1}[DA(x)^T(A(x)\\tilde{w}(x),s) + A(x)^TDA(x)(\\tilde{w}(x),s)] + (A(x)^TA(x))^{-1}DA(x)^T(b,s)\n",
    "$$\n",
    "So \n",
    "$$\n",
    "DF(x)s = DA(x)(\\tilde{w},s) - A(x) (A(x)^TA(x))^{-1}[DA(x)^T(A(x)\\tilde{w}(x),s) + A(x)^TDA(x)(\\tilde{w}(x),s)] + A(x)(A(x)^TA(x))^{-1}DA(x)^T(b,s)\n",
    "$$\n",
    "$$\n",
    "= (I-A(x)(A(x)^TA(x))^{-1}A(x)^T)DA(\\tilde{w}(x),s) + A(x)(A(x)^TA(x))^{-1}DA(x)^T(b-A(x)\\tilde{w}(x),s)\n",
    "$$\n",
    "The second term has the residual $b-A(x)\\tilde{w}(x)=b-F(x)$ as the first argument of the bilinear operator $DA(x)^T$. Kaufman first pointed this out in 1974, and proposed that this term be dropped with the same justification as underlies the transition from Newton to Gauss-Newton: that is, if the residual is small (nearly noise-free data and close to the solution), this term should be negligible. Accepting this proposal, obtain the VP-GN approximation\n",
    "$$\n",
    "DF(x)s \\approx (I-A(x)(A(x)^TA(x))^{-1}A(x)^T)DA(x)(\\tilde{w}(x),s)\n",
    "$$\n",
    "$$\n",
    "= P(x)DA(x)(\\tilde{w}(x),s)\n",
    "$$\n",
    "where $P(x)=I-A(x)(A(x)^TA(x))^{-1}A(x)^T$ is the projection of $Y$ onto the orthocomplement of the range of $A(x)$, introduced earlier.\n",
    "Since $P(x)$ is a projection, it is symmetric, positive semi-definite, and idempotent, that is $P(x)^TP(x)=P(x)^2=P(x)$. Thus the Gauss-Newton operator is approximately\n",
    "$$\n",
    "DF(x)^TDF(x)s \\approx H_{VP}(x)s = DA(x)^*(\\tilde{w}(x),P(x)DA(x)(\\tilde{w}(x),s)).\n",
    "$$\n",
    "The Kaufman modification of GN for VP is to replace $H(x)=DF(x)^TDF(x)$ with $H_{VP}$. The solution $s$ of the modified GN equation $H_{VP}(x)s=-\\mbox{grad}\\tilde{f}(x)$ is a descent (or at least non-ascent) direction for $\\tilde{f}$:\n",
    "$$\n",
    "\\langle \\mbox{grad}\\tilde{f}(x), s \\rangle \n",
    "= -\\langle H_{VP}(x)s, s\\rangle \n",
    "$$\n",
    "$$\n",
    "=-\\langle DA(x)^*(\\tilde{w}(x),P(x)DA(x)(\\tilde{w}(x),s)),s\\rangle\n",
    "= - \\langle DA(x)(\\tilde{w}(x),s),P(x)DA(x)(\\tilde{w}(x),s)\\rangle\n",
    "$$\n",
    "$$\n",
    "= -\\|P(x)DA(x)(\\tilde{w}(x),s)\\|^2 \\le 0\n",
    "$$\n",
    "since $P(x)$ is a projector. \n",
    "\n",
    "These calculations suggest a modified Gauss-Newton algorithm, which I will call the Kaufman-CG (KCG) algorithm since Kaufman supplied the key observation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
